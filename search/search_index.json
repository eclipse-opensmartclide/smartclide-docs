{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to the SmartCLIDE integrated platform. The SmartCLIDE project has received funding from the European Union's Horizon 2020 research and innovation programme under grant agreement No 871177. For more general information about the SmartCLIDE project visit our website . About the project The main objective of SmartCLIDE is to propose a radically new smart cloud-native development environment, based on the coding-by-demonstration principle, that will support creators of cloud services in the discovery , creation, composition, testing , and deployment of full-stack data-centered services and applications in the cloud. SmartCLIDE will provide high levels of abstraction at all stages (development, testing, deployment, and run-time) as well as self-discovery of IaaS and SaaS Services. SmartCLIDE will provide several categories of abstractions: at development stage, SmartCLIDE will provide abstractions on data transformations or processing at testing stage, mechanisms to visualize flow and status or artifacts to automatically test the expected behavior at deployment stage, abstractions of physical and virtual resources or at runtime, mechanisms to monitor the performance and operation of the service The cloud nature of the environment will enable collaboration between different stakeholders, and the self-discovery of IaaS and SaaS services and the high levels of abstraction will facilitate the composition and deployment of new services to nontechnical staff (with no previous experience on programming or on the administration of systems and infrastructure). Equally, hiding the complexity of the infrastructure, and adding intelligence to this layer, will allow selecting the most adequate infrastructure services in each moment. SmartCLIDE will allow SMEs and Public Administration to boost the adoption of Cloud solutions, being validated at one solution-oriented to Public Administration (Social Security System) and three different IoT products of software development SMEs within the consortium. Modules The SmartCLIDE platform consists of the following modules: Context Handling Service Creation Service Creation Service Maintenance Test Generation Design Patterns Service Discovery Smart Assistant (Deep Learning Engine - DLE) Consortium The SmartCLIDE Consortium consists of the following partners: ATB Netcompany-Intrasoft AIR University of Macedonia (UoM) Centre for Research and Technology Hellas (CERTH) The Open Group Eclipse Foundation Wellness Telecom UNPARALLEL CONTACT Software KAIROS Digital Solutions License Distributed under the Eclipse Public License 2.0. See LICENSE for more information.","title":"Home"},{"location":"#welcome-to-the-smartclide-integrated-platform","text":"The SmartCLIDE project has received funding from the European Union's Horizon 2020 research and innovation programme under grant agreement No 871177. For more general information about the SmartCLIDE project visit our website .","title":"Welcome to the SmartCLIDE integrated platform."},{"location":"#about-the-project","text":"The main objective of SmartCLIDE is to propose a radically new smart cloud-native development environment, based on the coding-by-demonstration principle, that will support creators of cloud services in the discovery , creation, composition, testing , and deployment of full-stack data-centered services and applications in the cloud. SmartCLIDE will provide high levels of abstraction at all stages (development, testing, deployment, and run-time) as well as self-discovery of IaaS and SaaS Services. SmartCLIDE will provide several categories of abstractions: at development stage, SmartCLIDE will provide abstractions on data transformations or processing at testing stage, mechanisms to visualize flow and status or artifacts to automatically test the expected behavior at deployment stage, abstractions of physical and virtual resources or at runtime, mechanisms to monitor the performance and operation of the service The cloud nature of the environment will enable collaboration between different stakeholders, and the self-discovery of IaaS and SaaS services and the high levels of abstraction will facilitate the composition and deployment of new services to nontechnical staff (with no previous experience on programming or on the administration of systems and infrastructure). Equally, hiding the complexity of the infrastructure, and adding intelligence to this layer, will allow selecting the most adequate infrastructure services in each moment. SmartCLIDE will allow SMEs and Public Administration to boost the adoption of Cloud solutions, being validated at one solution-oriented to Public Administration (Social Security System) and three different IoT products of software development SMEs within the consortium.","title":"About the project"},{"location":"#modules","text":"The SmartCLIDE platform consists of the following modules: Context Handling Service Creation Service Creation Service Maintenance Test Generation Design Patterns Service Discovery Smart Assistant (Deep Learning Engine - DLE)","title":"Modules"},{"location":"#consortium","text":"The SmartCLIDE Consortium consists of the following partners: ATB Netcompany-Intrasoft AIR University of Macedonia (UoM) Centre for Research and Technology Hellas (CERTH) The Open Group Eclipse Foundation Wellness Telecom UNPARALLEL CONTACT Software KAIROS Digital Solutions","title":"Consortium"},{"location":"#license","text":"Distributed under the Eclipse Public License 2.0. See LICENSE for more information.","title":"License"},{"location":"backend/","text":"Smartclide Service Creation Testing SmartCLIDE Service Creation Testing Backend Component Preconditions to build and run Service Creation Testing To build and run the backend service of Service Creation Testing, the following software is required: Java (at least version 8) Apache Maven (at least version 3.2+) Docker (for building and running the final image) How to build Service Creation Testing Service Creation Testing can be built using maven with the following command: mvn install In order to build a Docker image of the service that can be deployed, the following commands can be used: mvn install docker build -t ${IMAGE_NAME:IMAGE_TAG} . More specifically: mvn install docker build -t smartclide-service-creation-testing-backend:latest . How to run Service Creation Testing All the images of this component can be found here . You can run the backend service with the following command: docker run smartclide-service-creation-testing-backend:latest How to use Service Creation Testing This Spring Boot application generates automatically Unit tests, for a given project. The service includes one endpoint: Endpoint: \"/generateTests\" -> automatic generation of Unit tests. Request parameters: gitRepoURL -> String gitUsername -> String gitToken -> String","title":"Service Creation Testing"},{"location":"backend/#smartclide-service-creation-testing","text":"SmartCLIDE Service Creation Testing Backend Component","title":"Smartclide Service Creation Testing"},{"location":"backend/#preconditions-to-build-and-run-service-creation-testing","text":"To build and run the backend service of Service Creation Testing, the following software is required: Java (at least version 8) Apache Maven (at least version 3.2+) Docker (for building and running the final image)","title":"Preconditions to build and run Service Creation Testing"},{"location":"backend/#how-to-build-service-creation-testing","text":"Service Creation Testing can be built using maven with the following command: mvn install In order to build a Docker image of the service that can be deployed, the following commands can be used: mvn install docker build -t ${IMAGE_NAME:IMAGE_TAG} . More specifically: mvn install docker build -t smartclide-service-creation-testing-backend:latest .","title":"How to build Service Creation Testing"},{"location":"backend/#how-to-run-service-creation-testing","text":"All the images of this component can be found here . You can run the backend service with the following command: docker run smartclide-service-creation-testing-backend:latest","title":"How to run Service Creation Testing"},{"location":"backend/#how-to-use-service-creation-testing","text":"This Spring Boot application generates automatically Unit tests, for a given project. The service includes one endpoint: Endpoint: \"/generateTests\" -> automatic generation of Unit tests. Request parameters: gitRepoURL -> String gitUsername -> String gitToken -> String","title":"How to use Service Creation Testing"},{"location":"backend-reusability-index/","text":"Smartclide TD Reusability Index SmartCLIDE TD Reusability Index Backend Component Preconditions to build and run TD Reusability To build and run the backend service of TD Reusability Index, the following software is required: Java (at least version 11) Apache Maven (at least version 3.2+) Docker (for building and running the final image) How to build TD Reusability Index TD Reusability Index can be built using maven with the following command: mvn install In order to build a Docker image of the service that can be deployed, the following commands can be used: mvn install docker build -t ${IMAGE_NAME:IMAGE_TAG} . More specifically: mvn install docker build -t smartclide-td-reusability-index-backend:latest . How to run TD Reusability Index All the images of this component can be found here . You can run the backend service with the following command: docker run smartclide-td-reusability-index-backend:latest Extra dependencies of TD Reusability Index This backend has an external dependency with the TD Interest , in order to get the metrics of a specific project. This dependency is visible through the application.properties file, where there is the following variable: - interest-service.url","title":"TD Reusability Index Backend"},{"location":"backend-reusability-index/#smartclide-td-reusability-index","text":"SmartCLIDE TD Reusability Index Backend Component","title":"Smartclide TD Reusability Index"},{"location":"backend-reusability-index/#preconditions-to-build-and-run-td-reusability","text":"To build and run the backend service of TD Reusability Index, the following software is required: Java (at least version 11) Apache Maven (at least version 3.2+) Docker (for building and running the final image)","title":"Preconditions to build and run TD Reusability"},{"location":"backend-reusability-index/#how-to-build-td-reusability-index","text":"TD Reusability Index can be built using maven with the following command: mvn install In order to build a Docker image of the service that can be deployed, the following commands can be used: mvn install docker build -t ${IMAGE_NAME:IMAGE_TAG} . More specifically: mvn install docker build -t smartclide-td-reusability-index-backend:latest .","title":"How to build TD Reusability Index"},{"location":"backend-reusability-index/#how-to-run-td-reusability-index","text":"All the images of this component can be found here . You can run the backend service with the following command: docker run smartclide-td-reusability-index-backend:latest","title":"How to run TD Reusability Index"},{"location":"backend-reusability-index/#extra-dependencies-of-td-reusability-index","text":"This backend has an external dependency with the TD Interest , in order to get the metrics of a specific project. This dependency is visible through the application.properties file, where there is the following variable: - interest-service.url","title":"Extra dependencies of TD Reusability Index"},{"location":"backend-td-interest/","text":"Smartclide TD Interest SmartCLIDE TD Interest Backend Component Preconditions to build and run TD Interest To build and run the backend service of TD Interest, the following software is required: Java (at least version 11) Apache Maven (at least version 3.2+) Docker (for building and running the final image) How to build TD Interest TD Interest can be built using maven with the following command: mvn install In order to build a Docker image of the service that can be deployed, the following commands can be used: mvn install docker build -t ${IMAGE_NAME:IMAGE_TAG} . More specifically: mvn install docker build -t smartclide-td-interest-backend:latest . How to run TD Interest All the images of this component can be found here . You can run the backend service with the following command: docker run smartclide-td-interest-backend:latest Extra dependencies of TD Interest This component uses an internal database in order to calculate the TD Interest, as from its definition requires historical data. This database is a postgresql where the schema can be found here . After the creation of the database, the properties for accessing the database can be changed from here . In order to change the database url, username, and password. - spring.datasource.url=jdbc:postgresql://localhost:5432/tdinterest - spring.datasource.username=tdinterest - spring.datasource.password=tdinterest","title":"TD Interest Backend"},{"location":"backend-td-interest/#smartclide-td-interest","text":"SmartCLIDE TD Interest Backend Component","title":"Smartclide TD Interest"},{"location":"backend-td-interest/#preconditions-to-build-and-run-td-interest","text":"To build and run the backend service of TD Interest, the following software is required: Java (at least version 11) Apache Maven (at least version 3.2+) Docker (for building and running the final image)","title":"Preconditions to build and run TD Interest"},{"location":"backend-td-interest/#how-to-build-td-interest","text":"TD Interest can be built using maven with the following command: mvn install In order to build a Docker image of the service that can be deployed, the following commands can be used: mvn install docker build -t ${IMAGE_NAME:IMAGE_TAG} . More specifically: mvn install docker build -t smartclide-td-interest-backend:latest .","title":"How to build TD Interest"},{"location":"backend-td-interest/#how-to-run-td-interest","text":"All the images of this component can be found here . You can run the backend service with the following command: docker run smartclide-td-interest-backend:latest","title":"How to run TD Interest"},{"location":"backend-td-interest/#extra-dependencies-of-td-interest","text":"This component uses an internal database in order to calculate the TD Interest, as from its definition requires historical data. This database is a postgresql where the schema can be found here . After the creation of the database, the properties for accessing the database can be changed from here . In order to change the database url, username, and password. - spring.datasource.url=jdbc:postgresql://localhost:5432/tdinterest - spring.datasource.username=tdinterest - spring.datasource.password=tdinterest","title":"Extra dependencies of TD Interest"},{"location":"backend-td-principal/","text":"Smartclide TD Principal SmartCLIDE TD Principal Backend Component Preconditions to build and run TD Principal To build and run the backend service of TD Principal, the following software is required: Java (at least version 11) Apache Maven (at least version 3.2+) Docker (for building and running the final image) How to build TD Principal TD Principal can be built using maven with the following command: mvn install In order to build a Docker image of the service that can be deployed, the following commands can be used: mvn install docker build -t ${IMAGE_NAME:IMAGE_TAG} . More specifically: mvn install docker build -t smartclide-td-principal-backend:latest . How to run TD Principal All the images of this component can be found here . You can run the backend service with the following command: docker run -p 8555:8555 smartclide-td-principal-backend:latest How to configure TD Principal application.properties The main properties of this backend service can be found here , and are the following: - server.port=8555, In order for the service to start in the 8555 port - gr.nikos.smartclide.sonarqube.url=http://localhost:9000, In order to get the SonarQube instance and the in case this is not configured in the beginning of the service it's going to be the localhost. Docker The main thing that should be configured is the SonarQube instance, as in the majority of the cases the SonarQube is not going to be in the localhost. This can be achieved by using the following environment variable: docker run -p 8555:8555 -e GR_NIKOS_SMARTCLIDE_SONARQUBE_URL=${SONARQUBE_URL} smartclide-td-principal-backend:latest More specifically: docker run -p 8555:8555 -e GR_NIKOS_SMARTCLIDE_SONARQUBE_URL=http://1.1.1.1:9000 smartclide-td-principal-backend:latest","title":"TD Principal Backend"},{"location":"backend-td-principal/#smartclide-td-principal","text":"SmartCLIDE TD Principal Backend Component","title":"Smartclide TD Principal"},{"location":"backend-td-principal/#preconditions-to-build-and-run-td-principal","text":"To build and run the backend service of TD Principal, the following software is required: Java (at least version 11) Apache Maven (at least version 3.2+) Docker (for building and running the final image)","title":"Preconditions to build and run TD Principal"},{"location":"backend-td-principal/#how-to-build-td-principal","text":"TD Principal can be built using maven with the following command: mvn install In order to build a Docker image of the service that can be deployed, the following commands can be used: mvn install docker build -t ${IMAGE_NAME:IMAGE_TAG} . More specifically: mvn install docker build -t smartclide-td-principal-backend:latest .","title":"How to build TD Principal"},{"location":"backend-td-principal/#how-to-run-td-principal","text":"All the images of this component can be found here . You can run the backend service with the following command: docker run -p 8555:8555 smartclide-td-principal-backend:latest","title":"How to run TD Principal"},{"location":"backend-td-principal/#how-to-configure-td-principal","text":"application.properties The main properties of this backend service can be found here , and are the following: - server.port=8555, In order for the service to start in the 8555 port - gr.nikos.smartclide.sonarqube.url=http://localhost:9000, In order to get the SonarQube instance and the in case this is not configured in the beginning of the service it's going to be the localhost. Docker The main thing that should be configured is the SonarQube instance, as in the majority of the cases the SonarQube is not going to be in the localhost. This can be achieved by using the following environment variable: docker run -p 8555:8555 -e GR_NIKOS_SMARTCLIDE_SONARQUBE_URL=${SONARQUBE_URL} smartclide-td-principal-backend:latest More specifically: docker run -p 8555:8555 -e GR_NIKOS_SMARTCLIDE_SONARQUBE_URL=http://1.1.1.1:9000 smartclide-td-principal-backend:latest","title":"How to configure TD Principal"},{"location":"context-handling/","text":"smartclide-context SmartCLIDE Context Handling Component Preconditions to build and run Context Handling To build and run Context Handling, the following software is required: Java (at least version 11) Apache Maven (at least version 3.5.4) Docker (for running tests and deploying Context Handling on the SmartCLIDE cluster) docker-compose (for running local sample instance only) How to build Context Handling Context Handling can be built using maven with the following command: shell mvn install In order to build and push a container image that can be deployed, the following command can be used: shell mvn install mvn jib:build -pl smartclide-monitoring -Djib.to.image=\"${IMAGE_NAME:IMAGE_TAG}\" -Djib.to.auth.username=\"${CONTAINER_REGISTRY_USERNAME}\" -Djib.to.auth.password=\"${CONTAINER_REGISTRY_TOKEN}\" How to run Context Handling A sample configuration and docker-compose file can be found in the samples folder . You can run the sample with the following command: shell docker-compose -f samples/docker-compose.yml up How to configure Context Handling Monitoring Config monitoring-config.xml An example monitoring configuration can be found here: monitoring-config.xml monitoring-config.xsd The corresponding XSD file can be found here: monitoring-config.xsd Description indexes Each index entry has the following mandatory attributes id: The unique name of the index location: The URI of the location the index is stored datasources Each datasource entry has the following mandatory attributes id:The unique name of the datasource type:The type of the datasource. Possible values are: filesystem, webservice, database, messageBroker monitor:The class of the monitor to be used. Possible values are: package de.atb.context.monitoring.monitors.database.DatabaseMonitor package de.atb.context.monitoring.monitors.file.FileSystemMonitor package de.atb.context.monitoring.monitors.file.FilePairSystemMonitor package de.atb.context.monitoring.monitors.file.FileTripletSystemMonitor package de.atb.context.monitoring.monitors.webservice.MessageBrokerMonitor package de.atb.context.monitoring.monitors.webservice.WebServiceMonitor package de.atb.context.monitoring.monitors.GitlabCommitMonitor package de.atb.context.monitoring.monitors.GitMonitor options: Options for the datasource can be entered using this value. The options are dependent on the datasource to be used uri:The uri of the data source to be monitored class:The following datasource implementations are available package de.atb.context.monitoring.config.models.datasources.DatabaseDataSource package de.atb.context.monitoring.config.models.datasources.FilePairSystemDataSource package de.atb.context.monitoring.config.models.datasources.FileSystemDataSource package de.atb.context.monitoring.config.models.datasources.FileTripletSystemDataSource package de.atb.context.monitoring.config.models.datasources.MessageBrokerDataSource package de.atb.context.monitoring.config.models.datasources.WebServiceDataSource package de.atb.context.monitoring.config.models.datasources.GitlabDataSource interpreters Each interpreter entry has the following mandatory attributes id: The unique name of the interpreter configuration analyser: The analyser class to be used. The following implementations are available: package de.atb.context.monitoring.analyser.database.DatabaseAnalyser package de.atb.context.monitoring.analyser.file.FileAnalyser package de.atb.context.monitoring.analyser.file.FilePairAnalyser package de.atb.context.monitoring.analyser.file.FileTripletAnalyser package de.atb.context.monitoring.analyser.webservice.MessageBrokerAnalyser package de.atb.context.monitoring.analyser.webservice.WebServiceAnalyser package de.atb.context.monitoring.analyser.webserviceGitAnalyser package de.atb.context.monitoring.analyser.webservice.GitlabCommitAnalyser parser: The parser class to be used. The following implementations are available: package de.atb.context.monitoring.parser.database.DatabaseParser package de.atb.context.monitoring.parser.file.FileParser package de.atb.context.monitoring.parser.file.FilePairParser package de.atb.context.monitoring.parser.file.FileTripletParser package de.atb.context.monitoring.parser.webservice.MessageBrokerParser package de.atb.context.monitoring.parser.webservice.WebServiceParser package de.atb.context.monitoring.parser.GitlabCommitParser package de.atb.context.monitoring.parser.GitParser type: Currently only used for File analyser and parser. Defines the file extensions to be used. monitors Each monitor entry has the following mandatory attributes id: The unique name of the monitor datasource: The id of one previously defined datasource (see above) interpreter: The id of one previously defined interpreter (see above) index: The id of one previously defined index (see above)","title":"Context Handling"},{"location":"context-handling/#smartclide-context","text":"SmartCLIDE Context Handling Component","title":"smartclide-context"},{"location":"context-handling/#preconditions-to-build-and-run-context-handling","text":"To build and run Context Handling, the following software is required: Java (at least version 11) Apache Maven (at least version 3.5.4) Docker (for running tests and deploying Context Handling on the SmartCLIDE cluster) docker-compose (for running local sample instance only)","title":"Preconditions to build and run Context Handling"},{"location":"context-handling/#how-to-build-context-handling","text":"Context Handling can be built using maven with the following command: shell mvn install In order to build and push a container image that can be deployed, the following command can be used: shell mvn install mvn jib:build -pl smartclide-monitoring -Djib.to.image=\"${IMAGE_NAME:IMAGE_TAG}\" -Djib.to.auth.username=\"${CONTAINER_REGISTRY_USERNAME}\" -Djib.to.auth.password=\"${CONTAINER_REGISTRY_TOKEN}\"","title":"How to build Context Handling"},{"location":"context-handling/#how-to-run-context-handling","text":"A sample configuration and docker-compose file can be found in the samples folder . You can run the sample with the following command: shell docker-compose -f samples/docker-compose.yml up","title":"How to run Context Handling"},{"location":"context-handling/#how-to-configure-context-handling","text":"","title":"How to configure Context Handling"},{"location":"context-handling/#monitoring-config","text":"monitoring-config.xml An example monitoring configuration can be found here: monitoring-config.xml monitoring-config.xsd The corresponding XSD file can be found here: monitoring-config.xsd","title":"Monitoring Config"},{"location":"context-handling/#description","text":"","title":"Description"},{"location":"context-handling/#indexes","text":"Each index entry has the following mandatory attributes id: The unique name of the index location: The URI of the location the index is stored","title":"indexes"},{"location":"context-handling/#datasources","text":"Each datasource entry has the following mandatory attributes id:The unique name of the datasource type:The type of the datasource. Possible values are: filesystem, webservice, database, messageBroker monitor:The class of the monitor to be used. Possible values are: package de.atb.context.monitoring.monitors.database.DatabaseMonitor package de.atb.context.monitoring.monitors.file.FileSystemMonitor package de.atb.context.monitoring.monitors.file.FilePairSystemMonitor package de.atb.context.monitoring.monitors.file.FileTripletSystemMonitor package de.atb.context.monitoring.monitors.webservice.MessageBrokerMonitor package de.atb.context.monitoring.monitors.webservice.WebServiceMonitor package de.atb.context.monitoring.monitors.GitlabCommitMonitor package de.atb.context.monitoring.monitors.GitMonitor options: Options for the datasource can be entered using this value. The options are dependent on the datasource to be used uri:The uri of the data source to be monitored class:The following datasource implementations are available package de.atb.context.monitoring.config.models.datasources.DatabaseDataSource package de.atb.context.monitoring.config.models.datasources.FilePairSystemDataSource package de.atb.context.monitoring.config.models.datasources.FileSystemDataSource package de.atb.context.monitoring.config.models.datasources.FileTripletSystemDataSource package de.atb.context.monitoring.config.models.datasources.MessageBrokerDataSource package de.atb.context.monitoring.config.models.datasources.WebServiceDataSource package de.atb.context.monitoring.config.models.datasources.GitlabDataSource","title":"datasources"},{"location":"context-handling/#interpreters","text":"Each interpreter entry has the following mandatory attributes id: The unique name of the interpreter configuration analyser: The analyser class to be used. The following implementations are available: package de.atb.context.monitoring.analyser.database.DatabaseAnalyser package de.atb.context.monitoring.analyser.file.FileAnalyser package de.atb.context.monitoring.analyser.file.FilePairAnalyser package de.atb.context.monitoring.analyser.file.FileTripletAnalyser package de.atb.context.monitoring.analyser.webservice.MessageBrokerAnalyser package de.atb.context.monitoring.analyser.webservice.WebServiceAnalyser package de.atb.context.monitoring.analyser.webserviceGitAnalyser package de.atb.context.monitoring.analyser.webservice.GitlabCommitAnalyser parser: The parser class to be used. The following implementations are available: package de.atb.context.monitoring.parser.database.DatabaseParser package de.atb.context.monitoring.parser.file.FileParser package de.atb.context.monitoring.parser.file.FilePairParser package de.atb.context.monitoring.parser.file.FileTripletParser package de.atb.context.monitoring.parser.webservice.MessageBrokerParser package de.atb.context.monitoring.parser.webservice.WebServiceParser package de.atb.context.monitoring.parser.GitlabCommitParser package de.atb.context.monitoring.parser.GitParser type: Currently only used for File analyser and parser. Defines the file extensions to be used.","title":"interpreters"},{"location":"context-handling/#monitors","text":"Each monitor entry has the following mandatory attributes id: The unique name of the monitor datasource: The id of one previously defined datasource (see above) interpreter: The id of one previously defined interpreter (see above) index: The id of one previously defined index (see above)","title":"monitors"},{"location":"design-patterns/","text":"Smartclide Design Pattern Selection SmartCLIDE Design Pattern Selection Frontend Component Preconditions to build and run Design Pattern Selection Frontend To build and run the frontend Design Pattern Selection extension of Theia, the following software is required: Python Node.js with visual studio build tools (this can be selected in the optional tools during the node.js installation or after hand in several ways, ex. with npm, or with visual studio installer) Yarn package manager npm install --global yarn How to build Design Pattern Selection Frontend The Design Pattern Selection Frontend can be built using the following command: yarn How to run Design Pattern Selection Frontend After building the theia extension, you can start a local instance of theia with our extension. Running the browser example yarn start:browser or: yarn rebuild:browser cd browser-app yarn start or: launch Start Browser Backend configuration from VS code. Open http://localhost:3000 in the browser.","title":"Design Pattern Selection Frontend"},{"location":"design-patterns/#smartclide-design-pattern-selection","text":"SmartCLIDE Design Pattern Selection Frontend Component","title":"Smartclide Design Pattern Selection"},{"location":"design-patterns/#preconditions-to-build-and-run-design-pattern-selection-frontend","text":"To build and run the frontend Design Pattern Selection extension of Theia, the following software is required: Python Node.js with visual studio build tools (this can be selected in the optional tools during the node.js installation or after hand in several ways, ex. with npm, or with visual studio installer) Yarn package manager npm install --global yarn","title":"Preconditions to build and run Design Pattern Selection Frontend"},{"location":"design-patterns/#how-to-build-design-pattern-selection-frontend","text":"The Design Pattern Selection Frontend can be built using the following command: yarn","title":"How to build Design Pattern Selection Frontend"},{"location":"design-patterns/#how-to-run-design-pattern-selection-frontend","text":"After building the theia extension, you can start a local instance of theia with our extension.","title":"How to run Design Pattern Selection Frontend"},{"location":"design-patterns/#running-the-browser-example","text":"yarn start:browser or: yarn rebuild:browser cd browser-app yarn start or: launch Start Browser Backend configuration from VS code. Open http://localhost:3000 in the browser.","title":"Running the browser example"},{"location":"frontend/","text":"Smartclide Service Creation Testing SmartCLIDE Service Creation Testing Frontend Component Preconditions to build and run Service Creation Testing Frontend To build and run the frontend Design Pattern Selection extension of Theia, the following software is required: Python Node.js with visual studio build tools (this can be selected in the optional tools during the node.js installation or after hand in several ways, ex. with npm, or with visual studio installer) Yarn package manager npm install --global yarn How to build Service Creation Testing Frontend The Service Creation Testing Frontend can be built using the following command: yarn How to run Service Creation Testing Frontend After building the theia extension, you can start a local instance of theia with our extension. Running the browser example yarn start:browser or: yarn rebuild:browser cd browser-app yarn start or: launch Start Browser Backend configuration from VS code. Open http://localhost:3000 in the browser.","title":"Service Creation Testing Frontend"},{"location":"frontend/#smartclide-service-creation-testing","text":"SmartCLIDE Service Creation Testing Frontend Component","title":"Smartclide Service Creation Testing"},{"location":"frontend/#preconditions-to-build-and-run-service-creation-testing-frontend","text":"To build and run the frontend Design Pattern Selection extension of Theia, the following software is required: Python Node.js with visual studio build tools (this can be selected in the optional tools during the node.js installation or after hand in several ways, ex. with npm, or with visual studio installer) Yarn package manager npm install --global yarn","title":"Preconditions to build and run Service Creation Testing Frontend"},{"location":"frontend/#how-to-build-service-creation-testing-frontend","text":"The Service Creation Testing Frontend can be built using the following command: yarn","title":"How to build Service Creation Testing Frontend"},{"location":"frontend/#how-to-run-service-creation-testing-frontend","text":"After building the theia extension, you can start a local instance of theia with our extension.","title":"How to run Service Creation Testing Frontend"},{"location":"frontend/#running-the-browser-example","text":"yarn start:browser or: yarn rebuild:browser cd browser-app yarn start or: launch Start Browser Backend configuration from VS code. Open http://localhost:3000 in the browser.","title":"Running the browser example"},{"location":"frontend-td-reusability/","text":"Smartclide TD & Reusability SmartCLIDE TD Principal-Interest & Reusability Frontend Component Preconditions to build and run TD Principal-Interest & Reusability Frontend To build and run the frontend of TD Principal-Interest & Reusability, the following software is required: Python Node.js with visual studio build tools (this can be selected in the optional tools during the node.js installation or after hand in several ways, ex. with npm, or with visual studio installer) Yarn package manager npm install --global yarn How to build TD Principal-Interest & Reusability Frontend TD Principal-Interest & Reusability Frontend can be built using the following command: yarn How to run TD Principal-Interest & Reusability Frontend After building the theia extension, you can start a local instance of theia with our extension. Running the browser example yarn start:browser or: yarn rebuild:browser cd browser-app yarn start or: launch Start Browser Backend configuration from VS code. Open http://localhost:3000 in the browser. Running the Electron example yarn start:electron or: yarn rebuild:electron cd electron-app yarn start or: launch Start Electron Backend configuration from VS code.","title":"TD & Reusability Frontend"},{"location":"frontend-td-reusability/#smartclide-td-reusability","text":"SmartCLIDE TD Principal-Interest & Reusability Frontend Component","title":"Smartclide TD &amp; Reusability"},{"location":"frontend-td-reusability/#preconditions-to-build-and-run-td-principal-interest-reusability-frontend","text":"To build and run the frontend of TD Principal-Interest & Reusability, the following software is required: Python Node.js with visual studio build tools (this can be selected in the optional tools during the node.js installation or after hand in several ways, ex. with npm, or with visual studio installer) Yarn package manager npm install --global yarn","title":"Preconditions to build and run TD Principal-Interest &amp; Reusability Frontend"},{"location":"frontend-td-reusability/#how-to-build-td-principal-interest-reusability-frontend","text":"TD Principal-Interest & Reusability Frontend can be built using the following command: yarn","title":"How to build TD Principal-Interest &amp; Reusability Frontend"},{"location":"frontend-td-reusability/#how-to-run-td-principal-interest-reusability-frontend","text":"After building the theia extension, you can start a local instance of theia with our extension.","title":"How to run TD Principal-Interest &amp; Reusability Frontend"},{"location":"frontend-td-reusability/#running-the-browser-example","text":"yarn start:browser or: yarn rebuild:browser cd browser-app yarn start or: launch Start Browser Backend configuration from VS code. Open http://localhost:3000 in the browser.","title":"Running the browser example"},{"location":"frontend-td-reusability/#running-the-electron-example","text":"yarn start:electron or: yarn rebuild:electron cd electron-app yarn start or: launch Start Electron Backend configuration from VS code.","title":"Running the Electron example"},{"location":"runtime-monitoring-verification/","text":"SmartCLIDE-RMV Requirements The third-party libraries and utilities required are: How to Build RMV Component Install SWI Prolog Building RMV tool and RMV server How to run RMV Component Configuration Running the RMV Tool Running the RMV Server RMV Sub-components Monitor Creation Usage Property Monitor Usage Monitor Sensor Usage Monitoring Services Usage Logging and Notification Usage Security Auditing Usage","title":"SmartCLIDE-RMV"},{"location":"runtime-monitoring-verification/#smartclide-rmv","text":"","title":"SmartCLIDE-RMV"},{"location":"runtime-monitoring-verification/#requirements","text":"The third-party libraries and utilities required are:","title":"Requirements"},{"location":"runtime-monitoring-verification/#how-to-build-rmv-component","text":"","title":"How to Build RMV Component"},{"location":"runtime-monitoring-verification/#install-swi-prolog","text":"","title":"Install SWI Prolog"},{"location":"runtime-monitoring-verification/#building-rmv-tool-and-rmv-server","text":"","title":"Building RMV tool and RMV server"},{"location":"runtime-monitoring-verification/#how-to-run-rmv-component","text":"","title":"How to run RMV Component"},{"location":"runtime-monitoring-verification/#configuration","text":"","title":"Configuration"},{"location":"runtime-monitoring-verification/#running-the-rmv-tool","text":"","title":"Running the RMV Tool"},{"location":"runtime-monitoring-verification/#running-the-rmv-server","text":"","title":"Running the RMV Server"},{"location":"runtime-monitoring-verification/#rmv-sub-components","text":"","title":"RMV Sub-components"},{"location":"runtime-monitoring-verification/#monitor-creation","text":"","title":"Monitor Creation"},{"location":"runtime-monitoring-verification/#usage","text":"","title":"Usage"},{"location":"runtime-monitoring-verification/#property-monitor","text":"","title":"Property Monitor"},{"location":"runtime-monitoring-verification/#usage_1","text":"","title":"Usage"},{"location":"runtime-monitoring-verification/#monitor-sensor","text":"","title":"Monitor Sensor"},{"location":"runtime-monitoring-verification/#usage_2","text":"","title":"Usage"},{"location":"runtime-monitoring-verification/#monitoring-services","text":"","title":"Monitoring Services"},{"location":"runtime-monitoring-verification/#usage_3","text":"","title":"Usage"},{"location":"runtime-monitoring-verification/#logging-and-notification","text":"","title":"Logging and Notification"},{"location":"runtime-monitoring-verification/#usage_4","text":"","title":"Usage"},{"location":"runtime-monitoring-verification/#security-auditing","text":"","title":"Security Auditing"},{"location":"runtime-monitoring-verification/#usage_5","text":"","title":"Usage"},{"location":"service-creation-backend/","text":"SmartCLIDE Service Creation SmartCLIDE Service Creation Backend Component Preconditions to build and run Service Creation To build and run the backend service of Service Creation, the following software is required: Java (at least version 8) Apache Maven (at least version 3.2+) Docker (for building and running the final image) How to build Service Creation Service Creation can be built using maven with the following command: mvn install In order to build a Docker image of the service that can be deployed, the following commands can be used: mvn install docker build -t ${IMAGE_NAME:IMAGE_TAG} . More specifically: mvn install docker build -t smartclide-service-creation:latest . How to run Service Creation All the images of this component can be found here . You can run the backend service with the following command: docker run smartclide-service-creation:latest How to use Service Creation This backend service leverages the provided GitLab and Jenkins APIs in order to authenticate a User and create a new repository based on the options selected by the User. Furthermore, depending on the User\u2019s choices, the newly created repository can be paired with a Jenkins CI/CD. The component creates a new Jenkins CI/CD pipeline and then performs the necessary configuration actions (webhooks, ect.) in order to complete the pairing process. As a result, the process is completed automatically, thus sparing the User from the manual use and configuration of the external tools. The service includes two endpoints: Endpoint 1: \"/createStructure\" -> creates a GitLab repository. Request parameters: projectName -> String projVisibility -> String projDescription -> String gitLabServerURL -> String gitlabToken -> String Endpoint 2: \"/createStuctureJenkins\" -> creates a GitLab repository and a Jenkins pipeline and finaly configures and pairs them. Request parameters: projectName -> String projVisibility -> String projDescription -> String gitLabServerURL -> String gitlabToken -> String jenkinsServerUrl -> String jenkinsUsername -> String jenkinsToken -> String","title":"Service Creation Backend"},{"location":"service-creation-backend/#smartclide-service-creation","text":"SmartCLIDE Service Creation Backend Component","title":"SmartCLIDE Service Creation"},{"location":"service-creation-backend/#preconditions-to-build-and-run-service-creation","text":"To build and run the backend service of Service Creation, the following software is required: Java (at least version 8) Apache Maven (at least version 3.2+) Docker (for building and running the final image)","title":"Preconditions to build and run Service Creation"},{"location":"service-creation-backend/#how-to-build-service-creation","text":"Service Creation can be built using maven with the following command: mvn install In order to build a Docker image of the service that can be deployed, the following commands can be used: mvn install docker build -t ${IMAGE_NAME:IMAGE_TAG} . More specifically: mvn install docker build -t smartclide-service-creation:latest .","title":"How to build Service Creation"},{"location":"service-creation-backend/#how-to-run-service-creation","text":"All the images of this component can be found here . You can run the backend service with the following command: docker run smartclide-service-creation:latest","title":"How to run Service Creation"},{"location":"service-creation-backend/#how-to-use-service-creation","text":"This backend service leverages the provided GitLab and Jenkins APIs in order to authenticate a User and create a new repository based on the options selected by the User. Furthermore, depending on the User\u2019s choices, the newly created repository can be paired with a Jenkins CI/CD. The component creates a new Jenkins CI/CD pipeline and then performs the necessary configuration actions (webhooks, ect.) in order to complete the pairing process. As a result, the process is completed automatically, thus sparing the User from the manual use and configuration of the external tools. The service includes two endpoints: Endpoint 1: \"/createStructure\" -> creates a GitLab repository. Request parameters: projectName -> String projVisibility -> String projDescription -> String gitLabServerURL -> String gitlabToken -> String Endpoint 2: \"/createStuctureJenkins\" -> creates a GitLab repository and a Jenkins pipeline and finaly configures and pairs them. Request parameters: projectName -> String projVisibility -> String projDescription -> String gitLabServerURL -> String gitlabToken -> String jenkinsServerUrl -> String jenkinsUsername -> String jenkinsToken -> String","title":"How to use Service Creation"},{"location":"service-discovery/","text":"Service Discovery API - SmartCLIDE Maintainer: @dabm-git - AIR Institute Configure This package relies on tokens from GitGub, GitLab and BitBucket APIs that are configured in the config.ini file in the root of the service. The service also depends on an instance of Elasticsearch to store and collect information, where the configuration of the IP, port and credentials are done in this same file. See: https://github.com/eclipse-researchlabs/smartclide-service-discovery-poc/blob/main/ServiceDiscovery/config.ini The service makes use of the 2020 port, be sure to expose it. Build python3 -m pip install --no-cache-dir -r requirements.txt python3 -m pip install . --upgrade Or build the image with the provided dockerfile. Run python3 servicediscovery Or using the built docker image hosted by ghcr.io, with docker-compose. docker-compose up version: '3' services: service_discovery: restart: unless-stopped image: ghcr.io/eclipse-researchlabs/smartclide/service-discovery:2022-04-04 working_dir: /app/smartclide-service-discovery-poc/ServiceDiscovery command: python3 ServiceDiscovery ports: - \"2020:2020\" Be sure to replace the necessary configuration in the config.ini file, to do this you can overwrite it with the following section in docker-compose.yml: volumes: -./config.ini:/app/smartclide-service-discovery-poc/ServiceDiscovery/config.ini Web service listings - SmartCLIDE Maintainer: @dabm-git - AIR Institute Run This script collects information from Programableweb, to avoid saturation and blocking of IP addresses by a high number of requests, this script can be executed on a regular basis, obtaining a section each time. When launched, information is collected in batches that are exported to .csv files, and when a batch is finished, the results are merged under the same .csv file.","title":"Service Discovery"},{"location":"service-discovery/#service-discovery-api-smartclide","text":"Maintainer: @dabm-git - AIR Institute","title":"Service Discovery API - SmartCLIDE"},{"location":"service-discovery/#configure","text":"This package relies on tokens from GitGub, GitLab and BitBucket APIs that are configured in the config.ini file in the root of the service. The service also depends on an instance of Elasticsearch to store and collect information, where the configuration of the IP, port and credentials are done in this same file. See: https://github.com/eclipse-researchlabs/smartclide-service-discovery-poc/blob/main/ServiceDiscovery/config.ini The service makes use of the 2020 port, be sure to expose it.","title":"Configure"},{"location":"service-discovery/#build","text":"python3 -m pip install --no-cache-dir -r requirements.txt python3 -m pip install . --upgrade Or build the image with the provided dockerfile.","title":"Build"},{"location":"service-discovery/#run","text":"python3 servicediscovery Or using the built docker image hosted by ghcr.io, with docker-compose. docker-compose up version: '3' services: service_discovery: restart: unless-stopped image: ghcr.io/eclipse-researchlabs/smartclide/service-discovery:2022-04-04 working_dir: /app/smartclide-service-discovery-poc/ServiceDiscovery command: python3 ServiceDiscovery ports: - \"2020:2020\" Be sure to replace the necessary configuration in the config.ini file, to do this you can overwrite it with the following section in docker-compose.yml: volumes: -./config.ini:/app/smartclide-service-discovery-poc/ServiceDiscovery/config.ini","title":"Run"},{"location":"service-discovery/#web-service-listings-smartclide","text":"Maintainer: @dabm-git - AIR Institute","title":"Web service listings - SmartCLIDE"},{"location":"service-discovery/#run_1","text":"This script collects information from Programableweb, to avoid saturation and blocking of IP addresses by a high number of requests, this script can be executed on a regular basis, obtaining a section each time. When launched, information is collected in batches that are exported to .csv files, and when a batch is finished, the results are merged under the same .csv file.","title":"Run"},{"location":"smart-assistant/","text":"Smartclide-DLE The SmartCLIDE DLE and smart assistant has brought together the IDE assistant features within one component. Proposed models try to provide a learning algorithm with the information that data carries, including internal data history and external online web services identified from online resources. After providing AI models, the smart assistant and DLE models are deployed as APIs REST encapsulated in a Python package, which guarantees the portability of this component between Operating Systems. Afterward, to expose the component functionality, we have chosen to visualize these requests and responses through the API swagger, which consists of an interactive web interface. Requirements The list of the third-party library are listed on requirments.txt files in each sub-components; however, the two main used library and requirements are: Python 3.7+ Pytorch HuggingFace scikit-learn Note: The minimum requirement for installing each transformer learning models using this package is 30GB of disk storage, 2vCPU, 4GB RAM. The reason of disk storage is during package installation, and it uses temp storage and packages like a torch, which exceeds more spaces during the installation process. To use less storage, you can disable caching behavior by using --no-cache-dir in pip install command. more info How to Build DLE component In SmartCLIDE platform, trained models need a gateway between the trained models and user interfaces. In this regard, the smart-assistant will support this option through Flask-restx APIs developed, which serve SmartCLIDE DLE (Deep Learning Engine) and Smart Assistant. Moreover, some statistical models are supported by smart-assistant as well.In this regard, DLE needs to install both trained models sub-components and also API gateway. API Gateway Installation Install prerequisites : sudo python3 -m pip install torch==1.5.1+cpu torchvision==0.6.1+cpu -f https://download.pytorch.org/whl/torch_stable.html sudo pyton3 -m pip install git+https://github.com/Dih5/zadeh install smartclid getway: sudo apt update sudo apt install python3 python3-pip nodejs npm -y sudo npm install -g pm2 After installation, SmartCLIDE smart-assistant provides the API specification using Swagger specification on \"http:// /dle\", and \"http:// /iamodeler\". In summary, the available end-points are: - http:// /dle/codegen - http:// /dle/acceptance - http:// /dle/environment - http:// /dle/templatecodegen - http:// /dle/serviceclassification - http:// /dle/bpmnitemrecommendation - http:// /dle/predictivemodeltoolassistant - http:// /iamodeler/classification/bayes - http:// /iamodeler/classification/extra-trees - http:// /iamodeler/supervised/classification/forest - http:// /iamodeler/supervised/classification/gradient - http:// /iamodeler/supervised/classification/logistic - http:// /iamodeler/supervised/classification/mlp - http:// /iamodeler/supervised/classification/neighbors - http:// /iamodeler/supervised/classification/sv - http:// /iamodeler/supervised/classification/tree - http:// /iamodeler/supervised/regression/gradient - http:// /iamodeler/supervised/regression/linear - http:// /iamodeler/supervised/regression/mlp - http:// /iamodeler/supervised/regression/neighbors - http:// /iamodeler/supervised/regression/sv - http:// /iamodeler/supervised/regression/tree Sub-component Quick Installation The trained models have been packaged using the Python Setuptools library. Therefore, this component need to install the related package by cloning the package, browsing to main directory, and executing \u201cpython3 -m pip install . --upgrade\u201d command. git clone https://github.com/eclipse-researchlabs/smartclide-smart-assistant.git cd smartclide-dle-models/<sub-component> python3 -m pip install . --upgrade How to run DLE Component Configuration The application configuration is set via enviroment variables: SA_API_PORT : Port to bind to (default: 5001 ). SA_API_BIND : Address to bind to (default: 0.0.0.0 ). SA_MONGODB_PORT : MongoDB database to connect to (default: 27017 ). SA_MONGODB_HOST : MongoDB database to connect to (default: localhost ). SA_MONGODB_USER : MongoDB user to connect to db (default: user ). SA_MONGODB_PASSWROD : MongoDB password to connect to db (default: password ). SA_MONGODB_DB : MongoDB database to connect to (default: smartclide-smart-assistant ). DLE_BASE_URL : Base URL for DLE connection (default: http://smartclide.ddns.net:5001/smartclide/v1/dle ). SMART_ASSISTANT_BASE_URL : Base URL for Smart Assistant RabbitMQ connection (default: http://smartclide.ddns.net:5000/smartclide/v1/smartassistant ). RABBITMQ_HOST : RabbitMQ connection string host (default: localhost ). RABBITMQ_PORT : RabbitMQ connection string port (default: 5672 ). RABBITMQ_USER : RabbitMQ connection string user (default: user ). RABBITMQ_PASSWORD : RabbitMQ connection string password (default: password ). RABBITMQ_MAPPINGS : RabbitMQ mappings between queue and API's endpoint to connect to. (default: { 'acceptance_tests_queue': '{SMART_ASSISTANT_BASE_URL}/acceptance', 'bpmn_item_recommendation_queue': '{SMART_ASSISTANT_BASE_URL}/bpmnitemrecommendation', 'code_generation_queue': '{SMART_ASSISTANT_BASE_URL}/codegen', 'code_repo_recommendation_queue': '{SMART_ASSISTANT_BASE_URL}/coderepo', 'enviroment_queue': '{SMART_ASSISTANT_BASE_URL}/enviroment' } ). Note: All of them are prefixed with {SMART_ASSISTANT_BASE_URL}/ before start the connection. Run application Application can be launched with the launch script: sudo bash launch.bash Or using PM2: sudo pm2 start pm2.json Note: if the script launch.bash doesn't works, you can use launch2.bash instead. DLE Sub-components SmartCLIDE primarily works with text data, therefore, these components have the advantage of text processing trends and deep learning methods. The earlier approaches mostly combined key-word based feature engineering and traditional ML. However, the keyword-based approaches such as BoW mostly use one- hot encoded vectors, which are high-dimensional and sparse. The emergence of word-embedding techniques has im- proved keyword-based feature engineering. Additionally, the increasing word embedding of open-source projects such as Glove , word2vec , BERT , GPT2 help the fast and efficient low-dimensional representation of text data. Thus, despite these technologies being resource-demanding, SmartcLIDE considered them for some key functinalities. Service classification model Smartclide provides an environment to support the development of service-oriented softwares. The goal of this service classification is to classify the same web services based on their functionality which can be helpful in later stages such as service composition. The trained models have been packaged using the Python Setuptools library. Therefore, this component need to install the related package by cloning the package, browsing to main directory, and executing \u201cpython3 -m pip install . --upgrade\u201d command. git clone https://github.com/eclipse-researchlabs/smartclide-smart-assistant.git cd smartclide-dle-models/serviceclassification python3 -m pip install . --upgrade Testing the module installation python3 servclassify/examples/classify_service.py Usage This library provides two trained models; first, the prediction by ML model. Second, predict using the DL model; the default configuration uses the ML model, which is lighter. You can select method=\"Default\" for using ML model or method= 'Advanced' for using DL model. However, the \"AIPipelineConfiguration\" class is configured for Default mode; for using method= 'Advanced', you need to change the configuration in the AIPipelineConfiguration file to set service_classification_method= 'Advanced' in AIPipelineConfiguration.py and reinstall the package. Simple Usage from servclassify import PredictServiceClassModel service_name=\"service name text\" service_desc=\"find the distination on map\" method=\"Default\" predict_service_obj = PredictServiceClassModel() result = predict_service_obj.predict(service_name, service_description, method=method) print(result) The above class demonstrate using service classification interface class whuich is PredictServiceClassModel. After defining this class we can use it for predicting service class: {'result': [{'Service_name': 'service name text', 'Method': 'Default', 'Service_id': None, 'Service_class': ['Mapping', '']}]} \u2728Note \u2728 The advanced method will return the top 2 categories assigned to service metadata input. the format of output will be: {'result': [{'Service_name': 'service name text', 'Method': 'Default', 'Service_id': None, 'Service_class': ['Mapping', 'Transportation']}]} Singleton Classes Usage In SmartCLIDE, many tasks require to run in the background independently of the user interface (UI). AI Models is one of these tasks that need to serve requests in real-time and return results. Consequently, loading the AI model can be time-consuming due to late response. A strategy such as using singleton classes for loading the models can help minimize the application UI load, improve availability, and reduce interactive response times. from typing import Tuple from typing import List from servclassify import PredictServiceClassModel class Classify_service: def __init__(self): ''' The DL models input parameter for PredictServiceClassModel mention loading service model ''' self.predict_service_obj = PredictServiceClassModel() def predict(self, service_id: str, service_name: str, service_description: str, method:str = 'Default') -> Tuple[str,str]: # predict result = self.predict_service_obj.predict(service_name, service_description, method=method) return result #Loading model recommended to execute on background model2 = Classify_service() service_id=1 service_name=\"service name text\" service_desc=\"find the distination on map\" method=\"Advanced\" result=model2.predict(service_id,service_name, service_desc,method) print(result) You can find the example code which are in python script in the example folder. Code completion model This subcomponent is responsible for generating code based on internal templates. The API returns related code snippets based on templates to implement the workflow represented in BPMN in low code. The first version of this API is designed for finding Java codes. The trained models have been packaged using the Python Setuptools library. Therefore, this component need to install the related package by cloning the package, browsing to main directory, and executing \u201cpython3 -m pip install . --upgrade\u201d command. git clone https://github.com/eclipse-researchlabs/smartclide-smart-assistant.git cd smartclide-dle-models/codeautocomplete python3 -m pip install . --upgrade Testing the module installation python3 servcodegen/examples/generate_code.py Usage This library provides code-generator which uses language modeling, and after installation, the library can be used by importing the package. The model predicts the next tokens based on user input; in order to have better results, the following recommendation need to be considered: Max_sugges_line specifies max line suggestion; recommended value is between 1-3. Max_lenth specifies max length line suggestion, and the recommended value is between 15-20 Use Singletone call for acceptable response time, which this method is explained in the next section. Handling client requests need access to sufficient computing infrastructure. Therefore, it suggests calling code to autocomplete when the user uses \"Tab\" or \"Dot.\" Simple Usage from servcodegen import AutocompleteCodeModel model = AutocompleteCodeModel() method=\"GPT-2\" lang=\"java\" max_lenth=20 max_sugges_line=3 code_input=\"import android.\" result=model.generateCode(code_input, max_lenth, max_sugges_line,method) print(result) The above code demonstrate using servcodegen interface class whuich is AutocompleteCodeModel. the result will be {'result': {'code_sugg': ['import android.os.Bundle ;', 'import android.content.Intent ;', 'import android.content.Context ;'], 'Method': 'GPT-2', 'codeSuggLen': 20, 'codeSuggLines': 3, 'language': 'java'}} \u2728Note \u2728 loding model recommended to execute on background which is explained on singletone classes usage in below. Singleton classes Usage In SmartCLIDE, many tasks require to run in the background independently of the user interface (UI). AI Models is one of these tasks that need to serve requests in real-time and return results. Consequently, loading the AI model can be time-consuming due to late response. A strategy such as using singleton classes for loading the models can help minimize the application UI load, improve availability, and reduce interactive response times. from typing import Tuple from typing import List from servcodegen import AutocompleteCodeModel class CodeCompletion: def __init__(self): self.model = AutocompleteCodeModel() def predict2(self, method:str, language:str, code_input:str, code_sugg_len:int, code_sugg_lines:int) -> List[str]: # predict result = self.model.generateCode(code_input, code_sugg_len, code_sugg_lines,method) return result #Loading model recommended to execute on background codecomplete_obj = CodeCompletion() #Using loaded model Method=\"GPT-2\" lang=\"java\" max_lenth=20 max_sugges_line=3 code_input=\"file=new\" result=codecomplete_obj.predict2(Method,lang,code_input,max_lenth,max_sugges_line) print(result) Acceptance test suggestions model The acceptance test set suggestion system, based on collaborative filtering techniques, is responsible for providing the user with a set of tests defined in Gherkin format to be applied to the workflow defined in the BPMN and help verify if the expectations are met. The trained models have been packaged using the Python Setuptools library. Therefore, this component need to install the related package by cloning the package, browsing to main directory, and executing \u201cpython3 -m pip install . --upgrade\u201d command. git clone https://github.com/eclipse-researchlabs/smartclide-smart-assistant.git cd smartclide-dle-models/cbr-gherkin-recommendation python3 -m pip install . --upgrade To install also the dependencies to run the tests or to generate the documentation install some of the extras like (Mind the quotes): python3 -m pip install '.[docs]' --upgrade Case database initialization For that purpose, use the following command: python3 initialize_cbr_db.py Usage The main class is CBR wich also needs the clases Casebase, Recovery and Aggregation. You need a frist load with all your base cases. After that first inicial load you can pass an empty array to the class initializer: import pycbr cbr = pycbr.CBR([],\"ghrkn_recommendator\",\"smartclide.ddns.net\") Add case The method to add a case must recibe a dictionary with this format: cbr.add_case({ 'name': \"Sting with the file name\", 'text': \"All the bpmn text\", 'gherkins': [\"list with gherkins text\"] }) Get recommendation The method to get a recommendation must recibe a string with all the bpmn text: cbr.recommend(bpmn_text) >>> { 'gherkins': [[\"List of list with all the recomended gherkins for the first 5 matches\"]], 'sims': [\"List of similarity scores from 0 to 1\"] } Documentation To generate the documentation, the docs extra dependencies must be installed. Furthermore, pandoc must be available in your system. To generate an html documentation with sphinx run: make docs To generate a PDF documentation using LaTeX: make pdf Predictive model tool API This subcomponent utilized the automated machine learning (AutoML) concept, allowing users to define ML actions sequences via an interface. These sequences contain the Predictive model tool APIs, which include 4 primary steps. 1) Importing data 2) Creating a supervised model based on regression or classification Model 3) Performing Prediction based on user input 4) Providing validation matric results which can use for visualization. Installation You probably to set up and use a virtualenv: # Prepare a clean virtualenv and activate it virtualenv -p /usr/bin/python3.6 venv source venv/bin/activate Remember to activate it whenever you are working with the package. To install a development version clone the repo, cd to the directory and: pip install -e . Once installed, the development flask server might be started with the command: iamodeler For real deployment, gunicorn might be used instead: pip install gunicorn unicorn --workers 4 --bind 0.0.0.0:5000 --timeout 600 iamodeler.server:app To use a celery queue system (see configuration below), a celery broker like RabbitMQ must also be installed. With RabbitMQ installed and running, start the queue system by running: celery -A iamodeler.tasks.celery worker Note the gunicorn timeout parameter does not affect the celery queues. In Windows , the default celery pool might not work. You might try to add --pool=eventlet to run it. Configuration Configuration is done with environment variables. Variable Description IAMODELER_STORE Path to the local storage of the models. Defaults to a temporal directory. IAMODELER_CELERY If set and not empty, use a local Celery queue system. IAMODELER_CELERY_BROKER Address of the Celery broker. IAMODELER_AUTH Authentication token for the server. Client request must set X-IAMODELER-AUTH to this token in their headers. IAMODELER_LOG A path to a yaml logging configuration file. Defaults to logging.yaml The paths are relative to the CWD , provide full paths when needed. Pro-tip: A .env file can be used installing the python-dotenv package. An example of logging configuration file is provided in the root of the repo. BPMN Items suggestions This AI-based approach provides recommendations during service composition. The suggestions are based on a selected service composition approach by (BPMN-based work-flow) data representation, existing/history BPMN work-flows, and provided service specification information. Usage This sub-module receives the information of the last selected node in the target BPMN diagram. This information is in JSON format, which can include unique node id and other node metadata such as name or user_id. Afterwards, the query compositor merges it with the incomplete BPMN file , developers are working with. { \"dle\": { \"header\": \"bpmn suggestion\", \"state\": \"query\", \"previous node\": [ { \"id\": \"_13BAF867-3CA8-4C6F-85C6-D3FD748D07D2\" }, { \"name\": \"UserFound?\" } ] } } The module performs four main steps on the received JSON request, which are: 1) Query Compositor 2) Current BPMN Extractor 3) BPMN semantic identifier 4) Numerical vector transformer and finally suggesting nexrtBPMN node which will be in JSON response format: { \"dle\": { \"header\": \"bpmn suggestion\", \"state\": \"true\", \"previous node\": [ { \"id\": \"_13BAF867-3CA8-4C6F-85C6-D3FD748D07D2\" }, { \"name\": \"UserFound?\" } ], \"suggestionnode\": [ { \"id\": \"_E5D17755-D671-43ED-BD7D-F6538933069C\" }, { \"name\": \"AuditUser\" } ] } } { \"dle\": { \"header\": \"bpmnsuggestion\", \"state\": \"false\", \"previousnode\": [ { \"id\": \"_13BAF867-3CA8-4C6F-85C6-D3FD748D07D2\" }, { \"name\": \"UserFound?\" } ] } }","title":"Smartclide-DLE"},{"location":"smart-assistant/#smartclide-dle","text":"The SmartCLIDE DLE and smart assistant has brought together the IDE assistant features within one component. Proposed models try to provide a learning algorithm with the information that data carries, including internal data history and external online web services identified from online resources. After providing AI models, the smart assistant and DLE models are deployed as APIs REST encapsulated in a Python package, which guarantees the portability of this component between Operating Systems. Afterward, to expose the component functionality, we have chosen to visualize these requests and responses through the API swagger, which consists of an interactive web interface.","title":"Smartclide-DLE"},{"location":"smart-assistant/#requirements","text":"The list of the third-party library are listed on requirments.txt files in each sub-components; however, the two main used library and requirements are: Python 3.7+ Pytorch HuggingFace scikit-learn Note: The minimum requirement for installing each transformer learning models using this package is 30GB of disk storage, 2vCPU, 4GB RAM. The reason of disk storage is during package installation, and it uses temp storage and packages like a torch, which exceeds more spaces during the installation process. To use less storage, you can disable caching behavior by using --no-cache-dir in pip install command. more info","title":"Requirements"},{"location":"smart-assistant/#how-to-build-dle-component","text":"In SmartCLIDE platform, trained models need a gateway between the trained models and user interfaces. In this regard, the smart-assistant will support this option through Flask-restx APIs developed, which serve SmartCLIDE DLE (Deep Learning Engine) and Smart Assistant. Moreover, some statistical models are supported by smart-assistant as well.In this regard, DLE needs to install both trained models sub-components and also API gateway.","title":"How to Build DLE component"},{"location":"smart-assistant/#api-gateway-installation","text":"Install prerequisites : sudo python3 -m pip install torch==1.5.1+cpu torchvision==0.6.1+cpu -f https://download.pytorch.org/whl/torch_stable.html sudo pyton3 -m pip install git+https://github.com/Dih5/zadeh install smartclid getway: sudo apt update sudo apt install python3 python3-pip nodejs npm -y sudo npm install -g pm2 After installation, SmartCLIDE smart-assistant provides the API specification using Swagger specification on \"http:// /dle\", and \"http:// /iamodeler\". In summary, the available end-points are: - http:// /dle/codegen - http:// /dle/acceptance - http:// /dle/environment - http:// /dle/templatecodegen - http:// /dle/serviceclassification - http:// /dle/bpmnitemrecommendation - http:// /dle/predictivemodeltoolassistant - http:// /iamodeler/classification/bayes - http:// /iamodeler/classification/extra-trees - http:// /iamodeler/supervised/classification/forest - http:// /iamodeler/supervised/classification/gradient - http:// /iamodeler/supervised/classification/logistic - http:// /iamodeler/supervised/classification/mlp - http:// /iamodeler/supervised/classification/neighbors - http:// /iamodeler/supervised/classification/sv - http:// /iamodeler/supervised/classification/tree - http:// /iamodeler/supervised/regression/gradient - http:// /iamodeler/supervised/regression/linear - http:// /iamodeler/supervised/regression/mlp - http:// /iamodeler/supervised/regression/neighbors - http:// /iamodeler/supervised/regression/sv - http:// /iamodeler/supervised/regression/tree","title":"API Gateway Installation"},{"location":"smart-assistant/#sub-component-quick-installation","text":"The trained models have been packaged using the Python Setuptools library. Therefore, this component need to install the related package by cloning the package, browsing to main directory, and executing \u201cpython3 -m pip install . --upgrade\u201d command. git clone https://github.com/eclipse-researchlabs/smartclide-smart-assistant.git cd smartclide-dle-models/<sub-component> python3 -m pip install . --upgrade","title":"Sub-component Quick Installation"},{"location":"smart-assistant/#how-to-run-dle-component","text":"","title":"How to run DLE Component"},{"location":"smart-assistant/#configuration","text":"The application configuration is set via enviroment variables: SA_API_PORT : Port to bind to (default: 5001 ). SA_API_BIND : Address to bind to (default: 0.0.0.0 ). SA_MONGODB_PORT : MongoDB database to connect to (default: 27017 ). SA_MONGODB_HOST : MongoDB database to connect to (default: localhost ). SA_MONGODB_USER : MongoDB user to connect to db (default: user ). SA_MONGODB_PASSWROD : MongoDB password to connect to db (default: password ). SA_MONGODB_DB : MongoDB database to connect to (default: smartclide-smart-assistant ). DLE_BASE_URL : Base URL for DLE connection (default: http://smartclide.ddns.net:5001/smartclide/v1/dle ). SMART_ASSISTANT_BASE_URL : Base URL for Smart Assistant RabbitMQ connection (default: http://smartclide.ddns.net:5000/smartclide/v1/smartassistant ). RABBITMQ_HOST : RabbitMQ connection string host (default: localhost ). RABBITMQ_PORT : RabbitMQ connection string port (default: 5672 ). RABBITMQ_USER : RabbitMQ connection string user (default: user ). RABBITMQ_PASSWORD : RabbitMQ connection string password (default: password ). RABBITMQ_MAPPINGS : RabbitMQ mappings between queue and API's endpoint to connect to. (default: { 'acceptance_tests_queue': '{SMART_ASSISTANT_BASE_URL}/acceptance', 'bpmn_item_recommendation_queue': '{SMART_ASSISTANT_BASE_URL}/bpmnitemrecommendation', 'code_generation_queue': '{SMART_ASSISTANT_BASE_URL}/codegen', 'code_repo_recommendation_queue': '{SMART_ASSISTANT_BASE_URL}/coderepo', 'enviroment_queue': '{SMART_ASSISTANT_BASE_URL}/enviroment' } ). Note: All of them are prefixed with {SMART_ASSISTANT_BASE_URL}/ before start the connection.","title":"Configuration"},{"location":"smart-assistant/#run-application","text":"Application can be launched with the launch script: sudo bash launch.bash Or using PM2: sudo pm2 start pm2.json Note: if the script launch.bash doesn't works, you can use launch2.bash instead.","title":"Run application"},{"location":"smart-assistant/#dle-sub-components","text":"SmartCLIDE primarily works with text data, therefore, these components have the advantage of text processing trends and deep learning methods. The earlier approaches mostly combined key-word based feature engineering and traditional ML. However, the keyword-based approaches such as BoW mostly use one- hot encoded vectors, which are high-dimensional and sparse. The emergence of word-embedding techniques has im- proved keyword-based feature engineering. Additionally, the increasing word embedding of open-source projects such as Glove , word2vec , BERT , GPT2 help the fast and efficient low-dimensional representation of text data. Thus, despite these technologies being resource-demanding, SmartcLIDE considered them for some key functinalities.","title":"DLE Sub-components"},{"location":"smart-assistant/#service-classification-model","text":"Smartclide provides an environment to support the development of service-oriented softwares. The goal of this service classification is to classify the same web services based on their functionality which can be helpful in later stages such as service composition. The trained models have been packaged using the Python Setuptools library. Therefore, this component need to install the related package by cloning the package, browsing to main directory, and executing \u201cpython3 -m pip install . --upgrade\u201d command. git clone https://github.com/eclipse-researchlabs/smartclide-smart-assistant.git cd smartclide-dle-models/serviceclassification python3 -m pip install . --upgrade Testing the module installation python3 servclassify/examples/classify_service.py","title":"Service classification model"},{"location":"smart-assistant/#usage","text":"This library provides two trained models; first, the prediction by ML model. Second, predict using the DL model; the default configuration uses the ML model, which is lighter. You can select method=\"Default\" for using ML model or method= 'Advanced' for using DL model. However, the \"AIPipelineConfiguration\" class is configured for Default mode; for using method= 'Advanced', you need to change the configuration in the AIPipelineConfiguration file to set service_classification_method= 'Advanced' in AIPipelineConfiguration.py and reinstall the package.","title":"Usage"},{"location":"smart-assistant/#simple-usage","text":"from servclassify import PredictServiceClassModel service_name=\"service name text\" service_desc=\"find the distination on map\" method=\"Default\" predict_service_obj = PredictServiceClassModel() result = predict_service_obj.predict(service_name, service_description, method=method) print(result) The above class demonstrate using service classification interface class whuich is PredictServiceClassModel. After defining this class we can use it for predicting service class: {'result': [{'Service_name': 'service name text', 'Method': 'Default', 'Service_id': None, 'Service_class': ['Mapping', '']}]} \u2728Note \u2728 The advanced method will return the top 2 categories assigned to service metadata input. the format of output will be: {'result': [{'Service_name': 'service name text', 'Method': 'Default', 'Service_id': None, 'Service_class': ['Mapping', 'Transportation']}]}","title":"Simple Usage"},{"location":"smart-assistant/#singleton-classes-usage","text":"In SmartCLIDE, many tasks require to run in the background independently of the user interface (UI). AI Models is one of these tasks that need to serve requests in real-time and return results. Consequently, loading the AI model can be time-consuming due to late response. A strategy such as using singleton classes for loading the models can help minimize the application UI load, improve availability, and reduce interactive response times. from typing import Tuple from typing import List from servclassify import PredictServiceClassModel class Classify_service: def __init__(self): ''' The DL models input parameter for PredictServiceClassModel mention loading service model ''' self.predict_service_obj = PredictServiceClassModel() def predict(self, service_id: str, service_name: str, service_description: str, method:str = 'Default') -> Tuple[str,str]: # predict result = self.predict_service_obj.predict(service_name, service_description, method=method) return result #Loading model recommended to execute on background model2 = Classify_service() service_id=1 service_name=\"service name text\" service_desc=\"find the distination on map\" method=\"Advanced\" result=model2.predict(service_id,service_name, service_desc,method) print(result) You can find the example code which are in python script in the example folder.","title":"Singleton Classes Usage"},{"location":"smart-assistant/#code-completion-model","text":"This subcomponent is responsible for generating code based on internal templates. The API returns related code snippets based on templates to implement the workflow represented in BPMN in low code. The first version of this API is designed for finding Java codes. The trained models have been packaged using the Python Setuptools library. Therefore, this component need to install the related package by cloning the package, browsing to main directory, and executing \u201cpython3 -m pip install . --upgrade\u201d command. git clone https://github.com/eclipse-researchlabs/smartclide-smart-assistant.git cd smartclide-dle-models/codeautocomplete python3 -m pip install . --upgrade Testing the module installation python3 servcodegen/examples/generate_code.py","title":"Code completion model"},{"location":"smart-assistant/#usage_1","text":"This library provides code-generator which uses language modeling, and after installation, the library can be used by importing the package. The model predicts the next tokens based on user input; in order to have better results, the following recommendation need to be considered: Max_sugges_line specifies max line suggestion; recommended value is between 1-3. Max_lenth specifies max length line suggestion, and the recommended value is between 15-20 Use Singletone call for acceptable response time, which this method is explained in the next section. Handling client requests need access to sufficient computing infrastructure. Therefore, it suggests calling code to autocomplete when the user uses \"Tab\" or \"Dot.\"","title":"Usage"},{"location":"smart-assistant/#simple-usage_1","text":"from servcodegen import AutocompleteCodeModel model = AutocompleteCodeModel() method=\"GPT-2\" lang=\"java\" max_lenth=20 max_sugges_line=3 code_input=\"import android.\" result=model.generateCode(code_input, max_lenth, max_sugges_line,method) print(result) The above code demonstrate using servcodegen interface class whuich is AutocompleteCodeModel. the result will be {'result': {'code_sugg': ['import android.os.Bundle ;', 'import android.content.Intent ;', 'import android.content.Context ;'], 'Method': 'GPT-2', 'codeSuggLen': 20, 'codeSuggLines': 3, 'language': 'java'}} \u2728Note \u2728 loding model recommended to execute on background which is explained on singletone classes usage in below.","title":"Simple Usage"},{"location":"smart-assistant/#singleton-classes-usage_1","text":"In SmartCLIDE, many tasks require to run in the background independently of the user interface (UI). AI Models is one of these tasks that need to serve requests in real-time and return results. Consequently, loading the AI model can be time-consuming due to late response. A strategy such as using singleton classes for loading the models can help minimize the application UI load, improve availability, and reduce interactive response times. from typing import Tuple from typing import List from servcodegen import AutocompleteCodeModel class CodeCompletion: def __init__(self): self.model = AutocompleteCodeModel() def predict2(self, method:str, language:str, code_input:str, code_sugg_len:int, code_sugg_lines:int) -> List[str]: # predict result = self.model.generateCode(code_input, code_sugg_len, code_sugg_lines,method) return result #Loading model recommended to execute on background codecomplete_obj = CodeCompletion() #Using loaded model Method=\"GPT-2\" lang=\"java\" max_lenth=20 max_sugges_line=3 code_input=\"file=new\" result=codecomplete_obj.predict2(Method,lang,code_input,max_lenth,max_sugges_line) print(result)","title":"Singleton classes Usage"},{"location":"smart-assistant/#acceptance-test-suggestions-model","text":"The acceptance test set suggestion system, based on collaborative filtering techniques, is responsible for providing the user with a set of tests defined in Gherkin format to be applied to the workflow defined in the BPMN and help verify if the expectations are met. The trained models have been packaged using the Python Setuptools library. Therefore, this component need to install the related package by cloning the package, browsing to main directory, and executing \u201cpython3 -m pip install . --upgrade\u201d command. git clone https://github.com/eclipse-researchlabs/smartclide-smart-assistant.git cd smartclide-dle-models/cbr-gherkin-recommendation python3 -m pip install . --upgrade To install also the dependencies to run the tests or to generate the documentation install some of the extras like (Mind the quotes): python3 -m pip install '.[docs]' --upgrade","title":"Acceptance test suggestions model"},{"location":"smart-assistant/#case-database-initialization","text":"For that purpose, use the following command: python3 initialize_cbr_db.py","title":"Case database initialization"},{"location":"smart-assistant/#usage_2","text":"The main class is CBR wich also needs the clases Casebase, Recovery and Aggregation. You need a frist load with all your base cases. After that first inicial load you can pass an empty array to the class initializer: import pycbr cbr = pycbr.CBR([],\"ghrkn_recommendator\",\"smartclide.ddns.net\")","title":"Usage"},{"location":"smart-assistant/#add-case","text":"The method to add a case must recibe a dictionary with this format: cbr.add_case({ 'name': \"Sting with the file name\", 'text': \"All the bpmn text\", 'gherkins': [\"list with gherkins text\"] })","title":"Add case"},{"location":"smart-assistant/#get-recommendation","text":"The method to get a recommendation must recibe a string with all the bpmn text: cbr.recommend(bpmn_text) >>> { 'gherkins': [[\"List of list with all the recomended gherkins for the first 5 matches\"]], 'sims': [\"List of similarity scores from 0 to 1\"] }","title":"Get recommendation"},{"location":"smart-assistant/#documentation","text":"To generate the documentation, the docs extra dependencies must be installed. Furthermore, pandoc must be available in your system. To generate an html documentation with sphinx run: make docs To generate a PDF documentation using LaTeX: make pdf","title":"Documentation"},{"location":"smart-assistant/#predictive-model-tool-api","text":"This subcomponent utilized the automated machine learning (AutoML) concept, allowing users to define ML actions sequences via an interface. These sequences contain the Predictive model tool APIs, which include 4 primary steps. 1) Importing data 2) Creating a supervised model based on regression or classification Model 3) Performing Prediction based on user input 4) Providing validation matric results which can use for visualization.","title":"Predictive model tool API"},{"location":"smart-assistant/#installation","text":"You probably to set up and use a virtualenv: # Prepare a clean virtualenv and activate it virtualenv -p /usr/bin/python3.6 venv source venv/bin/activate Remember to activate it whenever you are working with the package. To install a development version clone the repo, cd to the directory and: pip install -e . Once installed, the development flask server might be started with the command: iamodeler For real deployment, gunicorn might be used instead: pip install gunicorn unicorn --workers 4 --bind 0.0.0.0:5000 --timeout 600 iamodeler.server:app To use a celery queue system (see configuration below), a celery broker like RabbitMQ must also be installed. With RabbitMQ installed and running, start the queue system by running: celery -A iamodeler.tasks.celery worker Note the gunicorn timeout parameter does not affect the celery queues. In Windows , the default celery pool might not work. You might try to add --pool=eventlet to run it.","title":"Installation"},{"location":"smart-assistant/#configuration_1","text":"Configuration is done with environment variables. Variable Description IAMODELER_STORE Path to the local storage of the models. Defaults to a temporal directory. IAMODELER_CELERY If set and not empty, use a local Celery queue system. IAMODELER_CELERY_BROKER Address of the Celery broker. IAMODELER_AUTH Authentication token for the server. Client request must set X-IAMODELER-AUTH to this token in their headers. IAMODELER_LOG A path to a yaml logging configuration file. Defaults to logging.yaml The paths are relative to the CWD , provide full paths when needed. Pro-tip: A .env file can be used installing the python-dotenv package. An example of logging configuration file is provided in the root of the repo.","title":"Configuration"},{"location":"smart-assistant/#bpmn-items-suggestions","text":"This AI-based approach provides recommendations during service composition. The suggestions are based on a selected service composition approach by (BPMN-based work-flow) data representation, existing/history BPMN work-flows, and provided service specification information.","title":"BPMN Items suggestions"},{"location":"smart-assistant/#usage_3","text":"This sub-module receives the information of the last selected node in the target BPMN diagram. This information is in JSON format, which can include unique node id and other node metadata such as name or user_id. Afterwards, the query compositor merges it with the incomplete BPMN file , developers are working with. { \"dle\": { \"header\": \"bpmn suggestion\", \"state\": \"query\", \"previous node\": [ { \"id\": \"_13BAF867-3CA8-4C6F-85C6-D3FD748D07D2\" }, { \"name\": \"UserFound?\" } ] } } The module performs four main steps on the received JSON request, which are: 1) Query Compositor 2) Current BPMN Extractor 3) BPMN semantic identifier 4) Numerical vector transformer and finally suggesting nexrtBPMN node which will be in JSON response format: { \"dle\": { \"header\": \"bpmn suggestion\", \"state\": \"true\", \"previous node\": [ { \"id\": \"_13BAF867-3CA8-4C6F-85C6-D3FD748D07D2\" }, { \"name\": \"UserFound?\" } ], \"suggestionnode\": [ { \"id\": \"_E5D17755-D671-43ED-BD7D-F6538933069C\" }, { \"name\": \"AuditUser\" } ] } } { \"dle\": { \"header\": \"bpmnsuggestion\", \"state\": \"false\", \"previousnode\": [ { \"id\": \"_13BAF867-3CA8-4C6F-85C6-D3FD748D07D2\" }, { \"name\": \"UserFound?\" } ] } }","title":"Usage"}]}