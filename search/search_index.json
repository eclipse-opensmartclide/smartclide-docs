{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to the SmartCLIDE IDE The SmartCLIDE project has received funding from the European Union's Horizon 2020 research and innovation programme under grant agreement No 871177. For more general information about the SmartCLIDE project visit our website . About the project The main objective of SmartCLIDE is to propose a radically new smart cloud-native development environment, based on the coding-by-demonstration principle, that will support creators of cloud services in the discovery, creation, composition, testing, and deployment of full-stack data-centered services and applications in the cloud. SmartCLIDE will provide high levels of abstraction at all stages (development, testing, deployment, and run-time) as well as self-discovery of IaaS and SaaS Services. SmartCLIDE will provide several categories of abstractions: at development stage, SmartCLIDE will provide abstractions on data transformations or processing at testing stage, mechanisms to visualize flow and status or artifacts to automatically test the expected behavior at deployment stage, abstractions of physical and virtual resources or at runtime, mechanisms to monitor the performance and operation of the service The cloud nature of the environment will enable collaboration between different stakeholders, and the self-discovery of IaaS and SaaS services and the high levels of abstraction will facilitate the composition and deployment of new services to nontechnical staff (with no previous experience on programming or on the administration of systems and infrastructure). Equally, hiding the complexity of the infrastructure, and adding intelligence to this layer, will allow selecting the most adequate infrastructure services in each moment. SmartCLIDE will allow SMEs and Public Administration to boost the adoption of Cloud solutions, being validated at one solution-oriented to Public Administration (Social Security System) and three different IoT products of software development SMEs within the consortium. Modules The SmartCLIDE platform consists of the following modules: Context Handling Service Creation Service Creation Service Maintenance Test Generation Design Patterns Service Discovery Smart Assistant (Deep Learning Engine - DLE) Consortium The SmartCLIDE Consortium consists of the following partners: ATB Netcompany-Intrasoft AIR University of Macedonia (UoM) Centre for Research and Technology Hellas (CERTH) The Open Group Eclipse Foundation Wellness Telecom UNPARALLEL CONTACT Software KAIROS Digital Solutions License Distributed under the Eclipse Public License 2.0. See LICENSE for more information.","title":"Home"},{"location":"#welcome-to-the-smartclide-ide","text":"The SmartCLIDE project has received funding from the European Union's Horizon 2020 research and innovation programme under grant agreement No 871177. For more general information about the SmartCLIDE project visit our website .","title":"Welcome to the SmartCLIDE IDE"},{"location":"#about-the-project","text":"The main objective of SmartCLIDE is to propose a radically new smart cloud-native development environment, based on the coding-by-demonstration principle, that will support creators of cloud services in the discovery, creation, composition, testing, and deployment of full-stack data-centered services and applications in the cloud. SmartCLIDE will provide high levels of abstraction at all stages (development, testing, deployment, and run-time) as well as self-discovery of IaaS and SaaS Services. SmartCLIDE will provide several categories of abstractions: at development stage, SmartCLIDE will provide abstractions on data transformations or processing at testing stage, mechanisms to visualize flow and status or artifacts to automatically test the expected behavior at deployment stage, abstractions of physical and virtual resources or at runtime, mechanisms to monitor the performance and operation of the service The cloud nature of the environment will enable collaboration between different stakeholders, and the self-discovery of IaaS and SaaS services and the high levels of abstraction will facilitate the composition and deployment of new services to nontechnical staff (with no previous experience on programming or on the administration of systems and infrastructure). Equally, hiding the complexity of the infrastructure, and adding intelligence to this layer, will allow selecting the most adequate infrastructure services in each moment. SmartCLIDE will allow SMEs and Public Administration to boost the adoption of Cloud solutions, being validated at one solution-oriented to Public Administration (Social Security System) and three different IoT products of software development SMEs within the consortium.","title":"About the project"},{"location":"#modules","text":"The SmartCLIDE platform consists of the following modules: Context Handling Service Creation Service Creation Service Maintenance Test Generation Design Patterns Service Discovery Smart Assistant (Deep Learning Engine - DLE)","title":"Modules"},{"location":"#consortium","text":"The SmartCLIDE Consortium consists of the following partners: ATB Netcompany-Intrasoft AIR University of Macedonia (UoM) Centre for Research and Technology Hellas (CERTH) The Open Group Eclipse Foundation Wellness Telecom UNPARALLEL CONTACT Software KAIROS Digital Solutions","title":"Consortium"},{"location":"#license","text":"Distributed under the Eclipse Public License 2.0. See LICENSE for more information.","title":"License"},{"location":"admin-guide/","text":"SmartCLIDE Admin Guide. Under construction...","title":"SmartCLIDE Admin Guide."},{"location":"admin-guide/#smartclide-admin-guide","text":"Under construction...","title":"SmartCLIDE Admin Guide."},{"location":"concepts/","text":"SmartCLIDE Concepts. Under construction...","title":"SmartCLIDE Concepts."},{"location":"concepts/#smartclide-concepts","text":"Under construction...","title":"SmartCLIDE Concepts."},{"location":"getting-started-developer/","text":"For the developer This page contains some very-easy-to-follow guides for developers to start building services using the SmartCLIDE IDE. Registration and login The first thing a developer needs to do in order to start using the SmartCLIDE IDE, is to navigate to the SmartCLIDE IDE landing page : Then, by hitting the 'SmartCLIDE Eclipse Che' button, the user is redirected to the SmartCLIDE Login Page: From there, the developer hits the \"Register\" button, and is redirected to the SmartCLIDE IDE Registration Page: After the developer has filled in his details, a verification link is sent through email to the email account that he provided, and the following page is presented to the user: The user clicks the verification link sent to him in order to verify his account. Then, he can navigate back to the SmartCLIDE Login Page, enter his credentials and log into the SmartCLIDE IDE. After that, the SmartCLIDE Main Page is presented to the user: Setting up profile After landing to the SmartCLIDE Main Page, the developer can edit his profile by clicking the little human icon in the upper right corner, and then clicking on the \"My Account\" option. By doing this, he is redirected to the SmartCLIDE Profile Editing Page: From the SmartCLIDE Profile Editing Page the user can edit his security credentials (SmartCLIDE uses Keycloak for user authentication and authorization), like email, first name, last name and password. In addition, by clicking on the \"Credentials\" option the user is redirected to a new view where he can add credentials related to source control repositories (e.g., GitHub credentials), CI/CD managers (e.g., Jenkins) and Deployment Platforms. For example, if the user wants to connect his GitHub profile with the SmartCLIDE IDE, he can add his credentials by clicking the '+' icon in the upper right corner, and fill the requested fields in the form that opens (i.e., Type->GitHub, GitHub URL, username and GitHub Personal Access Token). Building the first service","title":"Getting Started"},{"location":"getting-started-developer/#for-the-developer","text":"This page contains some very-easy-to-follow guides for developers to start building services using the SmartCLIDE IDE.","title":"For the developer"},{"location":"getting-started-developer/#registration-and-login","text":"The first thing a developer needs to do in order to start using the SmartCLIDE IDE, is to navigate to the SmartCLIDE IDE landing page : Then, by hitting the 'SmartCLIDE Eclipse Che' button, the user is redirected to the SmartCLIDE Login Page: From there, the developer hits the \"Register\" button, and is redirected to the SmartCLIDE IDE Registration Page: After the developer has filled in his details, a verification link is sent through email to the email account that he provided, and the following page is presented to the user: The user clicks the verification link sent to him in order to verify his account. Then, he can navigate back to the SmartCLIDE Login Page, enter his credentials and log into the SmartCLIDE IDE. After that, the SmartCLIDE Main Page is presented to the user:","title":"Registration and login"},{"location":"getting-started-developer/#setting-up-profile","text":"After landing to the SmartCLIDE Main Page, the developer can edit his profile by clicking the little human icon in the upper right corner, and then clicking on the \"My Account\" option. By doing this, he is redirected to the SmartCLIDE Profile Editing Page: From the SmartCLIDE Profile Editing Page the user can edit his security credentials (SmartCLIDE uses Keycloak for user authentication and authorization), like email, first name, last name and password. In addition, by clicking on the \"Credentials\" option the user is redirected to a new view where he can add credentials related to source control repositories (e.g., GitHub credentials), CI/CD managers (e.g., Jenkins) and Deployment Platforms. For example, if the user wants to connect his GitHub profile with the SmartCLIDE IDE, he can add his credentials by clicking the '+' icon in the upper right corner, and fill the requested fields in the form that opens (i.e., Type->GitHub, GitHub URL, username and GitHub Personal Access Token).","title":"Setting up profile"},{"location":"getting-started-developer/#building-the-first-service","text":"","title":"Building the first service"},{"location":"getting-started-sysadmin/","text":"For the sysadmin This page contains some very-easy-to-follow guides for sysadmins that want to set up and deploy an on-premises instance of the SmartCLIDE IDE. SmartCLIDE Helm Chart Requirements A publicly resolvable domain, like example.org or a subdomain like foo.example.org . You must be able to edit DNS records for that domain. A wildcard TLS certificate for your domain, i.e. for *.example.org or *.foo.example.org . It must be a wildcard certificate! Letsencrypt is not supported at the moment! Save the certificate files in smartclide.full-chain.crt and smartclide.key . smartclide.full-chain.crt must contain the full certificate chain, including intermediate certificates. An SMTP server A GitLab instance or an account on gitlab.com A GitHub account A kubernetes cluster (version 1.21+) with Ingress Controller Dynamic Volume Provisioning kubectl installed and configured for your cluster helm installed and configured for your cluster Disclaimer: the above tools can be installed on both Linux and Windows. However, the following instructions have only been tested on Linux. Prepare DNS Create a CNAME record for *.<YOUR_DOMAIN> - e.g. *.example.org - and set the target to the output of: kubectl get services --namespace ingress-nginx -o jsonpath=\"{.items[].status.loadBalancer.ingress[0].hostname}\" Replace ingress-nginx with the namespace where your ingress controller is installed. Eclipse Che Install chectl Download chectl version 7.38.0 for your operating system from the chectl release page , e.g. chectl-linux-x64.tar.gz . Unpack it to your HOME folder: shell tar -xvz -C ${HOME} -f chectl-linux-x64.tar.gz Create an alias for the chectl command: shell chectl=${HOME}/chectl/bin/run Namespace Create the namespace for Eclipse Che: kubectl create namespace eclipse-che Certificate Create the TLS certificate secret for Eclipse Che: kubectl create secret tls che-tls \\ --namespace eclipse-che \\ --cert=smartclide.full-chain.crt \\ --key=smartclide.key Deploy Eclipse Che Configure deployment scripts. Run: shell sed -i s/\"{THE_DOMAIN}\"/\"<YOUR_DOMAIN>\"/g che/*.{yaml,json} Replace <YOUR_DOMAIN> with your domain, e.g. example.org . Create initial deployment. Run: shell chectl server:deploy \\ --installer=helm \\ --platform=k8s \\ --chenamespace=eclipse-che \\ --domain=<YOUR_DOMAIN> \\ --telemetry=off \\ --no-auto-update \\ --multiuser \\ --helm-patch-yaml=che/my-values.yaml Replace <YOUR_DOMAIN> with your domain, e.g. example.org . Answer with n (\"no\") when asked Do you want to update chectl now? [y/n]: . Answer with y (\"yes\") when asked 'helm' installer is deprecated. Do you want to proceed? [y/n]: . The command will take a while. At the end you should see a message like: ```shell \u2714 Eclipse Che 7.38.0 has been successfully deployed. \u2714 Documentation : https://www.eclipse.org/che/docs/ \u2714 \u2714 Users Dashboard : https://che.example.org \u2714 Admin user login : \"admin:admin\". NOTE: must change after first login. \u2714 \u2714 Plug-in Registry : https://plugin-registry-eclipse-che.example.org/v3/ \u2714 Devfile Registry : https://devfile-registry-eclipse-che.example.org/ \u2714 \u2714 Identity Provider URL : https://keycloak-eclipse-che.example.org/auth/ \u2714 Identity Provider login : \"admin:s3cr3t\". \u2714 Command server:deploy has completed successfully in 02:34. ``` Update Eclipse Che configuration. Run: ```shell kubectl patch configmaps --namespace eclipse-che che --patch \"$(cat che/config-patch.yaml)\" -o yaml for theIngress in che-ingress che-dashboard-ingress devfile-registry plugin-registry keycloak-ingress do kubectl patch ingress --namespace eclipse-che ${theIngress} --patch \"$(cat che/ingress-patch.yaml)\" -o yaml done kubectl --namespace eclipse-che rollout restart deployment/che ``` Open Che Users Dashboard https://che.<YOUR_DOMAIN> , e.g. https://che.example.org , in your browser, and login with the admin username and password provided in the output of chectl server:deploy command above. Change the default admin password to a secure password! Open keycloak admin interface https://keycloak-eclipse-che.<YOUR_DOMAIN>/auth/admin/ , e.g. https://keycloak-eclipse-che.example.org/auth/admin/ , in your browser, and login with the keycloak admin username and password provided in the output of chectl server:deploy command above. Make sure that the realm Che is selected at the top of the left-hand menu. Under the menu section Manage , click on Import and select che/realm-patch.json . Make sure that Import clients and Import realm roles is ON . Click Import . You should see a notification that 12 records have been added. Under the menu section Configure , click on Realm Settings , then click on the tab Login and change Require SSL to external requests . Click Save . Click on the tab Email and fill in the values for your SMTP server, so that keycloak can send passwort reset emails. Click Save . Under the menu section Configure , click on Roles and the tab Default Roles . In the Available Roles box, select developer kie-sever rest-all user Click Add selected . Under the menu section Configure , click on Clients , and select business-central . Click on the tab Credentials and then on Regenerate Secret . Note down the new secret. Repeat this for the client kie-server . Dedicated Node Group for DLE The DLE component requires more resources than other SmartCLIDE components. The minimum requirements are 4 CPUs and 16 GiB RAM . It is therefore recommended to create a dedicated node group for the DLE with at least 1 node of the required minimum size. Add the following label and taint to the node group / all nodes in the node group: labels: smartclide-nodegroup-type: \"dle\" taints: - key: \"dle\" value: \"true\" effect: \"NoSchedule\" Settings Open my-values.yml in a text editor and change the values according to your needs. See the comments in the file for more information. Namespace Create the namespace: kubectl create namespace smartclide Certificate Create the TLS certificate secret: kubectl create secret tls smartclide-tls \\ --namespace smartclide \\ --cert=smartclide.full-chain.crt \\ --key=smartclide.key Install / Upgrade helm upgrade --install --namespace smartclide --values my-values.yaml smartclide helm-chart Uninstall helm uninstall --namespace smartclide smartclide","title":"For the sysadmin"},{"location":"getting-started-sysadmin/#for-the-sysadmin","text":"This page contains some very-easy-to-follow guides for sysadmins that want to set up and deploy an on-premises instance of the SmartCLIDE IDE.","title":"For the sysadmin"},{"location":"getting-started-sysadmin/#smartclide-helm-chart","text":"","title":"SmartCLIDE Helm Chart"},{"location":"getting-started-sysadmin/#requirements","text":"A publicly resolvable domain, like example.org or a subdomain like foo.example.org . You must be able to edit DNS records for that domain. A wildcard TLS certificate for your domain, i.e. for *.example.org or *.foo.example.org . It must be a wildcard certificate! Letsencrypt is not supported at the moment! Save the certificate files in smartclide.full-chain.crt and smartclide.key . smartclide.full-chain.crt must contain the full certificate chain, including intermediate certificates. An SMTP server A GitLab instance or an account on gitlab.com A GitHub account A kubernetes cluster (version 1.21+) with Ingress Controller Dynamic Volume Provisioning kubectl installed and configured for your cluster helm installed and configured for your cluster Disclaimer: the above tools can be installed on both Linux and Windows. However, the following instructions have only been tested on Linux.","title":"Requirements"},{"location":"getting-started-sysadmin/#prepare","text":"","title":"Prepare"},{"location":"getting-started-sysadmin/#dns","text":"Create a CNAME record for *.<YOUR_DOMAIN> - e.g. *.example.org - and set the target to the output of: kubectl get services --namespace ingress-nginx -o jsonpath=\"{.items[].status.loadBalancer.ingress[0].hostname}\" Replace ingress-nginx with the namespace where your ingress controller is installed.","title":"DNS"},{"location":"getting-started-sysadmin/#eclipse-che","text":"","title":"Eclipse Che"},{"location":"getting-started-sysadmin/#install-chectl","text":"Download chectl version 7.38.0 for your operating system from the chectl release page , e.g. chectl-linux-x64.tar.gz . Unpack it to your HOME folder: shell tar -xvz -C ${HOME} -f chectl-linux-x64.tar.gz Create an alias for the chectl command: shell chectl=${HOME}/chectl/bin/run","title":"Install chectl"},{"location":"getting-started-sysadmin/#namespace","text":"Create the namespace for Eclipse Che: kubectl create namespace eclipse-che","title":"Namespace"},{"location":"getting-started-sysadmin/#certificate","text":"Create the TLS certificate secret for Eclipse Che: kubectl create secret tls che-tls \\ --namespace eclipse-che \\ --cert=smartclide.full-chain.crt \\ --key=smartclide.key","title":"Certificate"},{"location":"getting-started-sysadmin/#deploy-eclipse-che","text":"Configure deployment scripts. Run: shell sed -i s/\"{THE_DOMAIN}\"/\"<YOUR_DOMAIN>\"/g che/*.{yaml,json} Replace <YOUR_DOMAIN> with your domain, e.g. example.org . Create initial deployment. Run: shell chectl server:deploy \\ --installer=helm \\ --platform=k8s \\ --chenamespace=eclipse-che \\ --domain=<YOUR_DOMAIN> \\ --telemetry=off \\ --no-auto-update \\ --multiuser \\ --helm-patch-yaml=che/my-values.yaml Replace <YOUR_DOMAIN> with your domain, e.g. example.org . Answer with n (\"no\") when asked Do you want to update chectl now? [y/n]: . Answer with y (\"yes\") when asked 'helm' installer is deprecated. Do you want to proceed? [y/n]: . The command will take a while. At the end you should see a message like: ```shell \u2714 Eclipse Che 7.38.0 has been successfully deployed. \u2714 Documentation : https://www.eclipse.org/che/docs/ \u2714 \u2714 Users Dashboard : https://che.example.org \u2714 Admin user login : \"admin:admin\". NOTE: must change after first login. \u2714 \u2714 Plug-in Registry : https://plugin-registry-eclipse-che.example.org/v3/ \u2714 Devfile Registry : https://devfile-registry-eclipse-che.example.org/ \u2714 \u2714 Identity Provider URL : https://keycloak-eclipse-che.example.org/auth/ \u2714 Identity Provider login : \"admin:s3cr3t\". \u2714 Command server:deploy has completed successfully in 02:34. ``` Update Eclipse Che configuration. Run: ```shell kubectl patch configmaps --namespace eclipse-che che --patch \"$(cat che/config-patch.yaml)\" -o yaml for theIngress in che-ingress che-dashboard-ingress devfile-registry plugin-registry keycloak-ingress do kubectl patch ingress --namespace eclipse-che ${theIngress} --patch \"$(cat che/ingress-patch.yaml)\" -o yaml done kubectl --namespace eclipse-che rollout restart deployment/che ``` Open Che Users Dashboard https://che.<YOUR_DOMAIN> , e.g. https://che.example.org , in your browser, and login with the admin username and password provided in the output of chectl server:deploy command above. Change the default admin password to a secure password! Open keycloak admin interface https://keycloak-eclipse-che.<YOUR_DOMAIN>/auth/admin/ , e.g. https://keycloak-eclipse-che.example.org/auth/admin/ , in your browser, and login with the keycloak admin username and password provided in the output of chectl server:deploy command above. Make sure that the realm Che is selected at the top of the left-hand menu. Under the menu section Manage , click on Import and select che/realm-patch.json . Make sure that Import clients and Import realm roles is ON . Click Import . You should see a notification that 12 records have been added. Under the menu section Configure , click on Realm Settings , then click on the tab Login and change Require SSL to external requests . Click Save . Click on the tab Email and fill in the values for your SMTP server, so that keycloak can send passwort reset emails. Click Save . Under the menu section Configure , click on Roles and the tab Default Roles . In the Available Roles box, select developer kie-sever rest-all user Click Add selected . Under the menu section Configure , click on Clients , and select business-central . Click on the tab Credentials and then on Regenerate Secret . Note down the new secret. Repeat this for the client kie-server .","title":"Deploy Eclipse Che"},{"location":"getting-started-sysadmin/#dedicated-node-group-for-dle","text":"The DLE component requires more resources than other SmartCLIDE components. The minimum requirements are 4 CPUs and 16 GiB RAM . It is therefore recommended to create a dedicated node group for the DLE with at least 1 node of the required minimum size. Add the following label and taint to the node group / all nodes in the node group: labels: smartclide-nodegroup-type: \"dle\" taints: - key: \"dle\" value: \"true\" effect: \"NoSchedule\"","title":"Dedicated Node Group for DLE"},{"location":"getting-started-sysadmin/#settings","text":"Open my-values.yml in a text editor and change the values according to your needs. See the comments in the file for more information.","title":"Settings"},{"location":"getting-started-sysadmin/#namespace_1","text":"Create the namespace: kubectl create namespace smartclide","title":"Namespace"},{"location":"getting-started-sysadmin/#certificate_1","text":"Create the TLS certificate secret: kubectl create secret tls smartclide-tls \\ --namespace smartclide \\ --cert=smartclide.full-chain.crt \\ --key=smartclide.key","title":"Certificate"},{"location":"getting-started-sysadmin/#install-upgrade","text":"helm upgrade --install --namespace smartclide --values my-values.yaml smartclide helm-chart","title":"Install / Upgrade"},{"location":"getting-started-sysadmin/#uninstall","text":"helm uninstall --namespace smartclide smartclide","title":"Uninstall"},{"location":"user-guide/","text":"SmartCLIDE User Guide. Under construction...","title":"SmartCLIDE User Guide."},{"location":"user-guide/#smartclide-user-guide","text":"Under construction...","title":"SmartCLIDE User Guide."},{"location":"api-gateway/","text":"smartclide-api-gateway SmartCLIDE API Gateway is wrapper module around all RESTful APIs create within the SmartCLIDE platform. It provides a common interface for accessing the different RESTful APIs, while in the same time preserves the interface of each of them. In this way, the SmartCLIDE API Gateway provides an interoperable way of accessing the services of the different SmartCLIDE backend components from the frontend side. The SmartCLIDE API Gateway serves as a single point of access for all SmartCLIDE Backend components. The SmartCLIDE API Gateway defines the following common interface for SmartCLIDE RESTful APIs: https://api.dev.smartclide.eu/{component_name}/{component_resource} Examples: Jave code generation (POST endpoint from the DLE RESTful API) : http://smartclide.ddns.net:5001/smartclide/v1/dle/codegen becomes https://api.dev.smartclide.eu/dle/codegen TD Principal analysis (POST endpoint from the TD Principal RESTful API) : https://td-principal.smartclide-td/api/analysis becomes https://api.dev.smartclide.eu/td-principal/analysis The SmartCLIDE API Gateway supports the following {component_name} elements: td-interest td-principal td-reusability service-creation dle smart-assistant service-creation-test-generation","title":"API Gateway"},{"location":"api-gateway/#smartclide-api-gateway","text":"SmartCLIDE API Gateway is wrapper module around all RESTful APIs create within the SmartCLIDE platform. It provides a common interface for accessing the different RESTful APIs, while in the same time preserves the interface of each of them. In this way, the SmartCLIDE API Gateway provides an interoperable way of accessing the services of the different SmartCLIDE backend components from the frontend side. The SmartCLIDE API Gateway serves as a single point of access for all SmartCLIDE Backend components. The SmartCLIDE API Gateway defines the following common interface for SmartCLIDE RESTful APIs: https://api.dev.smartclide.eu/{component_name}/{component_resource} Examples: Jave code generation (POST endpoint from the DLE RESTful API) : http://smartclide.ddns.net:5001/smartclide/v1/dle/codegen becomes https://api.dev.smartclide.eu/dle/codegen TD Principal analysis (POST endpoint from the TD Principal RESTful API) : https://td-principal.smartclide-td/api/analysis becomes https://api.dev.smartclide.eu/td-principal/analysis The SmartCLIDE API Gateway supports the following {component_name} elements: td-interest td-principal td-reusability service-creation dle smart-assistant service-creation-test-generation","title":"smartclide-api-gateway"},{"location":"context-handling/","text":"smartclide-context OpenSmartCLIDE Context Handling Component Preconditions to build and run Context Handling To build and run Context Handling, the following software is required: Java (at least version 11) Apache Maven (at least version 3.5.4) Docker (for running tests and deploying Context Handling on the SmartCLIDE cluster) docker-compose (for running local sample instance only) How to build Context Handling Context Handling can be built using maven with the following command: shell mvn install In order to build and push a container image that can be deployed, the following command can be used: shell mvn install mvn jib:build -pl smartclide-monitoring -Djib.to.image=\"${IMAGE_NAME:IMAGE_TAG}\" -Djib.to.auth.username=\"${CONTAINER_REGISTRY_USERNAME}\" -Djib.to.auth.password=\"${CONTAINER_REGISTRY_TOKEN}\" How to run Context Handling A sample configuration and docker-compose file can be found in the samples folder . You can run the sample with the following command: shell docker-compose -f samples/docker-compose.yml up How to configure Context Handling Monitoring Config monitoring-config.xml An example monitoring configuration can be found here: monitoring-config.xml monitoring-config.xsd The corresponding XSD file can be found here: monitoring-config.xsd Description indexes Each index entry has the following mandatory attributes id: The unique name of the index location: The URI of the location the index is stored datasources Each datasource entry has the following mandatory attributes id:The unique name of the datasource type:The type of the datasource. Possible values are: filesystem, webservice, database, messageBroker monitor:The class of the monitor to be used. Possible values are: package org.eclipse.opensmartclide.context.monitoring.monitors.database.DatabaseMonitor package org.eclipse.opensmartclide.context.monitoring.monitors.file.FileSystemMonitor package org.eclipse.opensmartclide.context.monitoring.monitors.file.FilePairSystemMonitor package org.eclipse.opensmartclide.context.monitoring.monitors.file.FileTripletSystemMonitor package org.eclipse.opensmartclide.context.monitoring.monitors.webservice.MessageBrokerMonitor package org.eclipse.opensmartclide.context.monitoring.monitors.webservice.WebServiceMonitor package org.eclipse.opensmartclide.context.monitoring.monitors.GitlabCommitMonitor options: Options for the datasource can be entered using this value. The options are dependent on the datasource to be used uri:The uri of the data source to be monitored class:The following datasource implementations are available package org.eclipse.opensmartclide.context.monitoring.config.models.datasources.DatabaseDataSource package org.eclipse.opensmartclide.context.monitoring.config.models.datasources.FilePairSystemDataSource package org.eclipse.opensmartclide.context.monitoring.config.models.datasources.FileSystemDataSource package org.eclipse.opensmartclide.context.monitoring.config.models.datasources.FileTripletSystemDataSource package org.eclipse.opensmartclide.context.monitoring.config.models.datasources.MessageBrokerDataSource package org.eclipse.opensmartclide.context.monitoring.config.models.datasources.WebServiceDataSource package org.eclipse.opensmartclide.context.monitoring.config.models.datasources.GitlabDataSource interpreters Each interpreter entry has the following mandatory attributes id: The unique name of the interpreter configuration analyser: The analyser class to be used. The following implementations are available: package org.eclipse.opensmartclide.context.monitoring.analyser.database.DatabaseAnalyser package org.eclipse.opensmartclide.context.monitoring.analyser.file.FileAnalyser package org.eclipse.opensmartclide.context.monitoring.analyser.file.FilePairAnalyser package org.eclipse.opensmartclide.context.monitoring.analyser.file.FileTripletAnalyser package org.eclipse.opensmartclide.context.monitoring.analyser.messagebroker.MessageBrokerAnalyser package org.eclipse.opensmartclide.context.monitoring.analyser.webservice.WebServiceAnalyser package org.eclipse.opensmartclide.context.monitoring.analyser.webserviceGitAnalyser package org.eclipse.opensmartclide.context.monitoring.analyser.webservice.GitlabCommitAnalyser parser: The parser class to be used. The following implementations are available: package org.eclipse.opensmartclide.context.monitoring.parser.database.DatabaseParser package org.eclipse.opensmartclide.context.monitoring.parser.file.FileParser package org.eclipse.opensmartclide.context.monitoring.parser.file.FilePairParser package org.eclipse.opensmartclide.context.monitoring.parser.file.FileTripletParser package org.eclipse.opensmartclide.context.monitoring.parser.messagebroker.MessageBrokerParser package org.eclipse.opensmartclide.context.monitoring.parser.webservice.WebServiceParser package org.eclipse.opensmartclide.context.monitoring.parser.GitlabCommitParser type: Currently only used for File analyser and parser. Defines the file extensions to be used. monitors Each monitor entry has the following mandatory attributes id: The unique name of the monitor datasource: The id of one previously defined datasource (see above) interpreter: The id of one previously defined interpreter (see above) index: The id of one previously defined index (see above)","title":"Context Handling"},{"location":"context-handling/#smartclide-context","text":"OpenSmartCLIDE Context Handling Component","title":"smartclide-context"},{"location":"context-handling/#preconditions-to-build-and-run-context-handling","text":"To build and run Context Handling, the following software is required: Java (at least version 11) Apache Maven (at least version 3.5.4) Docker (for running tests and deploying Context Handling on the SmartCLIDE cluster) docker-compose (for running local sample instance only)","title":"Preconditions to build and run Context Handling"},{"location":"context-handling/#how-to-build-context-handling","text":"Context Handling can be built using maven with the following command: shell mvn install In order to build and push a container image that can be deployed, the following command can be used: shell mvn install mvn jib:build -pl smartclide-monitoring -Djib.to.image=\"${IMAGE_NAME:IMAGE_TAG}\" -Djib.to.auth.username=\"${CONTAINER_REGISTRY_USERNAME}\" -Djib.to.auth.password=\"${CONTAINER_REGISTRY_TOKEN}\"","title":"How to build Context Handling"},{"location":"context-handling/#how-to-run-context-handling","text":"A sample configuration and docker-compose file can be found in the samples folder . You can run the sample with the following command: shell docker-compose -f samples/docker-compose.yml up","title":"How to run Context Handling"},{"location":"context-handling/#how-to-configure-context-handling","text":"","title":"How to configure Context Handling"},{"location":"context-handling/#monitoring-config","text":"monitoring-config.xml An example monitoring configuration can be found here: monitoring-config.xml monitoring-config.xsd The corresponding XSD file can be found here: monitoring-config.xsd","title":"Monitoring Config"},{"location":"context-handling/#description","text":"","title":"Description"},{"location":"context-handling/#indexes","text":"Each index entry has the following mandatory attributes id: The unique name of the index location: The URI of the location the index is stored","title":"indexes"},{"location":"context-handling/#datasources","text":"Each datasource entry has the following mandatory attributes id:The unique name of the datasource type:The type of the datasource. Possible values are: filesystem, webservice, database, messageBroker monitor:The class of the monitor to be used. Possible values are: package org.eclipse.opensmartclide.context.monitoring.monitors.database.DatabaseMonitor package org.eclipse.opensmartclide.context.monitoring.monitors.file.FileSystemMonitor package org.eclipse.opensmartclide.context.monitoring.monitors.file.FilePairSystemMonitor package org.eclipse.opensmartclide.context.monitoring.monitors.file.FileTripletSystemMonitor package org.eclipse.opensmartclide.context.monitoring.monitors.webservice.MessageBrokerMonitor package org.eclipse.opensmartclide.context.monitoring.monitors.webservice.WebServiceMonitor package org.eclipse.opensmartclide.context.monitoring.monitors.GitlabCommitMonitor options: Options for the datasource can be entered using this value. The options are dependent on the datasource to be used uri:The uri of the data source to be monitored class:The following datasource implementations are available package org.eclipse.opensmartclide.context.monitoring.config.models.datasources.DatabaseDataSource package org.eclipse.opensmartclide.context.monitoring.config.models.datasources.FilePairSystemDataSource package org.eclipse.opensmartclide.context.monitoring.config.models.datasources.FileSystemDataSource package org.eclipse.opensmartclide.context.monitoring.config.models.datasources.FileTripletSystemDataSource package org.eclipse.opensmartclide.context.monitoring.config.models.datasources.MessageBrokerDataSource package org.eclipse.opensmartclide.context.monitoring.config.models.datasources.WebServiceDataSource package org.eclipse.opensmartclide.context.monitoring.config.models.datasources.GitlabDataSource","title":"datasources"},{"location":"context-handling/#interpreters","text":"Each interpreter entry has the following mandatory attributes id: The unique name of the interpreter configuration analyser: The analyser class to be used. The following implementations are available: package org.eclipse.opensmartclide.context.monitoring.analyser.database.DatabaseAnalyser package org.eclipse.opensmartclide.context.monitoring.analyser.file.FileAnalyser package org.eclipse.opensmartclide.context.monitoring.analyser.file.FilePairAnalyser package org.eclipse.opensmartclide.context.monitoring.analyser.file.FileTripletAnalyser package org.eclipse.opensmartclide.context.monitoring.analyser.messagebroker.MessageBrokerAnalyser package org.eclipse.opensmartclide.context.monitoring.analyser.webservice.WebServiceAnalyser package org.eclipse.opensmartclide.context.monitoring.analyser.webserviceGitAnalyser package org.eclipse.opensmartclide.context.monitoring.analyser.webservice.GitlabCommitAnalyser parser: The parser class to be used. The following implementations are available: package org.eclipse.opensmartclide.context.monitoring.parser.database.DatabaseParser package org.eclipse.opensmartclide.context.monitoring.parser.file.FileParser package org.eclipse.opensmartclide.context.monitoring.parser.file.FilePairParser package org.eclipse.opensmartclide.context.monitoring.parser.file.FileTripletParser package org.eclipse.opensmartclide.context.monitoring.parser.messagebroker.MessageBrokerParser package org.eclipse.opensmartclide.context.monitoring.parser.webservice.WebServiceParser package org.eclipse.opensmartclide.context.monitoring.parser.GitlabCommitParser type: Currently only used for File analyser and parser. Defines the file extensions to be used.","title":"interpreters"},{"location":"context-handling/#monitors","text":"Each monitor entry has the following mandatory attributes id: The unique name of the monitor datasource: The id of one previously defined datasource (see above) interpreter: The id of one previously defined interpreter (see above) index: The id of one previously defined index (see above)","title":"monitors"},{"location":"database-api/","text":"eclipse-opensmartclide-db-api A RESTful API for accessing the resources of the internal OpenSmartCLIDE database. The API exposes the following endpoints: /users (GET,POST,PUT,DELETE) Example POST Request /users Example Request body { \"id\": \"628c87f6aa5a2857398a80a0\", \"email\": \"test@mail.com\", \"team_id\": \"628c87f6aa5a2857398a80d8\" } /teams (GET,POST,PUT,DELETE) Example POST Request /teams Example Request body { \"name\": \"Test team\", \"workflows\": [], \"services\": [], \"deployments\": [] } /ci_managers (GET,POST,PUT,DELETE) Example POST Request /ci_managers Example Request body { \"user_id\": \"628c87f6aa5a2857398a80a0\", \"type\": \"jenkins\", \"url\": \"http://some_url\", \"username\": \"test_user\", \"token\": \"giuggff8ff7igfigfiugi...\" } /deployment_platforms (GET,POST,PUT,DELETE) Example POST Request /deployment_platforms Example Request body { \"user_id\": \"628c87f6aa5a2857398a80a0\", \"url\": \"http://some_url\", \"username\": \"test_user\", \"token\": \"giuggff8ff7igfigfiugi...\" } /service_registries (GET,POST,PUT,DELETE) Example POST Request /service_registries Example Request body { \"user_id\": \"628c87f6aa5a2857398a80a0\", \"type\": \"github\", \"url\": \"http://some_url\", \"username\": \"test_user\", \"token\": \"dnsuibfifgyucgufgu...\" } /git_credentials (GET,POST,PUT,DELETE) Example POST Request /git_credentials Example Request body { \"user_id\": \"628c87f6aa5a2857398a80a0\", \"type\": \"github\", \"url\": \"http://some_url\", \"username\": \"test\", \"token\": \"bcsuifgsiufgsfiuiu\" } /services (GET,POST,PUT,DELETE) Example POST Request /services Example Request body { \"name\": \"Test service\", \"user_id\": \"628c87f6aa5a2857398a80a0\", \"registry_id\": \"628c8dab80b42501489a85da\", \"git_credentials_id\": \"628c922780b42501489a85dd\", \"url\": \"http://test_url\", \"description\": \"A short service description..\", \"is_public\": true, \"licence\": \"test_licence\", \"framework\": \"java\", \"created\": \"2022-01-14T15:42:25.000+00:00\", \"updated\": \"2022-01-14T15:42:25.000+00:00\" } /workflows (GET,POST,PUT,DELETE) Example POST Request /workflows Example Request body { \"name\": \"Test service\", \"user_id\": \"628c87f6aa5a2857398a80a0\", \"git_credentials_id\": \"628c922780b42501489a85dd\", \"url\": \"http://test_url\", \"description\": \"A short workflow description..\", \"is_public\": true, \"created\": \"2022-01-14T15:42:25.000+00:00\", \"updated\": \"2022-01-14T15:42:25.000+00:00\" } /deployments (GET,POST,PUT,DELETE) Example POST Request /deployments Example Request body { \"name\": \"Test service\", \"user_id\": \"628c87f6aa5a2857398a80a0\", \"git_credentials_id\": \"628c922780b42501489a85dd\", \"url\": \"http://test_url\", \"workflow_id\": \"6283ac19189ff14b1516c11c\", \"service_id\": \"628c928d80b42501489a85de\", \"version\": \"$deployment_version\", \"state\": \"$deployment_state\", \"created\": \"2022-01-14T15:42:25.000+00:00\", \"updated\": \"2022-01-14T15:42:25.000+00:00\" }","title":"DB API"},{"location":"database-api/#eclipse-opensmartclide-db-api","text":"A RESTful API for accessing the resources of the internal OpenSmartCLIDE database. The API exposes the following endpoints:","title":"eclipse-opensmartclide-db-api"},{"location":"database-api/#users-getpostputdelete","text":"","title":"/users (GET,POST,PUT,DELETE)"},{"location":"database-api/#example-post-request","text":"/users","title":"Example POST Request"},{"location":"database-api/#example-request-body","text":"{ \"id\": \"628c87f6aa5a2857398a80a0\", \"email\": \"test@mail.com\", \"team_id\": \"628c87f6aa5a2857398a80d8\" }","title":"Example Request body"},{"location":"database-api/#teams-getpostputdelete","text":"","title":"/teams (GET,POST,PUT,DELETE)"},{"location":"database-api/#example-post-request_1","text":"/teams","title":"Example POST Request"},{"location":"database-api/#example-request-body_1","text":"{ \"name\": \"Test team\", \"workflows\": [], \"services\": [], \"deployments\": [] }","title":"Example Request body"},{"location":"database-api/#ci_managers-getpostputdelete","text":"","title":"/ci_managers (GET,POST,PUT,DELETE)"},{"location":"database-api/#example-post-request_2","text":"/ci_managers","title":"Example POST Request"},{"location":"database-api/#example-request-body_2","text":"{ \"user_id\": \"628c87f6aa5a2857398a80a0\", \"type\": \"jenkins\", \"url\": \"http://some_url\", \"username\": \"test_user\", \"token\": \"giuggff8ff7igfigfiugi...\" }","title":"Example Request body"},{"location":"database-api/#deployment_platforms-getpostputdelete","text":"","title":"/deployment_platforms (GET,POST,PUT,DELETE)"},{"location":"database-api/#example-post-request_3","text":"/deployment_platforms","title":"Example POST Request"},{"location":"database-api/#example-request-body_3","text":"{ \"user_id\": \"628c87f6aa5a2857398a80a0\", \"url\": \"http://some_url\", \"username\": \"test_user\", \"token\": \"giuggff8ff7igfigfiugi...\" }","title":"Example Request body"},{"location":"database-api/#service_registries-getpostputdelete","text":"","title":"/service_registries (GET,POST,PUT,DELETE)"},{"location":"database-api/#example-post-request_4","text":"/service_registries","title":"Example POST Request"},{"location":"database-api/#example-request-body_4","text":"{ \"user_id\": \"628c87f6aa5a2857398a80a0\", \"type\": \"github\", \"url\": \"http://some_url\", \"username\": \"test_user\", \"token\": \"dnsuibfifgyucgufgu...\" }","title":"Example Request body"},{"location":"database-api/#git_credentials-getpostputdelete","text":"","title":"/git_credentials (GET,POST,PUT,DELETE)"},{"location":"database-api/#example-post-request_5","text":"/git_credentials","title":"Example POST Request"},{"location":"database-api/#example-request-body_5","text":"{ \"user_id\": \"628c87f6aa5a2857398a80a0\", \"type\": \"github\", \"url\": \"http://some_url\", \"username\": \"test\", \"token\": \"bcsuifgsiufgsfiuiu\" }","title":"Example Request body"},{"location":"database-api/#services-getpostputdelete","text":"","title":"/services (GET,POST,PUT,DELETE)"},{"location":"database-api/#example-post-request_6","text":"/services","title":"Example POST Request"},{"location":"database-api/#example-request-body_6","text":"{ \"name\": \"Test service\", \"user_id\": \"628c87f6aa5a2857398a80a0\", \"registry_id\": \"628c8dab80b42501489a85da\", \"git_credentials_id\": \"628c922780b42501489a85dd\", \"url\": \"http://test_url\", \"description\": \"A short service description..\", \"is_public\": true, \"licence\": \"test_licence\", \"framework\": \"java\", \"created\": \"2022-01-14T15:42:25.000+00:00\", \"updated\": \"2022-01-14T15:42:25.000+00:00\" }","title":"Example Request body"},{"location":"database-api/#workflows-getpostputdelete","text":"","title":"/workflows (GET,POST,PUT,DELETE)"},{"location":"database-api/#example-post-request_7","text":"/workflows","title":"Example POST Request"},{"location":"database-api/#example-request-body_7","text":"{ \"name\": \"Test service\", \"user_id\": \"628c87f6aa5a2857398a80a0\", \"git_credentials_id\": \"628c922780b42501489a85dd\", \"url\": \"http://test_url\", \"description\": \"A short workflow description..\", \"is_public\": true, \"created\": \"2022-01-14T15:42:25.000+00:00\", \"updated\": \"2022-01-14T15:42:25.000+00:00\" }","title":"Example Request body"},{"location":"database-api/#deployments-getpostputdelete","text":"","title":"/deployments (GET,POST,PUT,DELETE)"},{"location":"database-api/#example-post-request_8","text":"/deployments","title":"Example POST Request"},{"location":"database-api/#example-request-body_8","text":"{ \"name\": \"Test service\", \"user_id\": \"628c87f6aa5a2857398a80a0\", \"git_credentials_id\": \"628c922780b42501489a85dd\", \"url\": \"http://test_url\", \"workflow_id\": \"6283ac19189ff14b1516c11c\", \"service_id\": \"628c928d80b42501489a85de\", \"version\": \"$deployment_version\", \"state\": \"$deployment_state\", \"created\": \"2022-01-14T15:42:25.000+00:00\", \"updated\": \"2022-01-14T15:42:25.000+00:00\" }","title":"Example Request body"},{"location":"design-patterns/","text":"Smartclide Design Pattern Selection SmartCLIDE Design Pattern Selection Frontend Component Usage The user can access this extention from the tab View - Smartclide Design Pattern Selection. And the user is able to see the extension in the left side of the editor. The user is given two options (as its visible in the previous screen): - select a pattern from the drop-down menu, if he/she feels confident on the pattern that will be used (EXPERT-MODE). By selecting from the dropdown menu. - use the WIZARD to start the Q&A process (WIZARD-MODE). By selecting the \"Wizard\" button. In the left part of the next figure, we present the main layout of option-a (EXPERT-MODE), i.e., to directly select a pattern. Having selected a GoF pattern, the user is first reminded of the aim of the pattern, and he/she is guided in the application of the pattern through a textual example and an accompanying class diagram. In the right part of the next figure, we present the way that the Q&A process of pattern selection appears. For both cases, the roles of the pattern are mapped to either existing classes of the system, or new ones, relying on an autocomplete functionality. EXPERT-MODE WIZARD-MODE Finally, we can see an example of generated code for the Factory Method pattern, presented in the following figures. For the case of using existing classes, the code of the pattern is appended in the end of the existing code, whereas for new classes the new files are beeing created. Inputs Result Build and Run Preconditions to build and run To build and run the frontend Design Pattern Selection extension of Theia, the following software is required: Python Node.js with visual studio build tools (this can be selected in the optional tools during the node.js installation or after hand in several ways, ex. with npm, or with visual studio installer) Yarn package manager npm install --global yarn How to build Design Pattern Selection Frontend The Design Pattern Selection Frontend can be built using the following command: yarn How to run Design Pattern Selection Frontend After building the theia extension, you can start a local instance of theia with our extension. Running the browser example yarn start:browser or: yarn rebuild:browser cd browser-app yarn start or: launch Start Browser Backend configuration from VS code. Open http://localhost:3000 in the browser.","title":"Design Pattern Selection Frontend"},{"location":"design-patterns/#smartclide-design-pattern-selection","text":"SmartCLIDE Design Pattern Selection Frontend Component","title":"Smartclide Design Pattern Selection"},{"location":"design-patterns/#usage","text":"The user can access this extention from the tab View - Smartclide Design Pattern Selection. And the user is able to see the extension in the left side of the editor. The user is given two options (as its visible in the previous screen): - select a pattern from the drop-down menu, if he/she feels confident on the pattern that will be used (EXPERT-MODE). By selecting from the dropdown menu. - use the WIZARD to start the Q&A process (WIZARD-MODE). By selecting the \"Wizard\" button. In the left part of the next figure, we present the main layout of option-a (EXPERT-MODE), i.e., to directly select a pattern. Having selected a GoF pattern, the user is first reminded of the aim of the pattern, and he/she is guided in the application of the pattern through a textual example and an accompanying class diagram. In the right part of the next figure, we present the way that the Q&A process of pattern selection appears. For both cases, the roles of the pattern are mapped to either existing classes of the system, or new ones, relying on an autocomplete functionality. EXPERT-MODE WIZARD-MODE Finally, we can see an example of generated code for the Factory Method pattern, presented in the following figures. For the case of using existing classes, the code of the pattern is appended in the end of the existing code, whereas for new classes the new files are beeing created. Inputs Result","title":"Usage"},{"location":"design-patterns/#build-and-run","text":"","title":"Build and Run"},{"location":"design-patterns/#preconditions-to-build-and-run","text":"To build and run the frontend Design Pattern Selection extension of Theia, the following software is required: Python Node.js with visual studio build tools (this can be selected in the optional tools during the node.js installation or after hand in several ways, ex. with npm, or with visual studio installer) Yarn package manager npm install --global yarn","title":"Preconditions to build and run"},{"location":"design-patterns/#how-to-build-design-pattern-selection-frontend","text":"The Design Pattern Selection Frontend can be built using the following command: yarn","title":"How to build Design Pattern Selection Frontend"},{"location":"design-patterns/#how-to-run-design-pattern-selection-frontend","text":"After building the theia extension, you can start a local instance of theia with our extension.","title":"How to run Design Pattern Selection Frontend"},{"location":"design-patterns/#running-the-browser-example","text":"yarn start:browser or: yarn rebuild:browser cd browser-app yarn start or: launch Start Browser Backend configuration from VS code. Open http://localhost:3000 in the browser.","title":"Running the browser example"},{"location":"message-oriented-middleware-mom/","text":"smartclide-broker A MOM (Message-orientated Middleware) implementation for the communication of the SmartCLIDE components. RabbitMQ has been selected for the implementation of the message broker and the RabbitMQ server runs inside a Docker container. The broker is accessible via two APIs, both implemented in Spring Boot . Websocket API : a client can send messages to the broker using a websocket connection. MOM uses the /exchange destination prefix, thus supporting flexible, dynamic routing keys. A client should first establish a connection using the \"/websocket\" endpoint: ws://localhost:8080/websocket Then the client can send messages to arbitrary routing keys, e.g. /exchange/mom/foo.bar.baz or receive messages by subscribing to arbitrary binding patterns, e.g. /exchange/mom/foo.bar.* REST API : a client can send messages to a regular REST endpoint using a POST request with the JSON string as payload and the routing key in the path. The REST endpoint will then forward the JSON string directly to the broker using the given routing key: E.g. http://localhost:8080/mom/message/foo.bar.HURZ","title":"MOM"},{"location":"message-oriented-middleware-mom/#smartclide-broker","text":"A MOM (Message-orientated Middleware) implementation for the communication of the SmartCLIDE components. RabbitMQ has been selected for the implementation of the message broker and the RabbitMQ server runs inside a Docker container. The broker is accessible via two APIs, both implemented in Spring Boot . Websocket API : a client can send messages to the broker using a websocket connection. MOM uses the /exchange destination prefix, thus supporting flexible, dynamic routing keys. A client should first establish a connection using the \"/websocket\" endpoint: ws://localhost:8080/websocket Then the client can send messages to arbitrary routing keys, e.g. /exchange/mom/foo.bar.baz or receive messages by subscribing to arbitrary binding patterns, e.g. /exchange/mom/foo.bar.* REST API : a client can send messages to a regular REST endpoint using a POST request with the JSON string as payload and the routing key in the path. The REST endpoint will then forward the JSON string directly to the broker using the given routing key: E.g. http://localhost:8080/mom/message/foo.bar.HURZ","title":"smartclide-broker"},{"location":"runtime-monitoring-verification/","text":"SmartCLIDE-RMV Requirements The third-party libraries and utilities required are: How to Build RMV Component Install SWI Prolog Building RMV tool and RMV server How to run RMV Component Configuration Running the RMV Tool Running the RMV Server RMV Sub-components Monitor Creation Usage Property Monitor Usage Monitor Sensor Usage Monitoring Services Usage Logging and Notification Usage Security Auditing Usage","title":"SmartCLIDE-RMV"},{"location":"runtime-monitoring-verification/#smartclide-rmv","text":"","title":"SmartCLIDE-RMV"},{"location":"runtime-monitoring-verification/#requirements","text":"The third-party libraries and utilities required are:","title":"Requirements"},{"location":"runtime-monitoring-verification/#how-to-build-rmv-component","text":"","title":"How to Build RMV Component"},{"location":"runtime-monitoring-verification/#install-swi-prolog","text":"","title":"Install SWI Prolog"},{"location":"runtime-monitoring-verification/#building-rmv-tool-and-rmv-server","text":"","title":"Building RMV tool and RMV server"},{"location":"runtime-monitoring-verification/#how-to-run-rmv-component","text":"","title":"How to run RMV Component"},{"location":"runtime-monitoring-verification/#configuration","text":"","title":"Configuration"},{"location":"runtime-monitoring-verification/#running-the-rmv-tool","text":"","title":"Running the RMV Tool"},{"location":"runtime-monitoring-verification/#running-the-rmv-server","text":"","title":"Running the RMV Server"},{"location":"runtime-monitoring-verification/#rmv-sub-components","text":"","title":"RMV Sub-components"},{"location":"runtime-monitoring-verification/#monitor-creation","text":"","title":"Monitor Creation"},{"location":"runtime-monitoring-verification/#usage","text":"","title":"Usage"},{"location":"runtime-monitoring-verification/#property-monitor","text":"","title":"Property Monitor"},{"location":"runtime-monitoring-verification/#usage_1","text":"","title":"Usage"},{"location":"runtime-monitoring-verification/#monitor-sensor","text":"","title":"Monitor Sensor"},{"location":"runtime-monitoring-verification/#usage_2","text":"","title":"Usage"},{"location":"runtime-monitoring-verification/#monitoring-services","text":"","title":"Monitoring Services"},{"location":"runtime-monitoring-verification/#usage_3","text":"","title":"Usage"},{"location":"runtime-monitoring-verification/#logging-and-notification","text":"","title":"Logging and Notification"},{"location":"runtime-monitoring-verification/#usage_4","text":"","title":"Usage"},{"location":"runtime-monitoring-verification/#security-auditing","text":"","title":"Security Auditing"},{"location":"runtime-monitoring-verification/#usage_5","text":"","title":"Usage"},{"location":"security-patterns/","text":"Security Patterns Service Security Patterns Service Installation In this wiki page we describe how the Security Patterns Service can be installed. Installation using Anaconda In this section, we provide instructions on how the user can build the python Flask server of the Security Patterns Service from scratch, using the Anaconda virtual environment. The Security Patterns Service is developed to run on Windows systems with python 3.6.* installed. We suggest installing python via the Anaconda distribution as it provides an easy way to create a virtual environment and install dependencies. The configuration steps needed, are described below: Step 1 : Download the latest Anaconda distribution and follow the installation steps described in the Anaconda documentation . Step 2 : Open Anaconda cmd. Running Anaconda cmd activates the base environment. We need to create a specific environment to run the Security Patterns Service. Create a new python 3.6.4 environment by running the following command: conda create --name security_patterns python=3.6.4 This command will result in the creation of a conda environment named security_patterns. In order to activate the new environment, execute the following command: conda activate security_patterns Step 3 : Now that the environment is activated, install the required libraries: conda install -c anaconda flask flask-cors pymongo waitress dnspython colorama Step 4 : Clone the latest Security Patterns version that can be found in the present Github repository of the Security Patterns Service and navigate to the root directory. Step 5 : To start the Security Patterns using the built-in Flask server, use the command promt inside the active Conda environment and execute the following command: python main.py 0.0.0.0 5000 builtin --debug This command will start the built-in Flask server locally (0.0.0.0) on port 5000. Or you can start the Security Patterns using the Waitress server. Use the command promt inside the active Conda environment and execute the following command: python main.py 0.0.0.0 5000 waitress This command will start the Waitress server locally (0.0.0.0) on port 5000. Note that the Waitress mode is higly recommended in real production environments, since it supports scaling and multiple-request handling features. Security Patterns Service Usage In this wiki page we describe how the Security Patterns Service can be used. Given a problem statement and a set of forces acting on the system, design patterns instruct its stakeholders on how to build this system. Patterns in the information technology environment provide information system architects with a technique for creating reusable solutions to design challenges. In this document, we describe the main functionality of the Security Patterns web service and how it can be put in practice in order to provide the user with useful information regarding Security Requirements (e.g. Authentication, Authorization, etc.) Security Patterns (e.g. Authenticator, Cryptographic), Security Control Technologies (e.g. OAuth2.0, Two Factor Authentication, etc.) and Security Libraries in Java or Python that utilize the aforementioned technologies. As part of the SmartCLIDE platform, the current service is an attempt to assist developers in selecting the most suitable security patterns and implementing them in their software. Detailed description and indicative examples of the usage of the Security Patterns web service are provided, to facilitate better understanding. The Security Patterns web service provides the user with valuable information on the security domain as mentioned earlier. All information regarding this service is stored in a mongo database and the information of interest will be extracted and presented to the user when requested. This is achieved through a dedicated API exposed by the RESTful web server, which is, in fact, a simple HTTP GET request. The inputs that need to be provided as parameters to this request are listed below: | Parameter | Description | Required | Valid Inputs | |:------------:|:---------------------------------------------------------------:|:--------:|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:| | requirement | The main security category that the user is interested in. | Yes | A string value acting as security requirement (e.g. Authentication, Authorization, etc.). | | pattern | The security pattern that the user is interested in. | No | A string acting as security pattern (e.g. Authenticator, Cryptographic, etc.). Default value is None. | | technology | The security control technology that the user is interested in. | No | A string acting as security control Technology (e.g. Authenticator, Cryptographic, etc.). Default value is None. | | language | The programming language that the user is interested in employing for their purpose. | No | One of the following string values: Java, Python. Default value is None. | The output of the Security Patterns web service is a JSON file containing the information that the user requested. The JSON file contains the security requirement, the security pattern(s) of the aforementioned requirement, the security control technology(ies) of the corresponding security pattern(s) and the corresponding libraries all accompanied by descriptions, characteristics, online resources and other useful information. It should be noted that the requirement parameter is necessary. Thus, it should be provided by the user otherwise no results will be returned. If no values are given for all the rest of the parameters, all results related to that particular requirement will be presented. The produced files are usually very long especially for the case that the user selects a requirement without specifying any other parameter. By setting the value of the language parameter to \u201dJava\u201d or \u201cPython\u201d the user narrows down the libraries that will be returned to those designed only for Java or Python respectively. For better understanding, an example is presented demonstrating how the Security Patterns web service can be invoked through a curl command. In the given example, we set the following parameters: requirement : Authentication pattern : Authenticator technology : OAuth2.0 language : Java Hence, the following HTTP GET Request needs to be submitted: http://160.40.53.132:5000/DataBase?requirement=Authentication&pattern=Authenticator&technology=OAuth2.0&language=Java After submitting the request, the Security Patterns service is invoked. In brief, the service selects the \u201cAuthentication\u201d security requirement, the \u201cAuthenticator\u201d security pattern, the \u201cOAuth2.0\u201d security control technology and \u201cJava\u201d programming language and returns all the relevant information. After the successful execution of the analysis, a JSON report with the results is produced and sent as a response to the user. The produced JSON for the above example is presented below. { \"requirement name\":\"Authentication\", \"_id\":\"61dd5c88c43a7d33907484b2\", \"description\":\"The system must validate the identity of its externals before interacting with them. Customers' identities must be authenticated in order to prevent unauthorized access.\", \"standard\":\"ISO 27001\", \"patterns\":{ \"pattern name\":\"Authenticator\", \"_id\":\"61dd5cefc43a7d33907484b7\", \"description\":\"The Authenticator pattern describes a general mechanism for providing identification and authentication to a server from a client. It has the added feature of allowing protocol negotiation to take place using the same procedures. The pattern operates by offering an authentication negotiation object which then provides the protected object only after authentication is successful.\", \"pattern type\":\"design pattern\", \"technologies\":[ { \"_id\":\"61dd5d46c43a7d33907484c2\", \"technology name\":\"OAuth2.0\", \"description\":\"OAuth 2.0 is the industry-standard protocol for authorization. OAuth 2.0 focuses on client developer simplicity while providing specific authorization flows for web applications, desktop applications, mobile phones, and living room devices. This specification and its extensions are being developed within the IETF OAuth Working Group.\", \"strength\":\"\", \"related_security_libraries\":[ { \"_id\":\"61dd5e10c43a7d33907484c9\", \"library name\":\"Spring Security\", \"language\":\"Java\", \"resource\":[ \"https://spring.io/projects/spring-security\", \"https://github.com/spring-projects/spring-security\" ], \"guidelines\":[ \"https://google\", \"https://youtube\" ], \"code examples\":[ ] }, { \"_id\":\"61dd5e10c43a7d33907484ca\", \"library name\":\"Google Auth\", \"language\":\"Java\", \"resource\":[ \"https://github.com/googleapis/google-auth-library-java\" ], \"guidelines\":[ \"https://google\", \"https://youtube\" ], \"code examples\":[ { \"description\":\"This example shows how you can add the \\u2026\", \"code\":\"public void \\u2026.\\n \\n\" } ] } ] } ] } } As can be seen above, the JSON object consists of five main elements, which are listed below: requirement name : which is the main security requirement that the user is interested in (in this case \u201cAuthentication\u201d). id : which is the id number of this particular requirement as it is stored in the mongo database (in this case \u201c61dd5c88c43a7d33907484b2\u201d). description : which is the requirement\u2019s basic description (in this case \u201cThe system must validate the identity of its externals before interacting with them. Customers' identities must be authenticated in order to prevent unauthorized access.\u201d). standard : which is the standard that the requirement has been extracted from (in this case \"ISO 27001\"). patterns : which contains all related security patterns to that specific security requirement or the specific security pattern that the user requested (in this case \u201cAuthenticator\u201d). Accordingly to the requirement, the information regarding the patterns, technologies and libraries can be seen in the JSON file. Another example is presented below where only two parameters are specified in case the user is interested in a particular requirement that will be processed in a specific programming language. In this case, the user needs to be informed of all potential security patterns and control technologies that can be put in practice. The parameters for this example are shown below: requirement : Authentication language : Java The following HTTP GET Request needs to be submitted: http://160.40.53.132:5000/DataBase?requirement=Authentication&language=Java After submitting the request, the Security Patterns service returns the following JSON report with the results: { \"requirement name\":\"Authentication\", \"_id\":\"61dd5c88c43a7d33907484b2\", \"description\":\"The system must validate the identity of its externals before interacting with them. Customers' identities must be authenticated in order to prevent unauthorized access.\", \"standard\":\"ISO 27001\", \"patterns\":[ { \"pattern name\":\"Authenticator\", \"_id\":\"61dd5cefc43a7d33907484b7\", \"description\":\"The Authenticator pattern describes a general mechanism for providing identification and authentication to a server from a client. It has the added feature of allowing protocol negotiation to take place using the same procedures. The pattern operates by offering an authentication negotiation object which then provides the protected object only after authentication is successful.\", \"pattern type\":\"design pattern\", \"technologies\":[ { \"_id\":\"61dd5d46c43a7d33907484c0\", \"technology name\":\"Basic Authentication\", \"description\":\"\", \"strength\":\"\", \"related_security_libraries\":[ { \"_id\":\"61dd5e10c43a7d33907484c9\", \"library name\":\"Spring Security\", \"language\":\"Java\", \"resource\":[ \"https://spring.io/projects/spring-security\", \"https://github.com/spring-projects/spring-security\" ], \"guidelines\":[ \"https://www.youtube.com/watch?v=her_7pa0vrg&ab_channel=Amigoscode\" ], \"code examples\":[ ] } ] }, { \"_id\":\"61dd5d46c43a7d33907484c1\", \"technology name\":\"OAuth1.0\", \"description\":\"OAuth 2.1 is an in-progress effort to consolidate and simplify the most commonly used features of OAuth 2.0.\", \"strength\":\"\", \"related_security_libraries\":[ { \"_id\":\"61dd5e10c43a7d33907484c9\", \"library name\":\"Spring Security\", \"language\":\"Java\", \"resource\":[ \"https://spring.io/projects/spring-security\", \"https://github.com/spring-projects/spring-security\" ], \"guidelines\":[ \"https://www.youtube.com/watch?v=her_7pa0vrg&ab_channel=Amigoscode\" ], \"code examples\":[ ] } ] }, { \"_id\":\"61dd5d46c43a7d33907484c2\", \"technology name\":\"OAuth2.0\", \"description\":\"OAuth 2.0 is the industry-standard protocol for authorization. OAuth 2.0 focuses on client developer simplicity while providing specific authorization flows for web applications, desktop applications, mobile phones, and living room devices. This specification and its extensions are being developed within the IETF OAuth Working Group.\", \"strength\":\"\", \"related_security_libraries\":[ { \"_id\":\"61dd5e10c43a7d33907484c9\", \"library name\":\"Spring Security\", \"language\":\"Java\", \"resource\":[ \"https://spring.io/projects/spring-security\", \"https://github.com/spring-projects/spring-security\" ], \"guidelines\":[ \"https://www.youtube.com/watch?v=her_7pa0vrg&ab_channel=Amigoscode\" ], \"code examples\":[ ] }, { \"_id\":\"61dd5e10c43a7d33907484ca\", \"library name\":\"Google Auth\", \"language\":\"Java\", \"resource\":[ \"https://github.com/googleapis/google-auth-library-java\" ], \"guidelines\":[ \"https://google\", \"https://youtube\" ], \"code examples\":[ { \"description\":\"This example shows how you can add the \\u2026\", \"code\":\"public void \\u2026.\\n \\n\" } ] } ] }, { \"_id\":\"61dd5d46c43a7d33907484c4\", \"technology name\":\"Two Factor Authentication\", \"description\":\"\", \"strength\":\"\", \"related_security_libraries\":[ { \"_id\":\"61dd5e10c43a7d33907484c9\", \"library name\":\"Spring Security\", \"language\":\"Java\", \"resource\":[ \"https://spring.io/projects/spring-security\", \"https://github.com/spring-projects/spring-security\" ], \"guidelines\":[ \"https://www.youtube.com/watch?v=her_7pa0vrg&ab_channel=Amigoscode\" ], \"code examples\":[ ] } ] } ] } ] } As can be seen above, in this case all security patterns and control technologies related to Authentication are presented to the user, along with all the corresponding detailed information. In addition, all libraries associated with the aforementioned control technologies are presented but only those supported by Java programming language, as the user requested. Please note that the database containing all the information that the Security Patterns service extracts, is continuously updated and enriched. Fields that are currently empty will be gradually filled in the near future.","title":"Service Discovery"},{"location":"security-patterns/#security-patterns-service","text":"","title":"Security Patterns Service"},{"location":"security-patterns/#security-patterns-service-installation","text":"In this wiki page we describe how the Security Patterns Service can be installed.","title":"Security Patterns Service Installation"},{"location":"security-patterns/#installation-using-anaconda","text":"In this section, we provide instructions on how the user can build the python Flask server of the Security Patterns Service from scratch, using the Anaconda virtual environment. The Security Patterns Service is developed to run on Windows systems with python 3.6.* installed. We suggest installing python via the Anaconda distribution as it provides an easy way to create a virtual environment and install dependencies. The configuration steps needed, are described below: Step 1 : Download the latest Anaconda distribution and follow the installation steps described in the Anaconda documentation . Step 2 : Open Anaconda cmd. Running Anaconda cmd activates the base environment. We need to create a specific environment to run the Security Patterns Service. Create a new python 3.6.4 environment by running the following command: conda create --name security_patterns python=3.6.4 This command will result in the creation of a conda environment named security_patterns. In order to activate the new environment, execute the following command: conda activate security_patterns Step 3 : Now that the environment is activated, install the required libraries: conda install -c anaconda flask flask-cors pymongo waitress dnspython colorama Step 4 : Clone the latest Security Patterns version that can be found in the present Github repository of the Security Patterns Service and navigate to the root directory. Step 5 : To start the Security Patterns using the built-in Flask server, use the command promt inside the active Conda environment and execute the following command: python main.py 0.0.0.0 5000 builtin --debug This command will start the built-in Flask server locally (0.0.0.0) on port 5000. Or you can start the Security Patterns using the Waitress server. Use the command promt inside the active Conda environment and execute the following command: python main.py 0.0.0.0 5000 waitress This command will start the Waitress server locally (0.0.0.0) on port 5000. Note that the Waitress mode is higly recommended in real production environments, since it supports scaling and multiple-request handling features.","title":"Installation using Anaconda"},{"location":"security-patterns/#security-patterns-service-usage","text":"In this wiki page we describe how the Security Patterns Service can be used. Given a problem statement and a set of forces acting on the system, design patterns instruct its stakeholders on how to build this system. Patterns in the information technology environment provide information system architects with a technique for creating reusable solutions to design challenges. In this document, we describe the main functionality of the Security Patterns web service and how it can be put in practice in order to provide the user with useful information regarding Security Requirements (e.g. Authentication, Authorization, etc.) Security Patterns (e.g. Authenticator, Cryptographic), Security Control Technologies (e.g. OAuth2.0, Two Factor Authentication, etc.) and Security Libraries in Java or Python that utilize the aforementioned technologies. As part of the SmartCLIDE platform, the current service is an attempt to assist developers in selecting the most suitable security patterns and implementing them in their software. Detailed description and indicative examples of the usage of the Security Patterns web service are provided, to facilitate better understanding. The Security Patterns web service provides the user with valuable information on the security domain as mentioned earlier. All information regarding this service is stored in a mongo database and the information of interest will be extracted and presented to the user when requested. This is achieved through a dedicated API exposed by the RESTful web server, which is, in fact, a simple HTTP GET request. The inputs that need to be provided as parameters to this request are listed below: | Parameter | Description | Required | Valid Inputs | |:------------:|:---------------------------------------------------------------:|:--------:|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:| | requirement | The main security category that the user is interested in. | Yes | A string value acting as security requirement (e.g. Authentication, Authorization, etc.). | | pattern | The security pattern that the user is interested in. | No | A string acting as security pattern (e.g. Authenticator, Cryptographic, etc.). Default value is None. | | technology | The security control technology that the user is interested in. | No | A string acting as security control Technology (e.g. Authenticator, Cryptographic, etc.). Default value is None. | | language | The programming language that the user is interested in employing for their purpose. | No | One of the following string values: Java, Python. Default value is None. | The output of the Security Patterns web service is a JSON file containing the information that the user requested. The JSON file contains the security requirement, the security pattern(s) of the aforementioned requirement, the security control technology(ies) of the corresponding security pattern(s) and the corresponding libraries all accompanied by descriptions, characteristics, online resources and other useful information. It should be noted that the requirement parameter is necessary. Thus, it should be provided by the user otherwise no results will be returned. If no values are given for all the rest of the parameters, all results related to that particular requirement will be presented. The produced files are usually very long especially for the case that the user selects a requirement without specifying any other parameter. By setting the value of the language parameter to \u201dJava\u201d or \u201cPython\u201d the user narrows down the libraries that will be returned to those designed only for Java or Python respectively. For better understanding, an example is presented demonstrating how the Security Patterns web service can be invoked through a curl command. In the given example, we set the following parameters: requirement : Authentication pattern : Authenticator technology : OAuth2.0 language : Java Hence, the following HTTP GET Request needs to be submitted: http://160.40.53.132:5000/DataBase?requirement=Authentication&pattern=Authenticator&technology=OAuth2.0&language=Java After submitting the request, the Security Patterns service is invoked. In brief, the service selects the \u201cAuthentication\u201d security requirement, the \u201cAuthenticator\u201d security pattern, the \u201cOAuth2.0\u201d security control technology and \u201cJava\u201d programming language and returns all the relevant information. After the successful execution of the analysis, a JSON report with the results is produced and sent as a response to the user. The produced JSON for the above example is presented below. { \"requirement name\":\"Authentication\", \"_id\":\"61dd5c88c43a7d33907484b2\", \"description\":\"The system must validate the identity of its externals before interacting with them. Customers' identities must be authenticated in order to prevent unauthorized access.\", \"standard\":\"ISO 27001\", \"patterns\":{ \"pattern name\":\"Authenticator\", \"_id\":\"61dd5cefc43a7d33907484b7\", \"description\":\"The Authenticator pattern describes a general mechanism for providing identification and authentication to a server from a client. It has the added feature of allowing protocol negotiation to take place using the same procedures. The pattern operates by offering an authentication negotiation object which then provides the protected object only after authentication is successful.\", \"pattern type\":\"design pattern\", \"technologies\":[ { \"_id\":\"61dd5d46c43a7d33907484c2\", \"technology name\":\"OAuth2.0\", \"description\":\"OAuth 2.0 is the industry-standard protocol for authorization. OAuth 2.0 focuses on client developer simplicity while providing specific authorization flows for web applications, desktop applications, mobile phones, and living room devices. This specification and its extensions are being developed within the IETF OAuth Working Group.\", \"strength\":\"\", \"related_security_libraries\":[ { \"_id\":\"61dd5e10c43a7d33907484c9\", \"library name\":\"Spring Security\", \"language\":\"Java\", \"resource\":[ \"https://spring.io/projects/spring-security\", \"https://github.com/spring-projects/spring-security\" ], \"guidelines\":[ \"https://google\", \"https://youtube\" ], \"code examples\":[ ] }, { \"_id\":\"61dd5e10c43a7d33907484ca\", \"library name\":\"Google Auth\", \"language\":\"Java\", \"resource\":[ \"https://github.com/googleapis/google-auth-library-java\" ], \"guidelines\":[ \"https://google\", \"https://youtube\" ], \"code examples\":[ { \"description\":\"This example shows how you can add the \\u2026\", \"code\":\"public void \\u2026.\\n \\n\" } ] } ] } ] } } As can be seen above, the JSON object consists of five main elements, which are listed below: requirement name : which is the main security requirement that the user is interested in (in this case \u201cAuthentication\u201d). id : which is the id number of this particular requirement as it is stored in the mongo database (in this case \u201c61dd5c88c43a7d33907484b2\u201d). description : which is the requirement\u2019s basic description (in this case \u201cThe system must validate the identity of its externals before interacting with them. Customers' identities must be authenticated in order to prevent unauthorized access.\u201d). standard : which is the standard that the requirement has been extracted from (in this case \"ISO 27001\"). patterns : which contains all related security patterns to that specific security requirement or the specific security pattern that the user requested (in this case \u201cAuthenticator\u201d). Accordingly to the requirement, the information regarding the patterns, technologies and libraries can be seen in the JSON file. Another example is presented below where only two parameters are specified in case the user is interested in a particular requirement that will be processed in a specific programming language. In this case, the user needs to be informed of all potential security patterns and control technologies that can be put in practice. The parameters for this example are shown below: requirement : Authentication language : Java The following HTTP GET Request needs to be submitted: http://160.40.53.132:5000/DataBase?requirement=Authentication&language=Java After submitting the request, the Security Patterns service returns the following JSON report with the results: { \"requirement name\":\"Authentication\", \"_id\":\"61dd5c88c43a7d33907484b2\", \"description\":\"The system must validate the identity of its externals before interacting with them. Customers' identities must be authenticated in order to prevent unauthorized access.\", \"standard\":\"ISO 27001\", \"patterns\":[ { \"pattern name\":\"Authenticator\", \"_id\":\"61dd5cefc43a7d33907484b7\", \"description\":\"The Authenticator pattern describes a general mechanism for providing identification and authentication to a server from a client. It has the added feature of allowing protocol negotiation to take place using the same procedures. The pattern operates by offering an authentication negotiation object which then provides the protected object only after authentication is successful.\", \"pattern type\":\"design pattern\", \"technologies\":[ { \"_id\":\"61dd5d46c43a7d33907484c0\", \"technology name\":\"Basic Authentication\", \"description\":\"\", \"strength\":\"\", \"related_security_libraries\":[ { \"_id\":\"61dd5e10c43a7d33907484c9\", \"library name\":\"Spring Security\", \"language\":\"Java\", \"resource\":[ \"https://spring.io/projects/spring-security\", \"https://github.com/spring-projects/spring-security\" ], \"guidelines\":[ \"https://www.youtube.com/watch?v=her_7pa0vrg&ab_channel=Amigoscode\" ], \"code examples\":[ ] } ] }, { \"_id\":\"61dd5d46c43a7d33907484c1\", \"technology name\":\"OAuth1.0\", \"description\":\"OAuth 2.1 is an in-progress effort to consolidate and simplify the most commonly used features of OAuth 2.0.\", \"strength\":\"\", \"related_security_libraries\":[ { \"_id\":\"61dd5e10c43a7d33907484c9\", \"library name\":\"Spring Security\", \"language\":\"Java\", \"resource\":[ \"https://spring.io/projects/spring-security\", \"https://github.com/spring-projects/spring-security\" ], \"guidelines\":[ \"https://www.youtube.com/watch?v=her_7pa0vrg&ab_channel=Amigoscode\" ], \"code examples\":[ ] } ] }, { \"_id\":\"61dd5d46c43a7d33907484c2\", \"technology name\":\"OAuth2.0\", \"description\":\"OAuth 2.0 is the industry-standard protocol for authorization. OAuth 2.0 focuses on client developer simplicity while providing specific authorization flows for web applications, desktop applications, mobile phones, and living room devices. This specification and its extensions are being developed within the IETF OAuth Working Group.\", \"strength\":\"\", \"related_security_libraries\":[ { \"_id\":\"61dd5e10c43a7d33907484c9\", \"library name\":\"Spring Security\", \"language\":\"Java\", \"resource\":[ \"https://spring.io/projects/spring-security\", \"https://github.com/spring-projects/spring-security\" ], \"guidelines\":[ \"https://www.youtube.com/watch?v=her_7pa0vrg&ab_channel=Amigoscode\" ], \"code examples\":[ ] }, { \"_id\":\"61dd5e10c43a7d33907484ca\", \"library name\":\"Google Auth\", \"language\":\"Java\", \"resource\":[ \"https://github.com/googleapis/google-auth-library-java\" ], \"guidelines\":[ \"https://google\", \"https://youtube\" ], \"code examples\":[ { \"description\":\"This example shows how you can add the \\u2026\", \"code\":\"public void \\u2026.\\n \\n\" } ] } ] }, { \"_id\":\"61dd5d46c43a7d33907484c4\", \"technology name\":\"Two Factor Authentication\", \"description\":\"\", \"strength\":\"\", \"related_security_libraries\":[ { \"_id\":\"61dd5e10c43a7d33907484c9\", \"library name\":\"Spring Security\", \"language\":\"Java\", \"resource\":[ \"https://spring.io/projects/spring-security\", \"https://github.com/spring-projects/spring-security\" ], \"guidelines\":[ \"https://www.youtube.com/watch?v=her_7pa0vrg&ab_channel=Amigoscode\" ], \"code examples\":[ ] } ] } ] } ] } As can be seen above, in this case all security patterns and control technologies related to Authentication are presented to the user, along with all the corresponding detailed information. In addition, all libraries associated with the aforementioned control technologies are presented but only those supported by Java programming language, as the user requested. Please note that the database containing all the information that the Security Patterns service extracts, is continuously updated and enriched. Fields that are currently empty will be gradually filled in the near future.","title":"Security Patterns Service Usage"},{"location":"security-related-static-analysis/","text":"Installation and Invocation of the Security-related Static Analysis Subcomponent (SSAS) Installation: To install the SSAS and the Sonarqube platfrom, which the security component is base on, we will use Docker and specifically docker compose method to install the two individual containers using only one file. First we need to to clone the project from the repository $ git clone https://iti-gitlab.iti.gr/smartclide/security.git Then we have build the maven project to produce the necessary .jar file. Open a terminal inside the project folder and run $ mvn clean install Then run $ docker compose up The docker command will build and run the 2 containers using their Dockerfiles. Inside the project there is a specific structure of files needed for the installation, apart from the main project files: Files and Folders included for the installation Name Description docker-compose.yml Docker compose file Dockerfile ssas platform Dockerfile CppRules Folder that contains .xml files with all the rules of the C++ analysis Rulesets Folder that contains .xml files with the rules that are going to analyze sonarqube Folder that contains the Dockerfile that is needed to built SSAS platform container docker-compose.yml is the one that we are going to run and creates sonarqube and ssas containers . Inside we declare the containers and their parameters,the networks and the volumes needed. The parameters that need to be defined are presented in the table below Container parameters Parameter Name Description container_name the name we give to the container build the directory where the Dockerfile needed for the build is located. image the name we give to the docker image. ports mapping of the ports from the container to the host networks which docker network the container will be connected to volumes volumes that are going to be used for the container Then in the \u201cnetworks\u201d section we declare and create the network that the 2 containers will use to communicate with each other. And lastly we declare and create the docker volumes that will be used. You can see the docker-compose.yml file below: docker-compose.yml version: \"3.2\" services: sonarqube: container_name: sonarqube build: sonarqube image: smartclide2022/sonar ports: - \"9000:9000\" networks: - custom-bridge2 volumes: - sonarqube_logs:/opt/sonarqube/logs - sonarqube_data:/opt/sonarqube/data - sonarqube_extensions:/opt/sonarqube/extensions ssas: container_name: ssas build: . image: smartclide2022/ssas ports: - \"8080:8080\" networks: - custom-bridge2 networks: custom-bridge2: external: false volumes: sonarqube_logs: sonarqube_data: sonarqube_extensions: Below it is attached the Dockerfile that installs the SSAS platform container. First we install the OS needed for container and the necessary tools that we will use like maven and cpp check and also Node Js that the container downloads and declares its environmental path. Then we declare the name of token that the container will use to access Sonarqube, and store it as an environmental variable. After that we download the PMD tool needed for the analysis of Maven projects and the sonar scanner that will be used for Python and Javascript projects. We also copy the neccesary folders \u201cCppRules\u201d and \u201cRulesets\u201d which contain .xml files for the rules that we will be used for the analysis of the C++ projects. Lastly we copy the main .jar file that contains the Java Spring Boot application that we built for the API of the SSAS platform and we expose the port 8080 to reach the application. Dockerfile.yml that installs SSAS platform container ```FROM ubuntu:18.04 RUN apt-get update && apt-get install default-jdk -y RUN apt-get update && apt-get install cppcheck -y RUN apt install maven -y RUN apt-get install curl -y ENV PATH=\"/opt/node-v14.17.1-linux-x64/bin:${PATH}\" RUN curl https://nodejs.org/dist/v14.17.1/node-v14.17.1-linux-x64.tar.gz |tar xzf - -C /opt/ WORKDIR /opt/app ENV HOME=/opt/app RUN apt install wget -y RUN wget https://github.com/pmd/pmd/releases/download/pmd_releases%2F6.30.0/pmd-bin-6.30.0.zip -P /opt/app RUN apt-get install unzip -y RUN chmod -R 777 /opt/app RUN chmod -R 700 /opt/app/pmd-bin-6.30.0.zip RUN unzip pmd-bin-6.30.0.zip -d /opt/app/ RUN chmod -R 700 /opt/app/pmd-bin-6.30.0/ RUN wget https://binaries.sonarsource.com/Distribution/sonar-scanner-cli/sonar-scanner-cli-4.7.0.2747-linux.zip -P /opt/app RUN unzip sonar-scanner-cli-4.7.0.2747-linux.zip -d /opt/app ADD CppRules /opt/resources/CppRules ADD Rulesets /opt/resources/Rulesets #/ADD sonar-scanner-cli-4.7.0.2747-linux /opt/app #/ADD pmd-bin-6.30.0 /opt/app #/bin/bash ENV PATH=\"/opt/app/sonar-scanner-4.7.0.2747-linux/bin:${PATH}\" RUN pwd COPY target/Theia-BackEnd-0.0.1-SNAPSHOT.jar app.jar ENTRYPOINT [\"java\", \"-jar\", \"app.jar\"] EXPOSE 8080 Below we attach also the Dockerfile that installs Sonarqube platform container. First it installs Sonarqube using the image for the repository and then copies the CXX plugin needed to run C++analysis. You may need to update the version of the CXX plugin if there is a new release and copy it\u2019s url in the Dockerfile **Dockerfile that installs Sonarqube platform container.** ``` FROM sonarqube:9.6.1-community COPY CXXplugin/sonar-cxx-plugin-2.1.0.311.jar /opt/sonarqube/extensions/plugins/ ``` ### Usage of the SSAS Platform through REST API In this section, we present how the SSAS platform can be utilized for analazying the security level of a given software application. In particular, we present the REST API of the SSAS platform, by giving information about the endpoint, the type of request, and the mandatory and optional parameters that have to be provided. Indicative examples are also provided to help the reader understand how the SSAS service can be invoked, what information it returns after a successful analysis, and how the results of the analysis can be interpreted. After setting up the SSAS microservice (along with the required SonarQube instance) by following the instructions provided in Section 4.6.1, the SSAS service is accessible through the following endpoint: ```<local_IP>:<defined_port>/smartclide/analyze``` In the above endpoint, the <local_ip> placeholder should be replaced with the IP of the machine on which the SSAS microservice (i.e., the Docker Container) has been deployed, while the <defined_port> placeholder should be replaced with the port that was assigned to the SSAS Docker Container during the installation. This port is by default the port 8080; however, the users can use any port they wish, by properly defining it in the docker run command that builds the SSAS container It should be noted that SSAS has a single endpoint for performing the analysis of any software project that is written in one of its supported languages, namely Java, Python, JavaScript, C, and C++. In order to perform the analysis, the user needs to submit an HTTP POST request by providing a set of parameters in its body. These parameters are described in Table below **The parameters of the HTTP Request that should be submitted for analysing a specific software project using SSAS** |Parameter Name | Description | | ------------- |:-------------:| | url | The url of the project\u2019s repository to be analysed. This url must be a url from an online repository like GitHub, GitLab, and Bitbucket. | | language | Indicating the implementation language of the project. The possible values are Java, Python, JavaScript, and Cpp. | It gives a brief description of the parameters that are necessary in order to analyse a software project with SSAS. However, since some of them are quite complicated to understand, we provide a set of detailed examples, in order to further facilitate the understanding of the service. In particular, we showcase how the service could be used for analysing open-source software applications, and we provide the exact parameters that need to be applied. This will allow the reader to execute those requests in order to understand how the analysis work, as well as to prepare custom requests for analysing their own projects, by properly modifying the parameters of these examples. The user also needs to provide JSON description of the model to the Body of the request, a description of which is provided in the box below: A JSON containing the security categories, which are, in fact the properties of the security model that is used by the Security Measures Computation module (see D2.2), which are derived from external static analysis tools (namely PMD and CppCheck). For each one of these security categories/properties, similarly to the properties parameter/JSON described above, their thresholds that are used for the calculations of the Measures Computation module need to be provided. In particular, for each one of the security categories, an array with three values, which correspond to the lower, mid, and upper threshold of the corresponding category should be provided. This JSON also contains the Characteristics of the security model along with their weights that are required for the computation of the high-level measures, and the overall Security Index of the project. Also containing the names of the security categories that are supported by SonarQube. These security categories are the properties of the security model that are quantified by SonarQube. For each one of these security categories/properties, similarly to the properties parameter/JSON described above, their thresholds that are used for the calculations of the Measures Computation module need to be provided. In particular, for each one of the security categories, an array with three values, which correspond to the lower, mid, and upper threshold of the corresponding category should be provided. **JSON body raw** ```{\"CK\":{\"lcom\":[0,0.10910936800871021,3.1849529780564267],\"cbo\":[0.017050298380221656,0.03692993475020107,0.5714285714285714],\"wmc\":[0.13793103448275863,0.04986595433654195,0.2765273311897106]},\"PMD\":{\"ExceptionHandling\":[0,0.22938518010164353,12.987012987012987],\"Assignment\":[0,0.11160028050045479,7.6923076923076929],\"Logging\":[0,0.05692917472098835,6.8493150684931509],\"NullPointer\":[0,0.32358608981534067,25.966183574879229],\"ResourceHandling\":[0,2.201831659093579,166.66666666666667],\"MisusedFunctionality\":[0,0.13732179935769163,4.784688995215311]},\"Characteristics\":{\"Confidentiality\":[0.005,0.005,0.005,0.1,0.1,0.1,0.01,0.01,0.01,0.1,0.1,0.005,0.2,0.15,0.1],\"Integrity\":[0.01,0.005,0.005,0.1,0.15,0.01,0.01,0.01,0.01,0.15,0.15,0.01,0.16,0.21,0.01],\"Availability\":[0.005,0.005,0.01,0.1,0.01,0.01,0.2,0.3,0.01,0.01,0.01,0.3,0.01,0.01,0.01]},\"metricKeys\":{\"vulnerabilities\":[0,0.09848484848484848,4]},\"Sonarqube\":{\"sql-injection\":[0,0.013234192551328933,1.5479876160990714],\"dos\":[0,0.024419175132769336,2.2172949002217297],\"weak-cryptography\":[0,0.0015070136414874827,0.1989258006763477],\"auth\":[0,0.024207864640426639,3.0959752321981428],\"insecure-conf\":[0,0.7356100591012389,32.05128205128205]}}``` As can be seen by Table the JSON body is properly defined in order to encapsulate all the details of the security model that we proposed for Java applications . In particular, the JSON body contains all the Security Categories that were selected from the external static code analyser, namely PMD. Those properties are Exception Handling, Assignment, Logging, Null Pointer, Resource Handling, and Misused Functionality. For each one of these properties, there is an array with three numbers. Those three numbers correspond to the lower, mid, and upper threshold of each property For instance, the values of Assignment are 0, 0.11, 7.69, which are the same values reported . It should be noted, that the users can also compute some software metrics through the external CK tool. In this example, we state that the metrics lcom, cbo, and wmc, should be also computed. However, those values are not taken into account for the computation of the higher-level metrics as well as the overall Security Index of the project. Finally, in the same parameter, we also define the Characteristics of the model (which are the high-level measures that need to be computed). As can be seen, we have defined three Characteristics, namely Confidentiality, Availability, and Integrity. For each Characteristic there is an array containing the weights that need to be used for computing their overall score from the scores of the identified Properties. The selected Characteristics and weights of the given example are those of the Java model The Sonarqube parameter contains all those Security Categories, which are retrieved from the SonarQube Platform. As can be seen, we have selected those properties that have been defined for the Java model in . Inside the arrays, we have also added their thresholds. ### Scan Java project from zip file There is the option to scan a Java project that contains already its binary files. This option is available using the following endpoint ```<local_IP>:<defined_port>/smartclide/analyze_local``` In this case we need to pass our parameters as form-data inside the body. | Key | Value |Content-type | |-----------------|---|---| | zip | Select the zip file that contains the Java project | multipart-form-data | | sonarProperties | Json containing the properties for the analysis(same as above we used in JSON body raw |application-json After the analysis is complete, the service returns a JSON file with the results of the analysis. The response that is produced by SSAS is attached Below ### Response { \"CK\": { \"loc\": 190159.0, \"cbo\": 0.08822616862730662, \"lcom\": 0.8416430460824889, \"wmc\": 0.1914345363616763 }, \"PMD\": { \"Assignment\": 0.6731209146030427, \"Logging\": 0.07888135718004408, \"NullPointer\": 3.0553379014403736, \"MisusedFunctionality\": 0.9202825004338475, \"ResourceHandling\": 0.40492430019089287, \"ExceptionHandling\": 1.4934870292755011 }, \"Sonarqube\": { \"insecure-conf\": 0.1191886234458921, \"auth\": 0.0, \"weak-cryptography\": 0.007449288965368256, \"dos\": 0.09684075654978733, \"sql-injection\": 0.0 }, \"metrics\": { \"ncloc\": 134241.0, \"vulnerabilities\": 4.0 }, \"Property_Scores\": { \"Logging\": 0.49838405953354453, \"NullPointer\": 0.44673410497481375, \"MisusedFunctionality\": 0.41576298277290863, \"auth\": 1.0, \"lcom\": 0.38092150139219616, \"ExceptionHandling\": 0.4504570180167413, \"dos\": 0.4834870757634748, \"sql_injection\": 1.0, \"wmc\": 0.1877090751177195, \"weak_cryptography\": 0.4849500764007037, \"Assignment\": 0.46296383677676317, \"cbo\": 0.4520146260840956, \"insecure_conf\": 0.9189865459483278, \"ResourceHandling\": 0.9080483063910556 }, \"Characteristic_Scores\": { \"Availability\": 0.6791253872286296, \"Confidentiality\": 0.6237102387826031, \"Integrity\": 0.672086307863198 }, \"Security_index\": { \"Security_Index\": 0.6583073112914769 }, \"Hotspots\": { \"insecure-conf\": [ { \"component\": \"struts:core/src/main/java/org/apache/struts2/interceptor/I18nInterceptor.java\", \"project\": \"struts\", \"securityCategory\": \"insecure-conf\", \"vulnerabilityProbability\": \"LOW\", \"line\": 398, \"message\": \"Make sure creating this cookie without the \\\"secure\\\" flag is safe here.\", \"textRange\": { \"startLine\": 398, \"endLine\": 398, \"startOffset\": 32, \"endOffset\": 38 } }, { \"component\": \"struts:core/src/main/java/org/apache/struts2/result/plain/HttpCookies.java\", \"project\": \"struts\", \"securityCategory\": \"insecure-conf\", \"vulnerabilityProbability\": \"LOW\", \"line\": 31, \"message\": \"Make sure creating this cookie without the \\\"secure\\\" flag is safe here.\", \"textRange\": { \"startLine\": 31, \"endLine\": 31, \"startOffset\": 24, \"endOffset\": 30 } }, ... .. \"PMD_issues\": { \"Assignment\": [ { \"Problem\": \"1\", \"Package\": \"org.apache.struts2.showcase.source\", \"File\": \"/home/upload/struts/apps/showcase/src/main/java/org/apache/struts2/showcase/source/ViewSourceAction.java\", \"Priority\": \"3\", \"Line\": \"212\", \"Description\": \"Avoid assignments in operands\", \"Rule set\": \"Error Prone\", \"Rule\": \"AssignmentInOperand\" }, { \"Problem\": \"1\", \"Package\": \"org.apache.struts2.showcase.source\", \"File\": \"/home/upload/struts/apps/showcase/target/classes/org/apache/struts2/showcase/source/ViewSourceAction.java\", \"Priority\": \"3\", \"Line\": \"212\", \"Description\": \"Avoid assignments in operands\", \"Rule set\": \"Error Prone\", \"Rule\": \"AssignmentInOperand\" }, { \"Problem\": \"1\", \"Package\": \"org.apache.struts2.showcase.source\", \"File\": \"/home/upload/struts/apps/showcase/target/struts2-showcase/WEB-INF/classes/org/apache/struts2/showcase/source/ViewSourceAction.java\", \"Priority\": \"3\", \"Line\": \"212\", \"Description\": \"Avoid assignments in operands\", \"Rule set\": \"Error Prone\", \"Rule\": \"AssignmentInOperand\" }, ... ... As shown in the response, the first part of the JSON consists of the normalized values of the properties selected from each tool. Those values are the values that were used for computing the Property Scores of the defined properties, upon which the calculation of the higher-level measures, and, in turn, the security index is based. Afterwards, the \u201cProperty Scores\u201d includes the property scores as calculated by the utility function and the thresholds per each property. Following that, \u201cCharacteristics Scores\u201d included as calculated by the multiplication of the weight and the property score per each characteristic.The overall \u201cSecurity Index\u201d of the project analysed is included, containing the security score as it is calculated by the characteristic scores. Finally the detailed issues from the PMD analysis are shown in the case of Maven project, Hotspots(for Maven, Javascript and Python projects, and CPPcheck issues for CPP projects. ### Usage of the VA Model through REST API In this section, we present how the VA model can be utilized for predicting the vulnerable software components of a given software application. In particular, we present the REST API of the VA model, by giving information about the endpoint, the type of request, and the mandatory and optional parameters that have to be provided. An indicative example is also provided to help the reader understand how the VA API can be invoked, what information is returned after a successful analysis, and how the results of the analysis can be interpreted. The VA service is accessible through the following endpoint: ```<local_IP>:<defined_port>/smartclide/VulnerabilityAssessment ``` In the above endpoint, the <local_ip> placeholder should be replaced with the IP of the machine on which the VA microservice has been deployed (the same with SSAS), while the <defined_port> placeholder should be replaced with the port that was assigned to the Security Component during the installation. This port is by default the port 8080; however, the users can use any port they wish, by properly defining it in the docker run command that builds the container. It should be noted that the VA has a single endpoint for performing the analysis of any software project that is written in one of its supported languages, namely Java, Python, JavaScript, C, and C++. In order to perform the analysis, the user needs to submit an HTTP GET request by providing a set of parameters. These parameters are described in Table below: | Parameter Name | Description | | ------------- |:-------------| | project | The url of the project\u2019s repository to be analysed. This url must be a url from an online repository like GitHub, GitLab, and Bitbucket. | | lang | Indicating the implementation language of the project. The possible values are \u201cjava\u201d, \u201cpython\u201d, \u201cjavascript\u201d, and \u201ccpp\u201d. | |user_name (optional) | The name of the user of the project\u2019s Git repository. | The above table gives a brief description of the parameters that are necessary in order to analyse a software project with VA. In order to further facilitate the understanding of the service, we provide a detailed example. In particular, we showcase how the service could be used for analysing open-source software applications, and we provide the exact parameters that need to be applied. This will allow the reader to execute those requests in order to understand how the analysis work, as well as to prepare custom requests for analysing their own projects, by properly modifying the parameters of this example. |Parameter Name | Description | | ------------- |:-------------| | project | https://github.com/apache/cordova-ios.git | lang | javascript | After the analysis is complete, the service returns a JSON file with the results of the analysis. A fragment of the response that is produced by VA is attached below: \"commit\": \"f804a42e171b2ec9288421e0ce63eb35bb0afe14\", \"date\": \"19/10/2022 17:11:36\", \"project_name\": \"cordova-ios_1666199484118\", \"results\": [ { \"class_name\": \"create.spec.js\", \"confidence\": \"87 %\", \"is_vulnerable\": 1, \"package\": \"/apache_cordova-ios_1666199484118/tests/spec\", \"path\": \"VulnerabilityTestRepo/apache_cordova-ios_1666199484118/tests/spec/create.spec.js\", \"sigmoid\": 0.8755856752 }, { \"class_name\": \"BridgingHeader.spec.js\", \"confidence\": \"10 %\", \"is_vulnerable\": 0, \"package\": \"/apache_cordova-ios_1666199484118/tests/spec/unit\", \"path\": \"VulnerabilityTestRepo/apache_cordova-ios_1666199484118/tests/spec/unit/BridgingHeader.spec.js\", \"sigmoid\": 0.1044444144 }, { \"class_name\": \"PodsJson.spec.js\", \"confidence\": \"41 %\", \"is_vulnerable\": 0, \"package\": \"/apache_cordova-ios_1666199484118/tests/spec/unit\", \"path\": \"VulnerabilityTestRepo/apache_cordova-ios_1666199484118/tests/spec/unit/PodsJson.spec.js\", \"sigmoid\": 0.4148975015 }, ... ... { \"class_name\": \"versions.spec.js\", \"confidence\": \"16 %\", \"is_vulnerable\": 0, \"package\": \"/apache_cordova-ios_1666199484118/tests/spec/unit\", \"path\": \"VulnerabilityTestRepo/apache_cordova-ios_1666199484118/tests/spec/unit/versions.spec.js\", \"sigmoid\": 0.161198765 }] } ``` As can be seen from the above fragment, the JSON report comprises an array named \u201cresults\u201d, which contains several JSON Objects. Each one of these JSON Objects contains relevant information for each source code file of the analyzed application. More specifically, it contains the following entries: \u2022 class_name: The name of the source code file to which this JSON Object refers. \u2022 path: The exact path of the source code file to which this JSON Object refers. \u2022 package: The package of the source code file to which this JSON Object refers. \u2022 is_vulnerable: The vulnerability status of the corresponding source code file. It takes two possible values, i.e., 0 if the model decides that the source code file is potentially clean, and 1 if the model decides that the source code file is potentially vulnerable. \u2022 sigmoid: The actual output of the neural network. It corresponds to the probability of the source code file to be vulnerable. This value is actually used by the model in order to define the vulnerability status of the corresponding source code file (i.e., the value of the \u201cis_vulnerable\u201d entry of the corresponding JSON Object).","title":"Service Discovery"},{"location":"security-related-static-analysis/#installation-and-invocation-of-the-security-related-static-analysis-subcomponent-ssas","text":"Installation: To install the SSAS and the Sonarqube platfrom, which the security component is base on, we will use Docker and specifically docker compose method to install the two individual containers using only one file. First we need to to clone the project from the repository $ git clone https://iti-gitlab.iti.gr/smartclide/security.git Then we have build the maven project to produce the necessary .jar file. Open a terminal inside the project folder and run $ mvn clean install Then run $ docker compose up The docker command will build and run the 2 containers using their Dockerfiles. Inside the project there is a specific structure of files needed for the installation, apart from the main project files:","title":"Installation and Invocation of the Security-related Static Analysis Subcomponent (SSAS)"},{"location":"security-related-static-analysis/#files-and-folders-included-for-the-installation","text":"Name Description docker-compose.yml Docker compose file Dockerfile ssas platform Dockerfile CppRules Folder that contains .xml files with all the rules of the C++ analysis Rulesets Folder that contains .xml files with the rules that are going to analyze sonarqube Folder that contains the Dockerfile that is needed to built SSAS platform container docker-compose.yml is the one that we are going to run and creates sonarqube and ssas containers . Inside we declare the containers and their parameters,the networks and the volumes needed. The parameters that need to be defined are presented in the table below Container parameters Parameter Name Description container_name the name we give to the container build the directory where the Dockerfile needed for the build is located. image the name we give to the docker image. ports mapping of the ports from the container to the host networks which docker network the container will be connected to volumes volumes that are going to be used for the container Then in the \u201cnetworks\u201d section we declare and create the network that the 2 containers will use to communicate with each other. And lastly we declare and create the docker volumes that will be used. You can see the docker-compose.yml file below: docker-compose.yml version: \"3.2\" services: sonarqube: container_name: sonarqube build: sonarqube image: smartclide2022/sonar ports: - \"9000:9000\" networks: - custom-bridge2 volumes: - sonarqube_logs:/opt/sonarqube/logs - sonarqube_data:/opt/sonarqube/data - sonarqube_extensions:/opt/sonarqube/extensions ssas: container_name: ssas build: . image: smartclide2022/ssas ports: - \"8080:8080\" networks: - custom-bridge2 networks: custom-bridge2: external: false volumes: sonarqube_logs: sonarqube_data: sonarqube_extensions: Below it is attached the Dockerfile that installs the SSAS platform container. First we install the OS needed for container and the necessary tools that we will use like maven and cpp check and also Node Js that the container downloads and declares its environmental path. Then we declare the name of token that the container will use to access Sonarqube, and store it as an environmental variable. After that we download the PMD tool needed for the analysis of Maven projects and the sonar scanner that will be used for Python and Javascript projects. We also copy the neccesary folders \u201cCppRules\u201d and \u201cRulesets\u201d which contain .xml files for the rules that we will be used for the analysis of the C++ projects. Lastly we copy the main .jar file that contains the Java Spring Boot application that we built for the API of the SSAS platform and we expose the port 8080 to reach the application.","title":"Files and Folders included for the installation"},{"location":"security-related-static-analysis/#dockerfileyml-that-installs-ssas-platform-container","text":"```FROM ubuntu:18.04 RUN apt-get update && apt-get install default-jdk -y RUN apt-get update && apt-get install cppcheck -y RUN apt install maven -y RUN apt-get install curl -y ENV PATH=\"/opt/node-v14.17.1-linux-x64/bin:${PATH}\" RUN curl https://nodejs.org/dist/v14.17.1/node-v14.17.1-linux-x64.tar.gz |tar xzf - -C /opt/ WORKDIR /opt/app ENV HOME=/opt/app RUN apt install wget -y RUN wget https://github.com/pmd/pmd/releases/download/pmd_releases%2F6.30.0/pmd-bin-6.30.0.zip -P /opt/app RUN apt-get install unzip -y RUN chmod -R 777 /opt/app RUN chmod -R 700 /opt/app/pmd-bin-6.30.0.zip RUN unzip pmd-bin-6.30.0.zip -d /opt/app/ RUN chmod -R 700 /opt/app/pmd-bin-6.30.0/ RUN wget https://binaries.sonarsource.com/Distribution/sonar-scanner-cli/sonar-scanner-cli-4.7.0.2747-linux.zip -P /opt/app RUN unzip sonar-scanner-cli-4.7.0.2747-linux.zip -d /opt/app ADD CppRules /opt/resources/CppRules ADD Rulesets /opt/resources/Rulesets #/ADD sonar-scanner-cli-4.7.0.2747-linux /opt/app #/ADD pmd-bin-6.30.0 /opt/app #/bin/bash ENV PATH=\"/opt/app/sonar-scanner-4.7.0.2747-linux/bin:${PATH}\" RUN pwd COPY target/Theia-BackEnd-0.0.1-SNAPSHOT.jar app.jar ENTRYPOINT [\"java\", \"-jar\", \"app.jar\"] EXPOSE 8080 Below we attach also the Dockerfile that installs Sonarqube platform container. First it installs Sonarqube using the image for the repository and then copies the CXX plugin needed to run C++analysis. You may need to update the version of the CXX plugin if there is a new release and copy it\u2019s url in the Dockerfile **Dockerfile that installs Sonarqube platform container.** ``` FROM sonarqube:9.6.1-community COPY CXXplugin/sonar-cxx-plugin-2.1.0.311.jar /opt/sonarqube/extensions/plugins/ ``` ### Usage of the SSAS Platform through REST API In this section, we present how the SSAS platform can be utilized for analazying the security level of a given software application. In particular, we present the REST API of the SSAS platform, by giving information about the endpoint, the type of request, and the mandatory and optional parameters that have to be provided. Indicative examples are also provided to help the reader understand how the SSAS service can be invoked, what information it returns after a successful analysis, and how the results of the analysis can be interpreted. After setting up the SSAS microservice (along with the required SonarQube instance) by following the instructions provided in Section 4.6.1, the SSAS service is accessible through the following endpoint: ```<local_IP>:<defined_port>/smartclide/analyze``` In the above endpoint, the <local_ip> placeholder should be replaced with the IP of the machine on which the SSAS microservice (i.e., the Docker Container) has been deployed, while the <defined_port> placeholder should be replaced with the port that was assigned to the SSAS Docker Container during the installation. This port is by default the port 8080; however, the users can use any port they wish, by properly defining it in the docker run command that builds the SSAS container It should be noted that SSAS has a single endpoint for performing the analysis of any software project that is written in one of its supported languages, namely Java, Python, JavaScript, C, and C++. In order to perform the analysis, the user needs to submit an HTTP POST request by providing a set of parameters in its body. These parameters are described in Table below **The parameters of the HTTP Request that should be submitted for analysing a specific software project using SSAS** |Parameter Name | Description | | ------------- |:-------------:| | url | The url of the project\u2019s repository to be analysed. This url must be a url from an online repository like GitHub, GitLab, and Bitbucket. | | language | Indicating the implementation language of the project. The possible values are Java, Python, JavaScript, and Cpp. | It gives a brief description of the parameters that are necessary in order to analyse a software project with SSAS. However, since some of them are quite complicated to understand, we provide a set of detailed examples, in order to further facilitate the understanding of the service. In particular, we showcase how the service could be used for analysing open-source software applications, and we provide the exact parameters that need to be applied. This will allow the reader to execute those requests in order to understand how the analysis work, as well as to prepare custom requests for analysing their own projects, by properly modifying the parameters of these examples. The user also needs to provide JSON description of the model to the Body of the request, a description of which is provided in the box below: A JSON containing the security categories, which are, in fact the properties of the security model that is used by the Security Measures Computation module (see D2.2), which are derived from external static analysis tools (namely PMD and CppCheck). For each one of these security categories/properties, similarly to the properties parameter/JSON described above, their thresholds that are used for the calculations of the Measures Computation module need to be provided. In particular, for each one of the security categories, an array with three values, which correspond to the lower, mid, and upper threshold of the corresponding category should be provided. This JSON also contains the Characteristics of the security model along with their weights that are required for the computation of the high-level measures, and the overall Security Index of the project. Also containing the names of the security categories that are supported by SonarQube. These security categories are the properties of the security model that are quantified by SonarQube. For each one of these security categories/properties, similarly to the properties parameter/JSON described above, their thresholds that are used for the calculations of the Measures Computation module need to be provided. In particular, for each one of the security categories, an array with three values, which correspond to the lower, mid, and upper threshold of the corresponding category should be provided. **JSON body raw** ```{\"CK\":{\"lcom\":[0,0.10910936800871021,3.1849529780564267],\"cbo\":[0.017050298380221656,0.03692993475020107,0.5714285714285714],\"wmc\":[0.13793103448275863,0.04986595433654195,0.2765273311897106]},\"PMD\":{\"ExceptionHandling\":[0,0.22938518010164353,12.987012987012987],\"Assignment\":[0,0.11160028050045479,7.6923076923076929],\"Logging\":[0,0.05692917472098835,6.8493150684931509],\"NullPointer\":[0,0.32358608981534067,25.966183574879229],\"ResourceHandling\":[0,2.201831659093579,166.66666666666667],\"MisusedFunctionality\":[0,0.13732179935769163,4.784688995215311]},\"Characteristics\":{\"Confidentiality\":[0.005,0.005,0.005,0.1,0.1,0.1,0.01,0.01,0.01,0.1,0.1,0.005,0.2,0.15,0.1],\"Integrity\":[0.01,0.005,0.005,0.1,0.15,0.01,0.01,0.01,0.01,0.15,0.15,0.01,0.16,0.21,0.01],\"Availability\":[0.005,0.005,0.01,0.1,0.01,0.01,0.2,0.3,0.01,0.01,0.01,0.3,0.01,0.01,0.01]},\"metricKeys\":{\"vulnerabilities\":[0,0.09848484848484848,4]},\"Sonarqube\":{\"sql-injection\":[0,0.013234192551328933,1.5479876160990714],\"dos\":[0,0.024419175132769336,2.2172949002217297],\"weak-cryptography\":[0,0.0015070136414874827,0.1989258006763477],\"auth\":[0,0.024207864640426639,3.0959752321981428],\"insecure-conf\":[0,0.7356100591012389,32.05128205128205]}}``` As can be seen by Table the JSON body is properly defined in order to encapsulate all the details of the security model that we proposed for Java applications . In particular, the JSON body contains all the Security Categories that were selected from the external static code analyser, namely PMD. Those properties are Exception Handling, Assignment, Logging, Null Pointer, Resource Handling, and Misused Functionality. For each one of these properties, there is an array with three numbers. Those three numbers correspond to the lower, mid, and upper threshold of each property For instance, the values of Assignment are 0, 0.11, 7.69, which are the same values reported . It should be noted, that the users can also compute some software metrics through the external CK tool. In this example, we state that the metrics lcom, cbo, and wmc, should be also computed. However, those values are not taken into account for the computation of the higher-level metrics as well as the overall Security Index of the project. Finally, in the same parameter, we also define the Characteristics of the model (which are the high-level measures that need to be computed). As can be seen, we have defined three Characteristics, namely Confidentiality, Availability, and Integrity. For each Characteristic there is an array containing the weights that need to be used for computing their overall score from the scores of the identified Properties. The selected Characteristics and weights of the given example are those of the Java model The Sonarqube parameter contains all those Security Categories, which are retrieved from the SonarQube Platform. As can be seen, we have selected those properties that have been defined for the Java model in . Inside the arrays, we have also added their thresholds. ### Scan Java project from zip file There is the option to scan a Java project that contains already its binary files. This option is available using the following endpoint ```<local_IP>:<defined_port>/smartclide/analyze_local``` In this case we need to pass our parameters as form-data inside the body. | Key | Value |Content-type | |-----------------|---|---| | zip | Select the zip file that contains the Java project | multipart-form-data | | sonarProperties | Json containing the properties for the analysis(same as above we used in JSON body raw |application-json After the analysis is complete, the service returns a JSON file with the results of the analysis. The response that is produced by SSAS is attached Below ### Response { \"CK\": { \"loc\": 190159.0, \"cbo\": 0.08822616862730662, \"lcom\": 0.8416430460824889, \"wmc\": 0.1914345363616763 }, \"PMD\": { \"Assignment\": 0.6731209146030427, \"Logging\": 0.07888135718004408, \"NullPointer\": 3.0553379014403736, \"MisusedFunctionality\": 0.9202825004338475, \"ResourceHandling\": 0.40492430019089287, \"ExceptionHandling\": 1.4934870292755011 }, \"Sonarqube\": { \"insecure-conf\": 0.1191886234458921, \"auth\": 0.0, \"weak-cryptography\": 0.007449288965368256, \"dos\": 0.09684075654978733, \"sql-injection\": 0.0 }, \"metrics\": { \"ncloc\": 134241.0, \"vulnerabilities\": 4.0 }, \"Property_Scores\": { \"Logging\": 0.49838405953354453, \"NullPointer\": 0.44673410497481375, \"MisusedFunctionality\": 0.41576298277290863, \"auth\": 1.0, \"lcom\": 0.38092150139219616, \"ExceptionHandling\": 0.4504570180167413, \"dos\": 0.4834870757634748, \"sql_injection\": 1.0, \"wmc\": 0.1877090751177195, \"weak_cryptography\": 0.4849500764007037, \"Assignment\": 0.46296383677676317, \"cbo\": 0.4520146260840956, \"insecure_conf\": 0.9189865459483278, \"ResourceHandling\": 0.9080483063910556 }, \"Characteristic_Scores\": { \"Availability\": 0.6791253872286296, \"Confidentiality\": 0.6237102387826031, \"Integrity\": 0.672086307863198 }, \"Security_index\": { \"Security_Index\": 0.6583073112914769 }, \"Hotspots\": { \"insecure-conf\": [ { \"component\": \"struts:core/src/main/java/org/apache/struts2/interceptor/I18nInterceptor.java\", \"project\": \"struts\", \"securityCategory\": \"insecure-conf\", \"vulnerabilityProbability\": \"LOW\", \"line\": 398, \"message\": \"Make sure creating this cookie without the \\\"secure\\\" flag is safe here.\", \"textRange\": { \"startLine\": 398, \"endLine\": 398, \"startOffset\": 32, \"endOffset\": 38 } }, { \"component\": \"struts:core/src/main/java/org/apache/struts2/result/plain/HttpCookies.java\", \"project\": \"struts\", \"securityCategory\": \"insecure-conf\", \"vulnerabilityProbability\": \"LOW\", \"line\": 31, \"message\": \"Make sure creating this cookie without the \\\"secure\\\" flag is safe here.\", \"textRange\": { \"startLine\": 31, \"endLine\": 31, \"startOffset\": 24, \"endOffset\": 30 } }, ... .. \"PMD_issues\": { \"Assignment\": [ { \"Problem\": \"1\", \"Package\": \"org.apache.struts2.showcase.source\", \"File\": \"/home/upload/struts/apps/showcase/src/main/java/org/apache/struts2/showcase/source/ViewSourceAction.java\", \"Priority\": \"3\", \"Line\": \"212\", \"Description\": \"Avoid assignments in operands\", \"Rule set\": \"Error Prone\", \"Rule\": \"AssignmentInOperand\" }, { \"Problem\": \"1\", \"Package\": \"org.apache.struts2.showcase.source\", \"File\": \"/home/upload/struts/apps/showcase/target/classes/org/apache/struts2/showcase/source/ViewSourceAction.java\", \"Priority\": \"3\", \"Line\": \"212\", \"Description\": \"Avoid assignments in operands\", \"Rule set\": \"Error Prone\", \"Rule\": \"AssignmentInOperand\" }, { \"Problem\": \"1\", \"Package\": \"org.apache.struts2.showcase.source\", \"File\": \"/home/upload/struts/apps/showcase/target/struts2-showcase/WEB-INF/classes/org/apache/struts2/showcase/source/ViewSourceAction.java\", \"Priority\": \"3\", \"Line\": \"212\", \"Description\": \"Avoid assignments in operands\", \"Rule set\": \"Error Prone\", \"Rule\": \"AssignmentInOperand\" }, ... ... As shown in the response, the first part of the JSON consists of the normalized values of the properties selected from each tool. Those values are the values that were used for computing the Property Scores of the defined properties, upon which the calculation of the higher-level measures, and, in turn, the security index is based. Afterwards, the \u201cProperty Scores\u201d includes the property scores as calculated by the utility function and the thresholds per each property. Following that, \u201cCharacteristics Scores\u201d included as calculated by the multiplication of the weight and the property score per each characteristic.The overall \u201cSecurity Index\u201d of the project analysed is included, containing the security score as it is calculated by the characteristic scores. Finally the detailed issues from the PMD analysis are shown in the case of Maven project, Hotspots(for Maven, Javascript and Python projects, and CPPcheck issues for CPP projects. ### Usage of the VA Model through REST API In this section, we present how the VA model can be utilized for predicting the vulnerable software components of a given software application. In particular, we present the REST API of the VA model, by giving information about the endpoint, the type of request, and the mandatory and optional parameters that have to be provided. An indicative example is also provided to help the reader understand how the VA API can be invoked, what information is returned after a successful analysis, and how the results of the analysis can be interpreted. The VA service is accessible through the following endpoint: ```<local_IP>:<defined_port>/smartclide/VulnerabilityAssessment ``` In the above endpoint, the <local_ip> placeholder should be replaced with the IP of the machine on which the VA microservice has been deployed (the same with SSAS), while the <defined_port> placeholder should be replaced with the port that was assigned to the Security Component during the installation. This port is by default the port 8080; however, the users can use any port they wish, by properly defining it in the docker run command that builds the container. It should be noted that the VA has a single endpoint for performing the analysis of any software project that is written in one of its supported languages, namely Java, Python, JavaScript, C, and C++. In order to perform the analysis, the user needs to submit an HTTP GET request by providing a set of parameters. These parameters are described in Table below: | Parameter Name | Description | | ------------- |:-------------| | project | The url of the project\u2019s repository to be analysed. This url must be a url from an online repository like GitHub, GitLab, and Bitbucket. | | lang | Indicating the implementation language of the project. The possible values are \u201cjava\u201d, \u201cpython\u201d, \u201cjavascript\u201d, and \u201ccpp\u201d. | |user_name (optional) | The name of the user of the project\u2019s Git repository. | The above table gives a brief description of the parameters that are necessary in order to analyse a software project with VA. In order to further facilitate the understanding of the service, we provide a detailed example. In particular, we showcase how the service could be used for analysing open-source software applications, and we provide the exact parameters that need to be applied. This will allow the reader to execute those requests in order to understand how the analysis work, as well as to prepare custom requests for analysing their own projects, by properly modifying the parameters of this example. |Parameter Name | Description | | ------------- |:-------------| | project | https://github.com/apache/cordova-ios.git | lang | javascript | After the analysis is complete, the service returns a JSON file with the results of the analysis. A fragment of the response that is produced by VA is attached below: \"commit\": \"f804a42e171b2ec9288421e0ce63eb35bb0afe14\", \"date\": \"19/10/2022 17:11:36\", \"project_name\": \"cordova-ios_1666199484118\", \"results\": [ { \"class_name\": \"create.spec.js\", \"confidence\": \"87 %\", \"is_vulnerable\": 1, \"package\": \"/apache_cordova-ios_1666199484118/tests/spec\", \"path\": \"VulnerabilityTestRepo/apache_cordova-ios_1666199484118/tests/spec/create.spec.js\", \"sigmoid\": 0.8755856752 }, { \"class_name\": \"BridgingHeader.spec.js\", \"confidence\": \"10 %\", \"is_vulnerable\": 0, \"package\": \"/apache_cordova-ios_1666199484118/tests/spec/unit\", \"path\": \"VulnerabilityTestRepo/apache_cordova-ios_1666199484118/tests/spec/unit/BridgingHeader.spec.js\", \"sigmoid\": 0.1044444144 }, { \"class_name\": \"PodsJson.spec.js\", \"confidence\": \"41 %\", \"is_vulnerable\": 0, \"package\": \"/apache_cordova-ios_1666199484118/tests/spec/unit\", \"path\": \"VulnerabilityTestRepo/apache_cordova-ios_1666199484118/tests/spec/unit/PodsJson.spec.js\", \"sigmoid\": 0.4148975015 }, ... ... { \"class_name\": \"versions.spec.js\", \"confidence\": \"16 %\", \"is_vulnerable\": 0, \"package\": \"/apache_cordova-ios_1666199484118/tests/spec/unit\", \"path\": \"VulnerabilityTestRepo/apache_cordova-ios_1666199484118/tests/spec/unit/versions.spec.js\", \"sigmoid\": 0.161198765 }] } ``` As can be seen from the above fragment, the JSON report comprises an array named \u201cresults\u201d, which contains several JSON Objects. Each one of these JSON Objects contains relevant information for each source code file of the analyzed application. More specifically, it contains the following entries: \u2022 class_name: The name of the source code file to which this JSON Object refers. \u2022 path: The exact path of the source code file to which this JSON Object refers. \u2022 package: The package of the source code file to which this JSON Object refers. \u2022 is_vulnerable: The vulnerability status of the corresponding source code file. It takes two possible values, i.e., 0 if the model decides that the source code file is potentially clean, and 1 if the model decides that the source code file is potentially vulnerable. \u2022 sigmoid: The actual output of the neural network. It corresponds to the probability of the source code file to be vulnerable. This value is actually used by the model in order to define the vulnerability status of the corresponding source code file (i.e., the value of the \u201cis_vulnerable\u201d entry of the corresponding JSON Object).","title":"Dockerfile.yml that installs SSAS platform container"},{"location":"service-creation/","text":"SmartCLIDE Service Creation SmartCLIDE Service Creation Backend Component Preconditions to build and run Service Creation To build and run the backend service of Service Creation, the following software is required: Java (at least version 8) Apache Maven (at least version 3.2+) Docker (for building and running the final image) How to build Service Creation Service Creation can be built using maven with the following command: mvn install In order to build a Docker image of the service that can be deployed, the following commands can be used: mvn install docker build -t ${IMAGE_NAME:IMAGE_TAG} . More specifically: mvn install docker build -t smartclide-service-creation:latest . How to run Service Creation All the images of this component can be found here . You can run the backend service with the following command: docker run smartclide-service-creation:latest How to use Service Creation This backend service leverages the provided GitLab and Jenkins APIs in order to authenticate a User and create a new repository based on the options selected by the User. Furthermore, depending on the User\u2019s choices, the newly created repository can be paired with a Jenkins CI/CD. The component creates a new Jenkins CI/CD pipeline and then performs the necessary configuration actions (webhooks, ect.) in order to complete the pairing process. As a result, the process is completed automatically, thus sparing the User from the manual use and configuration of the external tools. The service includes two endpoints: Endpoint 1: \"/createStructure\" -> creates a GitLab repository. Request parameters: projectName -> String projVisibility -> String projDescription -> String gitLabServerURL -> String gitlabToken -> String Endpoint 2: \"/createStuctureJenkins\" -> creates a GitLab repository and a Jenkins pipeline and finaly configures and pairs them. Request parameters: projectName -> String projVisibility -> String projDescription -> String gitLabServerURL -> String gitlabToken -> String jenkinsServerUrl -> String jenkinsUsername -> String jenkinsToken -> String","title":"Service Creation Backend"},{"location":"service-creation/#smartclide-service-creation","text":"SmartCLIDE Service Creation Backend Component","title":"SmartCLIDE Service Creation"},{"location":"service-creation/#preconditions-to-build-and-run-service-creation","text":"To build and run the backend service of Service Creation, the following software is required: Java (at least version 8) Apache Maven (at least version 3.2+) Docker (for building and running the final image)","title":"Preconditions to build and run Service Creation"},{"location":"service-creation/#how-to-build-service-creation","text":"Service Creation can be built using maven with the following command: mvn install In order to build a Docker image of the service that can be deployed, the following commands can be used: mvn install docker build -t ${IMAGE_NAME:IMAGE_TAG} . More specifically: mvn install docker build -t smartclide-service-creation:latest .","title":"How to build Service Creation"},{"location":"service-creation/#how-to-run-service-creation","text":"All the images of this component can be found here . You can run the backend service with the following command: docker run smartclide-service-creation:latest","title":"How to run Service Creation"},{"location":"service-creation/#how-to-use-service-creation","text":"This backend service leverages the provided GitLab and Jenkins APIs in order to authenticate a User and create a new repository based on the options selected by the User. Furthermore, depending on the User\u2019s choices, the newly created repository can be paired with a Jenkins CI/CD. The component creates a new Jenkins CI/CD pipeline and then performs the necessary configuration actions (webhooks, ect.) in order to complete the pairing process. As a result, the process is completed automatically, thus sparing the User from the manual use and configuration of the external tools. The service includes two endpoints: Endpoint 1: \"/createStructure\" -> creates a GitLab repository. Request parameters: projectName -> String projVisibility -> String projDescription -> String gitLabServerURL -> String gitlabToken -> String Endpoint 2: \"/createStuctureJenkins\" -> creates a GitLab repository and a Jenkins pipeline and finaly configures and pairs them. Request parameters: projectName -> String projVisibility -> String projDescription -> String gitLabServerURL -> String gitlabToken -> String jenkinsServerUrl -> String jenkinsUsername -> String jenkinsToken -> String","title":"How to use Service Creation"},{"location":"service-creation-testing/","text":"Smartclide Service Creation Testing SmartCLIDE Service Creation Testing Backend Component Preconditions to build and run Service Creation Testing To build and run the backend service of Service Creation Testing, the following software is required: Java (at least version 8) Apache Maven (at least version 3.2+) Docker (for building and running the final image) How to build Service Creation Testing Service Creation Testing can be built using maven with the following command: mvn install In order to build a Docker image of the service that can be deployed, the following commands can be used: mvn install docker build -t ${IMAGE_NAME:IMAGE_TAG} . More specifically: mvn install docker build -t smartclide-service-creation-testing-backend:latest . How to run Service Creation Testing All the images of this component can be found here . You can run the backend service with the following command: docker run smartclide-service-creation-testing-backend:latest How to use Service Creation Testing This Spring Boot application generates automatically Unit tests, for a given project. The service includes one endpoint: Endpoint: \"/generateTests\" -> automatic generation of Unit tests. Request parameters: gitRepoURL -> String gitUsername -> String gitToken -> String","title":"Service Creation Testing"},{"location":"service-creation-testing/#smartclide-service-creation-testing","text":"SmartCLIDE Service Creation Testing Backend Component","title":"Smartclide Service Creation Testing"},{"location":"service-creation-testing/#preconditions-to-build-and-run-service-creation-testing","text":"To build and run the backend service of Service Creation Testing, the following software is required: Java (at least version 8) Apache Maven (at least version 3.2+) Docker (for building and running the final image)","title":"Preconditions to build and run Service Creation Testing"},{"location":"service-creation-testing/#how-to-build-service-creation-testing","text":"Service Creation Testing can be built using maven with the following command: mvn install In order to build a Docker image of the service that can be deployed, the following commands can be used: mvn install docker build -t ${IMAGE_NAME:IMAGE_TAG} . More specifically: mvn install docker build -t smartclide-service-creation-testing-backend:latest .","title":"How to build Service Creation Testing"},{"location":"service-creation-testing/#how-to-run-service-creation-testing","text":"All the images of this component can be found here . You can run the backend service with the following command: docker run smartclide-service-creation-testing-backend:latest","title":"How to run Service Creation Testing"},{"location":"service-creation-testing/#how-to-use-service-creation-testing","text":"This Spring Boot application generates automatically Unit tests, for a given project. The service includes one endpoint: Endpoint: \"/generateTests\" -> automatic generation of Unit tests. Request parameters: gitRepoURL -> String gitUsername -> String gitToken -> String","title":"How to use Service Creation Testing"},{"location":"service-creation-testing-theia/","text":"Smartclide Service Creation Testing SmartCLIDE Service Creation Testing Frontend Component Preconditions to build and run Service Creation Testing Frontend To build and run the frontend Design Pattern Selection extension of Theia, the following software is required: Python Node.js with visual studio build tools (this can be selected in the optional tools during the node.js installation or after hand in several ways, ex. with npm, or with visual studio installer) Yarn package manager npm install --global yarn How to build Service Creation Testing Frontend The Service Creation Testing Frontend can be built using the following command: yarn How to run Service Creation Testing Frontend After building the theia extension, you can start a local instance of theia with our extension. Running the browser example yarn start:browser or: yarn rebuild:browser cd browser-app yarn start or: launch Start Browser Backend configuration from VS code. Open http://localhost:3000 in the browser.","title":"Service Creation Testing Frontend"},{"location":"service-creation-testing-theia/#smartclide-service-creation-testing","text":"SmartCLIDE Service Creation Testing Frontend Component","title":"Smartclide Service Creation Testing"},{"location":"service-creation-testing-theia/#preconditions-to-build-and-run-service-creation-testing-frontend","text":"To build and run the frontend Design Pattern Selection extension of Theia, the following software is required: Python Node.js with visual studio build tools (this can be selected in the optional tools during the node.js installation or after hand in several ways, ex. with npm, or with visual studio installer) Yarn package manager npm install --global yarn","title":"Preconditions to build and run Service Creation Testing Frontend"},{"location":"service-creation-testing-theia/#how-to-build-service-creation-testing-frontend","text":"The Service Creation Testing Frontend can be built using the following command: yarn","title":"How to build Service Creation Testing Frontend"},{"location":"service-creation-testing-theia/#how-to-run-service-creation-testing-frontend","text":"After building the theia extension, you can start a local instance of theia with our extension.","title":"How to run Service Creation Testing Frontend"},{"location":"service-creation-testing-theia/#running-the-browser-example","text":"yarn start:browser or: yarn rebuild:browser cd browser-app yarn start or: launch Start Browser Backend configuration from VS code. Open http://localhost:3000 in the browser.","title":"Running the browser example"},{"location":"service-discovery/","text":"Service Discovery API - SmartCLIDE Maintainer: @dabm-git - AIR Institute Configure This package relies on tokens from GitGub, GitLab and BitBucket APIs that are configured in the config.ini file in the root of the service. The service also depends on an instance of Elasticsearch to store and collect information, where the configuration of the IP, port and credentials are done in this same file. See: https://github.com/eclipse-opensmartclide/smartclide-service-discovery-poc/blob/main/ServiceDiscovery/config.ini The service makes use of the 2020 port, be sure to expose it. Build python3 -m pip install --no-cache-dir -r requirements.txt python3 -m pip install . --upgrade Or build the image with the provided dockerfile. Run python3 servicediscovery Or using the built docker image hosted by ghcr.io, with docker-compose. docker-compose up version: '3' services: service_discovery: restart: unless-stopped image: ghcr.io/eclipse-opensmartclide/smartclide/service-discovery:2022-04-04 working_dir: /app/smartclide-service-discovery-poc/ServiceDiscovery command: python3 ServiceDiscovery ports: - \"2020:2020\" Be sure to replace the necessary configuration in the config.ini file, to do this you can overwrite it with the following section in docker-compose.yml: volumes: -./config.ini:/app/smartclide-service-discovery-poc/ServiceDiscovery/config.ini Web service listings - SmartCLIDE Maintainer: @dabm-git - AIR Institute Run This script collects information from Programableweb, to avoid saturation and blocking of IP addresses by a high number of requests, this script can be executed on a regular basis, obtaining a section each time. When launched, information is collected in batches that are exported to .csv files, and when a batch is finished, the results are merged under the same .csv file.","title":"Service Discovery"},{"location":"service-discovery/#service-discovery-api-smartclide","text":"Maintainer: @dabm-git - AIR Institute","title":"Service Discovery API - SmartCLIDE"},{"location":"service-discovery/#configure","text":"This package relies on tokens from GitGub, GitLab and BitBucket APIs that are configured in the config.ini file in the root of the service. The service also depends on an instance of Elasticsearch to store and collect information, where the configuration of the IP, port and credentials are done in this same file. See: https://github.com/eclipse-opensmartclide/smartclide-service-discovery-poc/blob/main/ServiceDiscovery/config.ini The service makes use of the 2020 port, be sure to expose it.","title":"Configure"},{"location":"service-discovery/#build","text":"python3 -m pip install --no-cache-dir -r requirements.txt python3 -m pip install . --upgrade Or build the image with the provided dockerfile.","title":"Build"},{"location":"service-discovery/#run","text":"python3 servicediscovery Or using the built docker image hosted by ghcr.io, with docker-compose. docker-compose up version: '3' services: service_discovery: restart: unless-stopped image: ghcr.io/eclipse-opensmartclide/smartclide/service-discovery:2022-04-04 working_dir: /app/smartclide-service-discovery-poc/ServiceDiscovery command: python3 ServiceDiscovery ports: - \"2020:2020\" Be sure to replace the necessary configuration in the config.ini file, to do this you can overwrite it with the following section in docker-compose.yml: volumes: -./config.ini:/app/smartclide-service-discovery-poc/ServiceDiscovery/config.ini","title":"Run"},{"location":"service-discovery/#web-service-listings-smartclide","text":"Maintainer: @dabm-git - AIR Institute","title":"Web service listings - SmartCLIDE"},{"location":"service-discovery/#run_1","text":"This script collects information from Programableweb, to avoid saturation and blocking of IP addresses by a high number of requests, this script can be executed on a regular basis, obtaining a section each time. When launched, information is collected in batches that are exported to .csv files, and when a batch is finished, the results are merged under the same .csv file.","title":"Run"},{"location":"smart-assistant/","text":"Smartclide-DLE The SmartCLIDE DLE and smart assistant has brought together the IDE assistant features within one component. Proposed models try to provide a learning algorithm with the information that data carries, including internal data history and external online web services identified from online resources. After providing AI models, the smart assistant and DLE models are deployed as APIs REST encapsulated in a Python package, which guarantees the portability of this component between Operating Systems. Afterward, to expose the component functionality, we have chosen to visualize these requests and responses through the API swagger, which consists of an interactive web interface. Requirements The list of the third-party library are listed on requirments.txt files in each sub-components; however, the two main used library and requirements are: Python 3.7+ Pytorch HuggingFace scikit-learn Note: The minimum requirement for installing each transformer learning models using this package is 30GB of disk storage, 2vCPU, 4GB RAM. The reason of disk storage is during package installation, and it uses temp storage and packages like a torch, which exceeds more spaces during the installation process. To use less storage, you can disable caching behavior by using --no-cache-dir in pip install command. more info How to Build DLE component In SmartCLIDE platform, trained models need a gateway between the trained models and user interfaces. In this regard, the smart-assistant will support this option through Flask-restx APIs developed, which serve SmartCLIDE DLE (Deep Learning Engine) and Smart Assistant. Moreover, some statistical models are supported by smart-assistant as well.In this regard, DLE needs to install both trained models sub-components and also API gateway. API Gateway Installation Install prerequisites : sudo python3 -m pip install torch==1.5.1+cpu torchvision==0.6.1+cpu -f https://download.pytorch.org/whl/torch_stable.html sudo pyton3 -m pip install git+https://github.com/Dih5/zadeh install smartclid getway: sudo apt update sudo apt install python3 python3-pip nodejs npm -y sudo npm install -g pm2 After installation, SmartCLIDE smart-assistant provides the API specification using Swagger specification on \"http:// /dle\", and \"http:// /iamodeler\". In summary, the available end-points are: - http:// /dle/codegen - http:// /dle/acceptance - http:// /dle/environment - http:// /dle/templatecodegen - http:// /dle/serviceclassification - http:// /dle/bpmnitemrecommendation - http:// /dle/predictivemodeltoolassistant - http:// /iamodeler/classification/bayes - http:// /iamodeler/classification/extra-trees - http:// /iamodeler/supervised/classification/forest - http:// /iamodeler/supervised/classification/gradient - http:// /iamodeler/supervised/classification/logistic - http:// /iamodeler/supervised/classification/mlp - http:// /iamodeler/supervised/classification/neighbors - http:// /iamodeler/supervised/classification/sv - http:// /iamodeler/supervised/classification/tree - http:// /iamodeler/supervised/regression/gradient - http:// /iamodeler/supervised/regression/linear - http:// /iamodeler/supervised/regression/mlp - http:// /iamodeler/supervised/regression/neighbors - http:// /iamodeler/supervised/regression/sv - http:// /iamodeler/supervised/regression/tree Sub-component Quick Installation The trained models have been packaged using the Python Setuptools library. Therefore, this component need to install the related package by cloning the package, browsing to main directory, and executing \u201cpython3 -m pip install . --upgrade\u201d command. git clone https://github.com/eclipse-opensmartclide/smartclide-smart-assistant.git cd smartclide-dle-models/<sub-component> python3 -m pip install . --upgrade How to run DLE Component Configuration The application configuration is set via enviroment variables: SA_API_PORT : Port to bind to (default: 5001 ). SA_API_BIND : Address to bind to (default: 0.0.0.0 ). SA_MONGODB_PORT : MongoDB database to connect to (default: 27017 ). SA_MONGODB_HOST : MongoDB database to connect to (default: localhost ). SA_MONGODB_USER : MongoDB user to connect to db (default: user ). SA_MONGODB_PASSWROD : MongoDB password to connect to db (default: password ). SA_MONGODB_DB : MongoDB database to connect to (default: smartclide-smart-assistant ). DLE_BASE_URL : Base URL for DLE connection (default: http://smartclide.ddns.net:5001/smartclide/v1/dle ). SMART_ASSISTANT_BASE_URL : Base URL for Smart Assistant RabbitMQ connection (default: http://smartclide.ddns.net:5000/smartclide/v1/smartassistant ). RABBITMQ_HOST : RabbitMQ connection string host (default: localhost ). RABBITMQ_PORT : RabbitMQ connection string port (default: 5672 ). RABBITMQ_USER : RabbitMQ connection string user (default: user ). RABBITMQ_PASSWORD : RabbitMQ connection string password (default: password ). RABBITMQ_MAPPINGS : RabbitMQ mappings between queue and API's endpoint to connect to. (default: { 'acceptance_tests_queue': '{SMART_ASSISTANT_BASE_URL}/acceptance', 'bpmn_item_recommendation_queue': '{SMART_ASSISTANT_BASE_URL}/bpmnitemrecommendation', 'code_generation_queue': '{SMART_ASSISTANT_BASE_URL}/codegen', 'code_repo_recommendation_queue': '{SMART_ASSISTANT_BASE_URL}/coderepo', 'enviroment_queue': '{SMART_ASSISTANT_BASE_URL}/enviroment' } ). Note: All of them are prefixed with {SMART_ASSISTANT_BASE_URL}/ before start the connection. Run application Application can be launched with the launch script: sudo bash launch.bash Or using PM2: sudo pm2 start pm2.json Note: if the script launch.bash doesn't works, you can use launch2.bash instead. DLE Sub-components SmartCLIDE primarily works with text data, therefore, these components have the advantage of text processing trends and deep learning methods. The earlier approaches mostly combined key-word based feature engineering and traditional ML. However, the keyword-based approaches such as BoW mostly use one- hot encoded vectors, which are high-dimensional and sparse. The emergence of word-embedding techniques has im- proved keyword-based feature engineering. Additionally, the increasing word embedding of open-source projects such as Glove , word2vec , BERT , GPT2 help the fast and efficient low-dimensional representation of text data. Thus, despite these technologies being resource-demanding, SmartcLIDE considered them for some key functinalities. Service classification model Smartclide provides an environment to support the development of service-oriented softwares. The goal of this service classification is to classify the same web services based on their functionality which can be helpful in later stages such as service composition. The trained models have been packaged using the Python Setuptools library. Therefore, this component need to install the related package by cloning the package, browsing to main directory, and executing \u201cpython3 -m pip install . --upgrade\u201d command. git clone https://github.com/eclipse-opensmartclide/smartclide-smart-assistant.git cd smartclide-dle-models/serviceclassification python3 -m pip install . --upgrade Testing the module installation python3 servclassify/examples/classify_service.py Usage This library provides two trained models; first, the prediction by ML model. Second, predict using the DL model; the default configuration uses the ML model, which is lighter. You can select method=\"Default\" for using ML model or method= 'Advanced' for using DL model. However, the \"AIPipelineConfiguration\" class is configured for Default mode; for using method= 'Advanced', you need to change the configuration in the AIPipelineConfiguration file to set service_classification_method= 'Advanced' in AIPipelineConfiguration.py and reinstall the package. Simple Usage from servclassify import PredictServiceClassModel service_name=\"service name text\" service_desc=\"find the distination on map\" method=\"Default\" predict_service_obj = PredictServiceClassModel() result = predict_service_obj.predict(service_name, service_description, method=method) print(result) The above class demonstrate using service classification interface class whuich is PredictServiceClassModel. After defining this class we can use it for predicting service class: {'result': [{'Service_name': 'service name text', 'Method': 'Default', 'Service_id': None, 'Service_class': ['Mapping', '']}]} \u2728Note \u2728 The advanced method will return the top 2 categories assigned to service metadata input. the format of output will be: {'result': [{'Service_name': 'service name text', 'Method': 'Default', 'Service_id': None, 'Service_class': ['Mapping', 'Transportation']}]} Singleton Classes Usage In SmartCLIDE, many tasks require to run in the background independently of the user interface (UI). AI Models is one of these tasks that need to serve requests in real-time and return results. Consequently, loading the AI model can be time-consuming due to late response. A strategy such as using singleton classes for loading the models can help minimize the application UI load, improve availability, and reduce interactive response times. from typing import Tuple from typing import List from servclassify import PredictServiceClassModel class Classify_service: def __init__(self): ''' The DL models input parameter for PredictServiceClassModel mention loading service model ''' self.predict_service_obj = PredictServiceClassModel() def predict(self, service_id: str, service_name: str, service_description: str, method:str = 'Default') -> Tuple[str,str]: # predict result = self.predict_service_obj.predict(service_name, service_description, method=method) return result #Loading model recommended to execute on background model2 = Classify_service() service_id=1 service_name=\"service name text\" service_desc=\"find the distination on map\" method=\"Advanced\" result=model2.predict(service_id,service_name, service_desc,method) print(result) You can find the example code which are in python script in the example folder. Code completion model This subcomponent is responsible for generating code based on internal templates. The API returns related code snippets based on templates to implement the workflow represented in BPMN in low code. The first version of this API is designed for finding Java codes. The trained models have been packaged using the Python Setuptools library. Therefore, this component need to install the related package by cloning the package, browsing to main directory, and executing \u201cpython3 -m pip install . --upgrade\u201d command. git clone https://github.com/eclipse-opensmartclide/smartclide-smart-assistant.git cd smartclide-dle-models/codeautocomplete python3 -m pip install . --upgrade Testing the module installation python3 servcodegen/examples/generate_code.py Usage This library provides code-generator which uses language modeling, and after installation, the library can be used by importing the package. The model predicts the next tokens based on user input; in order to have better results, the following recommendation need to be considered: Max_sugges_line specifies max line suggestion; recommended value is between 1-3. Max_lenth specifies max length line suggestion, and the recommended value is between 15-20 Use Singletone call for acceptable response time, which this method is explained in the next section. Handling client requests need access to sufficient computing infrastructure. Therefore, it suggests calling code to autocomplete when the user uses \"Tab\" or \"Dot.\" Simple Usage from servcodegen import AutocompleteCodeModel model = AutocompleteCodeModel() method=\"GPT-2\" lang=\"java\" max_lenth=20 max_sugges_line=3 code_input=\"import android.\" result=model.generateCode(code_input, max_lenth, max_sugges_line,method) print(result) The above code demonstrate using servcodegen interface class whuich is AutocompleteCodeModel. the result will be {'result': {'code_sugg': ['import android.os.Bundle ;', 'import android.content.Intent ;', 'import android.content.Context ;'], 'Method': 'GPT-2', 'codeSuggLen': 20, 'codeSuggLines': 3, 'language': 'java'}} \u2728Note \u2728 loding model recommended to execute on background which is explained on singletone classes usage in below. Singleton classes Usage In SmartCLIDE, many tasks require to run in the background independently of the user interface (UI). AI Models is one of these tasks that need to serve requests in real-time and return results. Consequently, loading the AI model can be time-consuming due to late response. A strategy such as using singleton classes for loading the models can help minimize the application UI load, improve availability, and reduce interactive response times. from typing import Tuple from typing import List from servcodegen import AutocompleteCodeModel class CodeCompletion: def __init__(self): self.model = AutocompleteCodeModel() def predict2(self, method:str, language:str, code_input:str, code_sugg_len:int, code_sugg_lines:int) -> List[str]: # predict result = self.model.generateCode(code_input, code_sugg_len, code_sugg_lines,method) return result #Loading model recommended to execute on background codecomplete_obj = CodeCompletion() #Using loaded model Method=\"GPT-2\" lang=\"java\" max_lenth=20 max_sugges_line=3 code_input=\"file=new\" result=codecomplete_obj.predict2(Method,lang,code_input,max_lenth,max_sugges_line) print(result) Acceptance test suggestions model The acceptance test set suggestion system, based on collaborative filtering techniques, is responsible for providing the user with a set of tests defined in Gherkin format to be applied to the workflow defined in the BPMN and help verify if the expectations are met. The trained models have been packaged using the Python Setuptools library. Therefore, this component need to install the related package by cloning the package, browsing to main directory, and executing \u201cpython3 -m pip install . --upgrade\u201d command. git clone https://github.com/eclipse-opensmartclide/smartclide-smart-assistant.git cd smartclide-dle-models/cbr-gherkin-recommendation python3 -m pip install . --upgrade To install also the dependencies to run the tests or to generate the documentation install some of the extras like (Mind the quotes): python3 -m pip install '.[docs]' --upgrade Case database initialization For that purpose, use the following command: python3 initialize_cbr_db.py Usage The main class is CBR wich also needs the clases Casebase, Recovery and Aggregation. You need a frist load with all your base cases. After that first inicial load you can pass an empty array to the class initializer: import pycbr cbr = pycbr.CBR([],\"ghrkn_recommendator\",\"smartclide.ddns.net\") Add case The method to add a case must recibe a dictionary with this format: cbr.add_case({ 'name': \"Sting with the file name\", 'text': \"All the bpmn text\", 'gherkins': [\"list with gherkins text\"] }) Get recommendation The method to get a recommendation must recibe a string with all the bpmn text: cbr.recommend(bpmn_text) >>> { 'gherkins': [[\"List of list with all the recomended gherkins for the first 5 matches\"]], 'sims': [\"List of similarity scores from 0 to 1\"] } Documentation To generate the documentation, the docs extra dependencies must be installed. Furthermore, pandoc must be available in your system. To generate an html documentation with sphinx run: make docs To generate a PDF documentation using LaTeX: make pdf Predictive model tool API This subcomponent utilized the automated machine learning (AutoML) concept, allowing users to define ML actions sequences via an interface. These sequences contain the Predictive model tool APIs, which include 4 primary steps. 1) Importing data 2) Creating a supervised model based on regression or classification Model 3) Performing Prediction based on user input 4) Providing validation matric results which can use for visualization. Installation You probably to set up and use a virtualenv: # Prepare a clean virtualenv and activate it virtualenv -p /usr/bin/python3.6 venv source venv/bin/activate Remember to activate it whenever you are working with the package. To install a development version clone the repo, cd to the directory and: pip install -e . Once installed, the development flask server might be started with the command: iamodeler For real deployment, gunicorn might be used instead: pip install gunicorn unicorn --workers 4 --bind 0.0.0.0:5000 --timeout 600 iamodeler.server:app To use a celery queue system (see configuration below), a celery broker like RabbitMQ must also be installed. With RabbitMQ installed and running, start the queue system by running: celery -A iamodeler.tasks.celery worker Note the gunicorn timeout parameter does not affect the celery queues. In Windows , the default celery pool might not work. You might try to add --pool=eventlet to run it. Configuration Configuration is done with environment variables. Variable Description IAMODELER_STORE Path to the local storage of the models. Defaults to a temporal directory. IAMODELER_CELERY If set and not empty, use a local Celery queue system. IAMODELER_CELERY_BROKER Address of the Celery broker. IAMODELER_AUTH Authentication token for the server. Client request must set X-IAMODELER-AUTH to this token in their headers. IAMODELER_LOG A path to a yaml logging configuration file. Defaults to logging.yaml The paths are relative to the CWD , provide full paths when needed. Pro-tip: A .env file can be used installing the python-dotenv package. An example of logging configuration file is provided in the root of the repo. BPMN Items suggestions This AI-based approach provides recommendations during service composition. The suggestions are based on a selected service composition approach by (BPMN-based work-flow) data representation, existing/history BPMN work-flows, and provided service specification information. Usage This sub-module receives the information of the last selected node in the target BPMN diagram. This information is in JSON format, which can include unique node id and other node metadata such as name or user_id. Afterwards, the query compositor merges it with the incomplete BPMN file , developers are working with. { \"dle\": { \"header\": \"bpmn suggestion\", \"state\": \"query\", \"previous node\": [ { \"id\": \"_13BAF867-3CA8-4C6F-85C6-D3FD748D07D2\" }, { \"name\": \"UserFound?\" } ] } } The module performs four main steps on the received JSON request, which are: 1) Query Compositor 2) Current BPMN Extractor 3) BPMN semantic identifier 4) Numerical vector transformer and finally suggesting nexrtBPMN node which will be in JSON response format: { \"dle\": { \"header\": \"bpmn suggestion\", \"state\": \"true\", \"previous node\": [ { \"id\": \"_13BAF867-3CA8-4C6F-85C6-D3FD748D07D2\" }, { \"name\": \"UserFound?\" } ], \"suggestionnode\": [ { \"id\": \"_E5D17755-D671-43ED-BD7D-F6538933069C\" }, { \"name\": \"AuditUser\" } ] } } { \"dle\": { \"header\": \"bpmnsuggestion\", \"state\": \"false\", \"previousnode\": [ { \"id\": \"_13BAF867-3CA8-4C6F-85C6-D3FD748D07D2\" }, { \"name\": \"UserFound?\" } ] } }","title":"Smartclide-DLE"},{"location":"smart-assistant/#smartclide-dle","text":"The SmartCLIDE DLE and smart assistant has brought together the IDE assistant features within one component. Proposed models try to provide a learning algorithm with the information that data carries, including internal data history and external online web services identified from online resources. After providing AI models, the smart assistant and DLE models are deployed as APIs REST encapsulated in a Python package, which guarantees the portability of this component between Operating Systems. Afterward, to expose the component functionality, we have chosen to visualize these requests and responses through the API swagger, which consists of an interactive web interface.","title":"Smartclide-DLE"},{"location":"smart-assistant/#requirements","text":"The list of the third-party library are listed on requirments.txt files in each sub-components; however, the two main used library and requirements are: Python 3.7+ Pytorch HuggingFace scikit-learn Note: The minimum requirement for installing each transformer learning models using this package is 30GB of disk storage, 2vCPU, 4GB RAM. The reason of disk storage is during package installation, and it uses temp storage and packages like a torch, which exceeds more spaces during the installation process. To use less storage, you can disable caching behavior by using --no-cache-dir in pip install command. more info","title":"Requirements"},{"location":"smart-assistant/#how-to-build-dle-component","text":"In SmartCLIDE platform, trained models need a gateway between the trained models and user interfaces. In this regard, the smart-assistant will support this option through Flask-restx APIs developed, which serve SmartCLIDE DLE (Deep Learning Engine) and Smart Assistant. Moreover, some statistical models are supported by smart-assistant as well.In this regard, DLE needs to install both trained models sub-components and also API gateway.","title":"How to Build DLE component"},{"location":"smart-assistant/#api-gateway-installation","text":"Install prerequisites : sudo python3 -m pip install torch==1.5.1+cpu torchvision==0.6.1+cpu -f https://download.pytorch.org/whl/torch_stable.html sudo pyton3 -m pip install git+https://github.com/Dih5/zadeh install smartclid getway: sudo apt update sudo apt install python3 python3-pip nodejs npm -y sudo npm install -g pm2 After installation, SmartCLIDE smart-assistant provides the API specification using Swagger specification on \"http:// /dle\", and \"http:// /iamodeler\". In summary, the available end-points are: - http:// /dle/codegen - http:// /dle/acceptance - http:// /dle/environment - http:// /dle/templatecodegen - http:// /dle/serviceclassification - http:// /dle/bpmnitemrecommendation - http:// /dle/predictivemodeltoolassistant - http:// /iamodeler/classification/bayes - http:// /iamodeler/classification/extra-trees - http:// /iamodeler/supervised/classification/forest - http:// /iamodeler/supervised/classification/gradient - http:// /iamodeler/supervised/classification/logistic - http:// /iamodeler/supervised/classification/mlp - http:// /iamodeler/supervised/classification/neighbors - http:// /iamodeler/supervised/classification/sv - http:// /iamodeler/supervised/classification/tree - http:// /iamodeler/supervised/regression/gradient - http:// /iamodeler/supervised/regression/linear - http:// /iamodeler/supervised/regression/mlp - http:// /iamodeler/supervised/regression/neighbors - http:// /iamodeler/supervised/regression/sv - http:// /iamodeler/supervised/regression/tree","title":"API Gateway Installation"},{"location":"smart-assistant/#sub-component-quick-installation","text":"The trained models have been packaged using the Python Setuptools library. Therefore, this component need to install the related package by cloning the package, browsing to main directory, and executing \u201cpython3 -m pip install . --upgrade\u201d command. git clone https://github.com/eclipse-opensmartclide/smartclide-smart-assistant.git cd smartclide-dle-models/<sub-component> python3 -m pip install . --upgrade","title":"Sub-component Quick Installation"},{"location":"smart-assistant/#how-to-run-dle-component","text":"","title":"How to run DLE Component"},{"location":"smart-assistant/#configuration","text":"The application configuration is set via enviroment variables: SA_API_PORT : Port to bind to (default: 5001 ). SA_API_BIND : Address to bind to (default: 0.0.0.0 ). SA_MONGODB_PORT : MongoDB database to connect to (default: 27017 ). SA_MONGODB_HOST : MongoDB database to connect to (default: localhost ). SA_MONGODB_USER : MongoDB user to connect to db (default: user ). SA_MONGODB_PASSWROD : MongoDB password to connect to db (default: password ). SA_MONGODB_DB : MongoDB database to connect to (default: smartclide-smart-assistant ). DLE_BASE_URL : Base URL for DLE connection (default: http://smartclide.ddns.net:5001/smartclide/v1/dle ). SMART_ASSISTANT_BASE_URL : Base URL for Smart Assistant RabbitMQ connection (default: http://smartclide.ddns.net:5000/smartclide/v1/smartassistant ). RABBITMQ_HOST : RabbitMQ connection string host (default: localhost ). RABBITMQ_PORT : RabbitMQ connection string port (default: 5672 ). RABBITMQ_USER : RabbitMQ connection string user (default: user ). RABBITMQ_PASSWORD : RabbitMQ connection string password (default: password ). RABBITMQ_MAPPINGS : RabbitMQ mappings between queue and API's endpoint to connect to. (default: { 'acceptance_tests_queue': '{SMART_ASSISTANT_BASE_URL}/acceptance', 'bpmn_item_recommendation_queue': '{SMART_ASSISTANT_BASE_URL}/bpmnitemrecommendation', 'code_generation_queue': '{SMART_ASSISTANT_BASE_URL}/codegen', 'code_repo_recommendation_queue': '{SMART_ASSISTANT_BASE_URL}/coderepo', 'enviroment_queue': '{SMART_ASSISTANT_BASE_URL}/enviroment' } ). Note: All of them are prefixed with {SMART_ASSISTANT_BASE_URL}/ before start the connection.","title":"Configuration"},{"location":"smart-assistant/#run-application","text":"Application can be launched with the launch script: sudo bash launch.bash Or using PM2: sudo pm2 start pm2.json Note: if the script launch.bash doesn't works, you can use launch2.bash instead.","title":"Run application"},{"location":"smart-assistant/#dle-sub-components","text":"SmartCLIDE primarily works with text data, therefore, these components have the advantage of text processing trends and deep learning methods. The earlier approaches mostly combined key-word based feature engineering and traditional ML. However, the keyword-based approaches such as BoW mostly use one- hot encoded vectors, which are high-dimensional and sparse. The emergence of word-embedding techniques has im- proved keyword-based feature engineering. Additionally, the increasing word embedding of open-source projects such as Glove , word2vec , BERT , GPT2 help the fast and efficient low-dimensional representation of text data. Thus, despite these technologies being resource-demanding, SmartcLIDE considered them for some key functinalities.","title":"DLE Sub-components"},{"location":"smart-assistant/#service-classification-model","text":"Smartclide provides an environment to support the development of service-oriented softwares. The goal of this service classification is to classify the same web services based on their functionality which can be helpful in later stages such as service composition. The trained models have been packaged using the Python Setuptools library. Therefore, this component need to install the related package by cloning the package, browsing to main directory, and executing \u201cpython3 -m pip install . --upgrade\u201d command. git clone https://github.com/eclipse-opensmartclide/smartclide-smart-assistant.git cd smartclide-dle-models/serviceclassification python3 -m pip install . --upgrade Testing the module installation python3 servclassify/examples/classify_service.py","title":"Service classification model"},{"location":"smart-assistant/#usage","text":"This library provides two trained models; first, the prediction by ML model. Second, predict using the DL model; the default configuration uses the ML model, which is lighter. You can select method=\"Default\" for using ML model or method= 'Advanced' for using DL model. However, the \"AIPipelineConfiguration\" class is configured for Default mode; for using method= 'Advanced', you need to change the configuration in the AIPipelineConfiguration file to set service_classification_method= 'Advanced' in AIPipelineConfiguration.py and reinstall the package.","title":"Usage"},{"location":"smart-assistant/#simple-usage","text":"from servclassify import PredictServiceClassModel service_name=\"service name text\" service_desc=\"find the distination on map\" method=\"Default\" predict_service_obj = PredictServiceClassModel() result = predict_service_obj.predict(service_name, service_description, method=method) print(result) The above class demonstrate using service classification interface class whuich is PredictServiceClassModel. After defining this class we can use it for predicting service class: {'result': [{'Service_name': 'service name text', 'Method': 'Default', 'Service_id': None, 'Service_class': ['Mapping', '']}]} \u2728Note \u2728 The advanced method will return the top 2 categories assigned to service metadata input. the format of output will be: {'result': [{'Service_name': 'service name text', 'Method': 'Default', 'Service_id': None, 'Service_class': ['Mapping', 'Transportation']}]}","title":"Simple Usage"},{"location":"smart-assistant/#singleton-classes-usage","text":"In SmartCLIDE, many tasks require to run in the background independently of the user interface (UI). AI Models is one of these tasks that need to serve requests in real-time and return results. Consequently, loading the AI model can be time-consuming due to late response. A strategy such as using singleton classes for loading the models can help minimize the application UI load, improve availability, and reduce interactive response times. from typing import Tuple from typing import List from servclassify import PredictServiceClassModel class Classify_service: def __init__(self): ''' The DL models input parameter for PredictServiceClassModel mention loading service model ''' self.predict_service_obj = PredictServiceClassModel() def predict(self, service_id: str, service_name: str, service_description: str, method:str = 'Default') -> Tuple[str,str]: # predict result = self.predict_service_obj.predict(service_name, service_description, method=method) return result #Loading model recommended to execute on background model2 = Classify_service() service_id=1 service_name=\"service name text\" service_desc=\"find the distination on map\" method=\"Advanced\" result=model2.predict(service_id,service_name, service_desc,method) print(result) You can find the example code which are in python script in the example folder.","title":"Singleton Classes Usage"},{"location":"smart-assistant/#code-completion-model","text":"This subcomponent is responsible for generating code based on internal templates. The API returns related code snippets based on templates to implement the workflow represented in BPMN in low code. The first version of this API is designed for finding Java codes. The trained models have been packaged using the Python Setuptools library. Therefore, this component need to install the related package by cloning the package, browsing to main directory, and executing \u201cpython3 -m pip install . --upgrade\u201d command. git clone https://github.com/eclipse-opensmartclide/smartclide-smart-assistant.git cd smartclide-dle-models/codeautocomplete python3 -m pip install . --upgrade Testing the module installation python3 servcodegen/examples/generate_code.py","title":"Code completion model"},{"location":"smart-assistant/#usage_1","text":"This library provides code-generator which uses language modeling, and after installation, the library can be used by importing the package. The model predicts the next tokens based on user input; in order to have better results, the following recommendation need to be considered: Max_sugges_line specifies max line suggestion; recommended value is between 1-3. Max_lenth specifies max length line suggestion, and the recommended value is between 15-20 Use Singletone call for acceptable response time, which this method is explained in the next section. Handling client requests need access to sufficient computing infrastructure. Therefore, it suggests calling code to autocomplete when the user uses \"Tab\" or \"Dot.\"","title":"Usage"},{"location":"smart-assistant/#simple-usage_1","text":"from servcodegen import AutocompleteCodeModel model = AutocompleteCodeModel() method=\"GPT-2\" lang=\"java\" max_lenth=20 max_sugges_line=3 code_input=\"import android.\" result=model.generateCode(code_input, max_lenth, max_sugges_line,method) print(result) The above code demonstrate using servcodegen interface class whuich is AutocompleteCodeModel. the result will be {'result': {'code_sugg': ['import android.os.Bundle ;', 'import android.content.Intent ;', 'import android.content.Context ;'], 'Method': 'GPT-2', 'codeSuggLen': 20, 'codeSuggLines': 3, 'language': 'java'}} \u2728Note \u2728 loding model recommended to execute on background which is explained on singletone classes usage in below.","title":"Simple Usage"},{"location":"smart-assistant/#singleton-classes-usage_1","text":"In SmartCLIDE, many tasks require to run in the background independently of the user interface (UI). AI Models is one of these tasks that need to serve requests in real-time and return results. Consequently, loading the AI model can be time-consuming due to late response. A strategy such as using singleton classes for loading the models can help minimize the application UI load, improve availability, and reduce interactive response times. from typing import Tuple from typing import List from servcodegen import AutocompleteCodeModel class CodeCompletion: def __init__(self): self.model = AutocompleteCodeModel() def predict2(self, method:str, language:str, code_input:str, code_sugg_len:int, code_sugg_lines:int) -> List[str]: # predict result = self.model.generateCode(code_input, code_sugg_len, code_sugg_lines,method) return result #Loading model recommended to execute on background codecomplete_obj = CodeCompletion() #Using loaded model Method=\"GPT-2\" lang=\"java\" max_lenth=20 max_sugges_line=3 code_input=\"file=new\" result=codecomplete_obj.predict2(Method,lang,code_input,max_lenth,max_sugges_line) print(result)","title":"Singleton classes Usage"},{"location":"smart-assistant/#acceptance-test-suggestions-model","text":"The acceptance test set suggestion system, based on collaborative filtering techniques, is responsible for providing the user with a set of tests defined in Gherkin format to be applied to the workflow defined in the BPMN and help verify if the expectations are met. The trained models have been packaged using the Python Setuptools library. Therefore, this component need to install the related package by cloning the package, browsing to main directory, and executing \u201cpython3 -m pip install . --upgrade\u201d command. git clone https://github.com/eclipse-opensmartclide/smartclide-smart-assistant.git cd smartclide-dle-models/cbr-gherkin-recommendation python3 -m pip install . --upgrade To install also the dependencies to run the tests or to generate the documentation install some of the extras like (Mind the quotes): python3 -m pip install '.[docs]' --upgrade","title":"Acceptance test suggestions model"},{"location":"smart-assistant/#case-database-initialization","text":"For that purpose, use the following command: python3 initialize_cbr_db.py","title":"Case database initialization"},{"location":"smart-assistant/#usage_2","text":"The main class is CBR wich also needs the clases Casebase, Recovery and Aggregation. You need a frist load with all your base cases. After that first inicial load you can pass an empty array to the class initializer: import pycbr cbr = pycbr.CBR([],\"ghrkn_recommendator\",\"smartclide.ddns.net\")","title":"Usage"},{"location":"smart-assistant/#add-case","text":"The method to add a case must recibe a dictionary with this format: cbr.add_case({ 'name': \"Sting with the file name\", 'text': \"All the bpmn text\", 'gherkins': [\"list with gherkins text\"] })","title":"Add case"},{"location":"smart-assistant/#get-recommendation","text":"The method to get a recommendation must recibe a string with all the bpmn text: cbr.recommend(bpmn_text) >>> { 'gherkins': [[\"List of list with all the recomended gherkins for the first 5 matches\"]], 'sims': [\"List of similarity scores from 0 to 1\"] }","title":"Get recommendation"},{"location":"smart-assistant/#documentation","text":"To generate the documentation, the docs extra dependencies must be installed. Furthermore, pandoc must be available in your system. To generate an html documentation with sphinx run: make docs To generate a PDF documentation using LaTeX: make pdf","title":"Documentation"},{"location":"smart-assistant/#predictive-model-tool-api","text":"This subcomponent utilized the automated machine learning (AutoML) concept, allowing users to define ML actions sequences via an interface. These sequences contain the Predictive model tool APIs, which include 4 primary steps. 1) Importing data 2) Creating a supervised model based on regression or classification Model 3) Performing Prediction based on user input 4) Providing validation matric results which can use for visualization.","title":"Predictive model tool API"},{"location":"smart-assistant/#installation","text":"You probably to set up and use a virtualenv: # Prepare a clean virtualenv and activate it virtualenv -p /usr/bin/python3.6 venv source venv/bin/activate Remember to activate it whenever you are working with the package. To install a development version clone the repo, cd to the directory and: pip install -e . Once installed, the development flask server might be started with the command: iamodeler For real deployment, gunicorn might be used instead: pip install gunicorn unicorn --workers 4 --bind 0.0.0.0:5000 --timeout 600 iamodeler.server:app To use a celery queue system (see configuration below), a celery broker like RabbitMQ must also be installed. With RabbitMQ installed and running, start the queue system by running: celery -A iamodeler.tasks.celery worker Note the gunicorn timeout parameter does not affect the celery queues. In Windows , the default celery pool might not work. You might try to add --pool=eventlet to run it.","title":"Installation"},{"location":"smart-assistant/#configuration_1","text":"Configuration is done with environment variables. Variable Description IAMODELER_STORE Path to the local storage of the models. Defaults to a temporal directory. IAMODELER_CELERY If set and not empty, use a local Celery queue system. IAMODELER_CELERY_BROKER Address of the Celery broker. IAMODELER_AUTH Authentication token for the server. Client request must set X-IAMODELER-AUTH to this token in their headers. IAMODELER_LOG A path to a yaml logging configuration file. Defaults to logging.yaml The paths are relative to the CWD , provide full paths when needed. Pro-tip: A .env file can be used installing the python-dotenv package. An example of logging configuration file is provided in the root of the repo.","title":"Configuration"},{"location":"smart-assistant/#bpmn-items-suggestions","text":"This AI-based approach provides recommendations during service composition. The suggestions are based on a selected service composition approach by (BPMN-based work-flow) data representation, existing/history BPMN work-flows, and provided service specification information.","title":"BPMN Items suggestions"},{"location":"smart-assistant/#usage_3","text":"This sub-module receives the information of the last selected node in the target BPMN diagram. This information is in JSON format, which can include unique node id and other node metadata such as name or user_id. Afterwards, the query compositor merges it with the incomplete BPMN file , developers are working with. { \"dle\": { \"header\": \"bpmn suggestion\", \"state\": \"query\", \"previous node\": [ { \"id\": \"_13BAF867-3CA8-4C6F-85C6-D3FD748D07D2\" }, { \"name\": \"UserFound?\" } ] } } The module performs four main steps on the received JSON request, which are: 1) Query Compositor 2) Current BPMN Extractor 3) BPMN semantic identifier 4) Numerical vector transformer and finally suggesting nexrtBPMN node which will be in JSON response format: { \"dle\": { \"header\": \"bpmn suggestion\", \"state\": \"true\", \"previous node\": [ { \"id\": \"_13BAF867-3CA8-4C6F-85C6-D3FD748D07D2\" }, { \"name\": \"UserFound?\" } ], \"suggestionnode\": [ { \"id\": \"_E5D17755-D671-43ED-BD7D-F6538933069C\" }, { \"name\": \"AuditUser\" } ] } } { \"dle\": { \"header\": \"bpmnsuggestion\", \"state\": \"false\", \"previousnode\": [ { \"id\": \"_13BAF867-3CA8-4C6F-85C6-D3FD748D07D2\" }, { \"name\": \"UserFound?\" } ] } }","title":"Usage"},{"location":"technical-debt-td-interest/","text":"Smartclide TD Interest SmartCLIDE TD Interest Backend Component Preconditions to build and run TD Interest To build and run the backend service of TD Interest, the following software is required: Java (at least version 11) Apache Maven (at least version 3.2+) Docker (for building and running the final image) How to build TD Interest TD Interest can be built using maven with the following command: mvn install In order to build a Docker image of the service that can be deployed, the following commands can be used: mvn install docker build -t ${IMAGE_NAME:IMAGE_TAG} . More specifically: mvn install docker build -t smartclide-td-interest-backend:latest . How to run TD Interest All the images of this component can be found here . You can run the backend service with the following command: docker run smartclide-td-interest-backend:latest Extra dependencies of TD Interest This component uses an internal database in order to calculate the TD Interest, as from its definition requires historical data. This database is a postgresql where the schema can be found here . After the creation of the database, the properties for accessing the database can be changed from here . In order to change the database url, username, and password. - spring.datasource.url=jdbc:postgresql://localhost:5432/tdinterest - spring.datasource.username=tdinterest - spring.datasource.password=tdinterest","title":"TD Interest Backend"},{"location":"technical-debt-td-interest/#smartclide-td-interest","text":"SmartCLIDE TD Interest Backend Component","title":"Smartclide TD Interest"},{"location":"technical-debt-td-interest/#preconditions-to-build-and-run-td-interest","text":"To build and run the backend service of TD Interest, the following software is required: Java (at least version 11) Apache Maven (at least version 3.2+) Docker (for building and running the final image)","title":"Preconditions to build and run TD Interest"},{"location":"technical-debt-td-interest/#how-to-build-td-interest","text":"TD Interest can be built using maven with the following command: mvn install In order to build a Docker image of the service that can be deployed, the following commands can be used: mvn install docker build -t ${IMAGE_NAME:IMAGE_TAG} . More specifically: mvn install docker build -t smartclide-td-interest-backend:latest .","title":"How to build TD Interest"},{"location":"technical-debt-td-interest/#how-to-run-td-interest","text":"All the images of this component can be found here . You can run the backend service with the following command: docker run smartclide-td-interest-backend:latest","title":"How to run TD Interest"},{"location":"technical-debt-td-interest/#extra-dependencies-of-td-interest","text":"This component uses an internal database in order to calculate the TD Interest, as from its definition requires historical data. This database is a postgresql where the schema can be found here . After the creation of the database, the properties for accessing the database can be changed from here . In order to change the database url, username, and password. - spring.datasource.url=jdbc:postgresql://localhost:5432/tdinterest - spring.datasource.username=tdinterest - spring.datasource.password=tdinterest","title":"Extra dependencies of TD Interest"},{"location":"technical-debt-td-principal/","text":"Smartclide TD Principal SmartCLIDE TD Principal Backend Component Preconditions to build and run TD Principal To build and run the backend service of TD Principal, the following software is required: Java (at least version 11) Apache Maven (at least version 3.2+) Docker (for building and running the final image) How to build TD Principal TD Principal can be built using maven with the following command: mvn install In order to build a Docker image of the service that can be deployed, the following commands can be used: mvn install docker build -t ${IMAGE_NAME:IMAGE_TAG} . More specifically: mvn install docker build -t smartclide-td-principal-backend:latest . How to run TD Principal All the images of this component can be found here . You can run the backend service with the following command: docker run -p 8555:8555 smartclide-td-principal-backend:latest How to configure TD Principal application.properties The main properties of this backend service can be found here , and are the following: - server.port=8555, In order for the service to start in the 8555 port - gr.nikos.smartclide.sonarqube.url=http://localhost:9000, In order to get the SonarQube instance and the in case this is not configured in the beginning of the service it's going to be the localhost. Docker The main thing that should be configured is the SonarQube instance, as in the majority of the cases the SonarQube is not going to be in the localhost. This can be achieved by using the following environment variable: docker run -p 8555:8555 -e GR_NIKOS_SMARTCLIDE_SONARQUBE_URL=${SONARQUBE_URL} smartclide-td-principal-backend:latest More specifically: docker run -p 8555:8555 -e GR_NIKOS_SMARTCLIDE_SONARQUBE_URL=http://1.1.1.1:9000 smartclide-td-principal-backend:latest","title":"TD Principal Backend"},{"location":"technical-debt-td-principal/#smartclide-td-principal","text":"SmartCLIDE TD Principal Backend Component","title":"Smartclide TD Principal"},{"location":"technical-debt-td-principal/#preconditions-to-build-and-run-td-principal","text":"To build and run the backend service of TD Principal, the following software is required: Java (at least version 11) Apache Maven (at least version 3.2+) Docker (for building and running the final image)","title":"Preconditions to build and run TD Principal"},{"location":"technical-debt-td-principal/#how-to-build-td-principal","text":"TD Principal can be built using maven with the following command: mvn install In order to build a Docker image of the service that can be deployed, the following commands can be used: mvn install docker build -t ${IMAGE_NAME:IMAGE_TAG} . More specifically: mvn install docker build -t smartclide-td-principal-backend:latest .","title":"How to build TD Principal"},{"location":"technical-debt-td-principal/#how-to-run-td-principal","text":"All the images of this component can be found here . You can run the backend service with the following command: docker run -p 8555:8555 smartclide-td-principal-backend:latest","title":"How to run TD Principal"},{"location":"technical-debt-td-principal/#how-to-configure-td-principal","text":"application.properties The main properties of this backend service can be found here , and are the following: - server.port=8555, In order for the service to start in the 8555 port - gr.nikos.smartclide.sonarqube.url=http://localhost:9000, In order to get the SonarQube instance and the in case this is not configured in the beginning of the service it's going to be the localhost. Docker The main thing that should be configured is the SonarQube instance, as in the majority of the cases the SonarQube is not going to be in the localhost. This can be achieved by using the following environment variable: docker run -p 8555:8555 -e GR_NIKOS_SMARTCLIDE_SONARQUBE_URL=${SONARQUBE_URL} smartclide-td-principal-backend:latest More specifically: docker run -p 8555:8555 -e GR_NIKOS_SMARTCLIDE_SONARQUBE_URL=http://1.1.1.1:9000 smartclide-td-principal-backend:latest","title":"How to configure TD Principal"},{"location":"technical-debt-td-reusability-index/","text":"Smartclide TD Reusability Index SmartCLIDE TD Reusability Index Backend Component Preconditions to build and run TD Reusability To build and run the backend service of TD Reusability Index, the following software is required: Java (at least version 11) Apache Maven (at least version 3.2+) Docker (for building and running the final image) How to build TD Reusability Index TD Reusability Index can be built using maven with the following command: mvn install In order to build a Docker image of the service that can be deployed, the following commands can be used: mvn install docker build -t ${IMAGE_NAME:IMAGE_TAG} . More specifically: mvn install docker build -t smartclide-td-reusability-index-backend:latest . How to run TD Reusability Index All the images of this component can be found here . You can run the backend service with the following command: docker run smartclide-td-reusability-index-backend:latest Extra dependencies of TD Reusability Index This backend has an external dependency with the TD Interest , in order to get the metrics of a specific project. This dependency is visible through the application.properties file, where there is the following variable: - interest-service.url","title":"TD Reusability Index Backend"},{"location":"technical-debt-td-reusability-index/#smartclide-td-reusability-index","text":"SmartCLIDE TD Reusability Index Backend Component","title":"Smartclide TD Reusability Index"},{"location":"technical-debt-td-reusability-index/#preconditions-to-build-and-run-td-reusability","text":"To build and run the backend service of TD Reusability Index, the following software is required: Java (at least version 11) Apache Maven (at least version 3.2+) Docker (for building and running the final image)","title":"Preconditions to build and run TD Reusability"},{"location":"technical-debt-td-reusability-index/#how-to-build-td-reusability-index","text":"TD Reusability Index can be built using maven with the following command: mvn install In order to build a Docker image of the service that can be deployed, the following commands can be used: mvn install docker build -t ${IMAGE_NAME:IMAGE_TAG} . More specifically: mvn install docker build -t smartclide-td-reusability-index-backend:latest .","title":"How to build TD Reusability Index"},{"location":"technical-debt-td-reusability-index/#how-to-run-td-reusability-index","text":"All the images of this component can be found here . You can run the backend service with the following command: docker run smartclide-td-reusability-index-backend:latest","title":"How to run TD Reusability Index"},{"location":"technical-debt-td-reusability-index/#extra-dependencies-of-td-reusability-index","text":"This backend has an external dependency with the TD Interest , in order to get the metrics of a specific project. This dependency is visible through the application.properties file, where there is the following variable: - interest-service.url","title":"Extra dependencies of TD Reusability Index"},{"location":"technical-debt-td-reusability-index-theia/","text":"Smartclide TD & Reusability SmartCLIDE TD Principal-Interest & Reusability Frontend Component Usage The user can access this extention from the tab View - SmartClide TD and Reusability. From the user can see the extension in the left side of the editor. From here the user is able to see the following 3 different categories: - TD Principal - TD Interest - Reusability TD Principal In the first tab of the extension namely \"TD Principal\", the used is able to see the Principal in hours and monetary values, along with the actual inefficiencies. - The user should provide the Project Url, which is the git url of the project. - The \"SonarQube Project Key\" is a key that is used in SonarQube, which is an automatic generated value in the following format {projectOwner}:{projectName} - The \u201cGit Token\u201d if it\u2019s a private project - And if the user wants they can provide specific endpoints for analysis. If there is already a Analysis of this project, the user can get the Project Analysis or Endpoint Analysis with the according buttons. If there isnt an Analysis, the user should perform a New Analysis first. You can see the following example for reference. It should be noted, that a lot of this fields could be removed in the next versions since we have in other places the majority of the info. TD Interest In the second tab namely \"TD Interest\", the user can provide the following: - The Project Url, which is the git url of the project - The \u201cGit Token\u201d if it\u2019s a private project If there is already an interest analysis the used can get it, through the \"Load Interest\" button. Or if there are new commits they can select \"Analyze Interest\" in order to get up to date. Files Evolution Reusability In the final tab, the user should provide the Project Url, and given the Interest Analysis that have already been made, the reusablity values are calculated. Files Evolution Build and Run Preconditions to build and run To build and run the frontend of TD Principal-Interest & Reusability, the following software is required: Python Node.js with visual studio build tools (this can be selected in the optional tools during the node.js installation or after hand in several ways, ex. with npm, or with visual studio installer) Yarn package manager npm install --global yarn How to build TD Principal-Interest & Reusability Frontend TD Principal-Interest & Reusability Frontend can be built using the following command: yarn How to run TD Principal-Interest & Reusability Frontend After building the theia extension, you can start a local instance of theia with our extension. Running the browser example yarn start:browser or: yarn rebuild:browser cd browser-app yarn start or: launch Start Browser Backend configuration from VS code. Open http://localhost:3000 in the browser.","title":"TD & Reusability Frontend"},{"location":"technical-debt-td-reusability-index-theia/#smartclide-td-reusability","text":"SmartCLIDE TD Principal-Interest & Reusability Frontend Component","title":"Smartclide TD &amp; Reusability"},{"location":"technical-debt-td-reusability-index-theia/#usage","text":"The user can access this extention from the tab View - SmartClide TD and Reusability. From the user can see the extension in the left side of the editor. From here the user is able to see the following 3 different categories: - TD Principal - TD Interest - Reusability","title":"Usage"},{"location":"technical-debt-td-reusability-index-theia/#td-principal","text":"In the first tab of the extension namely \"TD Principal\", the used is able to see the Principal in hours and monetary values, along with the actual inefficiencies. - The user should provide the Project Url, which is the git url of the project. - The \"SonarQube Project Key\" is a key that is used in SonarQube, which is an automatic generated value in the following format {projectOwner}:{projectName} - The \u201cGit Token\u201d if it\u2019s a private project - And if the user wants they can provide specific endpoints for analysis. If there is already a Analysis of this project, the user can get the Project Analysis or Endpoint Analysis with the according buttons. If there isnt an Analysis, the user should perform a New Analysis first. You can see the following example for reference. It should be noted, that a lot of this fields could be removed in the next versions since we have in other places the majority of the info.","title":"TD Principal"},{"location":"technical-debt-td-reusability-index-theia/#td-interest","text":"In the second tab namely \"TD Interest\", the user can provide the following: - The Project Url, which is the git url of the project - The \u201cGit Token\u201d if it\u2019s a private project If there is already an interest analysis the used can get it, through the \"Load Interest\" button. Or if there are new commits they can select \"Analyze Interest\" in order to get up to date. Files Evolution","title":"TD Interest"},{"location":"technical-debt-td-reusability-index-theia/#reusability","text":"In the final tab, the user should provide the Project Url, and given the Interest Analysis that have already been made, the reusablity values are calculated. Files Evolution","title":"Reusability"},{"location":"technical-debt-td-reusability-index-theia/#build-and-run","text":"","title":"Build and Run"},{"location":"technical-debt-td-reusability-index-theia/#preconditions-to-build-and-run","text":"To build and run the frontend of TD Principal-Interest & Reusability, the following software is required: Python Node.js with visual studio build tools (this can be selected in the optional tools during the node.js installation or after hand in several ways, ex. with npm, or with visual studio installer) Yarn package manager npm install --global yarn","title":"Preconditions to build and run"},{"location":"technical-debt-td-reusability-index-theia/#how-to-build-td-principal-interest-reusability-frontend","text":"TD Principal-Interest & Reusability Frontend can be built using the following command: yarn","title":"How to build TD Principal-Interest &amp; Reusability Frontend"},{"location":"technical-debt-td-reusability-index-theia/#how-to-run-td-principal-interest-reusability-frontend","text":"After building the theia extension, you can start a local instance of theia with our extension.","title":"How to run TD Principal-Interest &amp; Reusability Frontend"},{"location":"technical-debt-td-reusability-index-theia/#running-the-browser-example","text":"yarn start:browser or: yarn rebuild:browser cd browser-app yarn start or: launch Start Browser Backend configuration from VS code. Open http://localhost:3000 in the browser.","title":"Running the browser example"},{"location":"workflows/","text":"Getting started with Workflows In order to start creating your own workflows, you should head to the \"Workflows\" section from the left side of the SmarCLIDE IDE. Once the page loads you will be able to see the jBPM instance inside the IDE, just like the following figure. From here you can select the: - \"Design\" in order to start creating and editing your BMPN diagrams/ - \"Deploy\" in order to see the deployment units. - \"Manage\" in order to manage the process instances, tasks, or jobs. - \"Track\" in order to see metrics and reports regarding your workflows. BPMN with SmartCLIDE By selecting \"Design\" you are able to see your spaces, projects, and assets. You can open/create a project and from there you can add different kind of Assets. By selecting the \"Business Process\" asset, you will be promoted to add a name for this workflow, and after that an editor is going to open. From here you will be able to start creating your BPMN diagrams. The jBPM environment offers a lot of different nodes that can be added in the workflow, by simply drag and drop. All these nodes exist on the left side of the editor. Main SmartCLIDE Additions The main addition of SmartCLIDE is in the assignments section of each Rest Task, which looks like this. As SmartCLIDE wants help the reuse of existing service, the \"SmartCLIDE Service Discovery\" section exist. If the User provided a descriptive Task Name and Description, by simply selecting the \"Search\" button is able to get the appropriate results. After that the user is able to select \"Fetch Code\", in order to get the according code in a Theia workspace in order to start the development process. In case the user wasn't able to find a suitable existing service they can use the \"Service Creation\", in order to easily create a new one. Moreover, another addition of SmartCLIDE is regarding the Technical Debt (TD) of the workflow. In order to get the TD of the workflow, you have to navigate to the \"Documentation\" Tab of the workflow, and from there you select the \"Calculate TD\".","title":"Getting Started"},{"location":"workflows/#getting-started-with-workflows","text":"In order to start creating your own workflows, you should head to the \"Workflows\" section from the left side of the SmarCLIDE IDE. Once the page loads you will be able to see the jBPM instance inside the IDE, just like the following figure. From here you can select the: - \"Design\" in order to start creating and editing your BMPN diagrams/ - \"Deploy\" in order to see the deployment units. - \"Manage\" in order to manage the process instances, tasks, or jobs. - \"Track\" in order to see metrics and reports regarding your workflows.","title":"Getting started with Workflows"},{"location":"workflows/#bpmn-with-smartclide","text":"By selecting \"Design\" you are able to see your spaces, projects, and assets. You can open/create a project and from there you can add different kind of Assets. By selecting the \"Business Process\" asset, you will be promoted to add a name for this workflow, and after that an editor is going to open. From here you will be able to start creating your BPMN diagrams. The jBPM environment offers a lot of different nodes that can be added in the workflow, by simply drag and drop. All these nodes exist on the left side of the editor.","title":"BPMN with SmartCLIDE"},{"location":"workflows/#main-smartclide-additions","text":"The main addition of SmartCLIDE is in the assignments section of each Rest Task, which looks like this. As SmartCLIDE wants help the reuse of existing service, the \"SmartCLIDE Service Discovery\" section exist. If the User provided a descriptive Task Name and Description, by simply selecting the \"Search\" button is able to get the appropriate results. After that the user is able to select \"Fetch Code\", in order to get the according code in a Theia workspace in order to start the development process. In case the user wasn't able to find a suitable existing service they can use the \"Service Creation\", in order to easily create a new one. Moreover, another addition of SmartCLIDE is regarding the Technical Debt (TD) of the workflow. In order to get the TD of the workflow, you have to navigate to the \"Documentation\" Tab of the workflow, and from there you select the \"Calculate TD\".","title":"Main SmartCLIDE Additions"},{"location":"workflows/example1/","text":"Example 1 Design Start Node End Node These first Nodes are necessary in order to provide the start and end of your workflow. Moreover, for this example we are going to make a Rest API call, so we need to add a Rest node as well. After having all the nodes in the editor we can start making the appropriate connections. By selecting a node a lot of different options appear and from there you can select the arrow in order to connect this node with another one. For this example that we want to make only one API call we have the following simple BPMN diagram. In order to configure our Rest node in order to make the call to the API that we want, we can select it and change anything that we want from the right side of the editor. The majority of the configuration exist inside the Data Assignments section. By selecting the Assignments we are able to see and edit all the information regarding the Rest API call. In the \"Data Inputs and Assignments\" section we can change for example - The \"Url\" to \"https://reqres.in/api/users\" - The \"Method\" to \"GET\" In the \"Data Outputs and Assignments\" section we can make configurations regarding the output of the node. In order to send the result of the call to the next node. But for this example we are not going to change anything. In the \"SmartCLIDE service discovery\" section you can search for existing service, provided that you have a descriptive node Name, and Description. If you don't find any appropriate service in the previous search you can use the \"SmartCLIDE service creation\". That redirects you in the service creation page of SmartCLIDE. Finally, the assignments are going to look like this: Deploy After the Design of the workflow, you can go ahead and select \"Save\". It's the first button in the top bar above the editor. Now by going back to the project (by selecting the name of the project from the top) From here you have to add to install the Rest Task to your project since it not in the main components of jBPM. You have to navigate to the \"Settings\" tab and from there select the \"Custom Tasks\" from the left side, and finally select \"Install\" for th Rest task. So by navigating once again in the Assets Tab, you can see the Rest Asset as well. And by selecting the \"Deploy\" button from the top right, the project is being build and deployed. So now that our project is deployed if we want to create a process instance of our workflow, we navigate from the top menu to the \"Process Definitions\". From here you are able to see all the processes, and you can start a new instance like this: Now we can navigate to the Manage of \"Process Instances\" like this: From here we see all the Process Instances that are active or have finished. We can see that our Process Instance is finished since it had to do only an API call. And if we select it, we can see more information about it.","title":"Example 1"},{"location":"workflows/example1/#example-1","text":"","title":"Example 1"},{"location":"workflows/example1/#design","text":"Start Node End Node These first Nodes are necessary in order to provide the start and end of your workflow. Moreover, for this example we are going to make a Rest API call, so we need to add a Rest node as well. After having all the nodes in the editor we can start making the appropriate connections. By selecting a node a lot of different options appear and from there you can select the arrow in order to connect this node with another one. For this example that we want to make only one API call we have the following simple BPMN diagram. In order to configure our Rest node in order to make the call to the API that we want, we can select it and change anything that we want from the right side of the editor. The majority of the configuration exist inside the Data Assignments section. By selecting the Assignments we are able to see and edit all the information regarding the Rest API call. In the \"Data Inputs and Assignments\" section we can change for example - The \"Url\" to \"https://reqres.in/api/users\" - The \"Method\" to \"GET\" In the \"Data Outputs and Assignments\" section we can make configurations regarding the output of the node. In order to send the result of the call to the next node. But for this example we are not going to change anything. In the \"SmartCLIDE service discovery\" section you can search for existing service, provided that you have a descriptive node Name, and Description. If you don't find any appropriate service in the previous search you can use the \"SmartCLIDE service creation\". That redirects you in the service creation page of SmartCLIDE. Finally, the assignments are going to look like this:","title":"Design"},{"location":"workflows/example1/#deploy","text":"After the Design of the workflow, you can go ahead and select \"Save\". It's the first button in the top bar above the editor. Now by going back to the project (by selecting the name of the project from the top) From here you have to add to install the Rest Task to your project since it not in the main components of jBPM. You have to navigate to the \"Settings\" tab and from there select the \"Custom Tasks\" from the left side, and finally select \"Install\" for th Rest task. So by navigating once again in the Assets Tab, you can see the Rest Asset as well. And by selecting the \"Deploy\" button from the top right, the project is being build and deployed. So now that our project is deployed if we want to create a process instance of our workflow, we navigate from the top menu to the \"Process Definitions\". From here you are able to see all the processes, and you can start a new instance like this: Now we can navigate to the Manage of \"Process Instances\" like this: From here we see all the Process Instances that are active or have finished. We can see that our Process Instance is finished since it had to do only an API call. And if we select it, we can see more information about it.","title":"Deploy"},{"location":"workflows/example2/","text":"Example 2 For this example we are going to create a more complex workflow, as an extension from the first example. Design First things first we are going to start from the previous workflow, and we are going to add 2 public variable. This can be done from the properties of the projects (when nothing is selected from the diagram). As it is visible from the following figure, you can add a new \"Process Variable\" in the \"Process Data\" section. - A variable with name \"data\" and the type \"String\". - A variable with name \"data2\" and the type \"String\". So, after that we can use this variable in order to store our API request result. By selecting the Rest Task, and navigating to the Assignments once again, in the \"Data Outputs and Assignments\" section we can do the following. We can simply select our first public variable from the dropdown list from the \"Target\" column for the Results. Like the following figure. From there we can add an \"Exclusive\" Gateway, in order to check if the \"data\" variable is empty, and make another call or end the workflow accordingly. The workflow is going to look like the following diagram. In order to create the true-false value that the Gateway is going to use, we select the arrow that goes to the End and from the Implementation/Execution section we could add the following. In the \"Process Variable\" we select the variable \"data\", and in the \"Condition\" we can select the \"Is empty\". Also, in the arrow that goes to the second Rest Task we can add the opposite in the following way. After that, in the second Rest Task like before we add the \"Url\", \"Method\", and in the output we could use the variable \"data2\". Deploy After these changes we can Save our workflow, and we deploy it like before. By navigating once more to the \"Process Definitions\" in order to Start a new instance. We don't provide any initial values to our variables, and we select \"Submit\". And in the Tab \"Process Variables\" we can see that both of our variables were populated. And in the Tab Diagram we can even see the Tasks that were executed.","title":"Example 2"},{"location":"workflows/example2/#example-2","text":"For this example we are going to create a more complex workflow, as an extension from the first example.","title":"Example 2"},{"location":"workflows/example2/#design","text":"First things first we are going to start from the previous workflow, and we are going to add 2 public variable. This can be done from the properties of the projects (when nothing is selected from the diagram). As it is visible from the following figure, you can add a new \"Process Variable\" in the \"Process Data\" section. - A variable with name \"data\" and the type \"String\". - A variable with name \"data2\" and the type \"String\". So, after that we can use this variable in order to store our API request result. By selecting the Rest Task, and navigating to the Assignments once again, in the \"Data Outputs and Assignments\" section we can do the following. We can simply select our first public variable from the dropdown list from the \"Target\" column for the Results. Like the following figure. From there we can add an \"Exclusive\" Gateway, in order to check if the \"data\" variable is empty, and make another call or end the workflow accordingly. The workflow is going to look like the following diagram. In order to create the true-false value that the Gateway is going to use, we select the arrow that goes to the End and from the Implementation/Execution section we could add the following. In the \"Process Variable\" we select the variable \"data\", and in the \"Condition\" we can select the \"Is empty\". Also, in the arrow that goes to the second Rest Task we can add the opposite in the following way. After that, in the second Rest Task like before we add the \"Url\", \"Method\", and in the output we could use the variable \"data2\".","title":"Design"},{"location":"workflows/example2/#deploy","text":"After these changes we can Save our workflow, and we deploy it like before. By navigating once more to the \"Process Definitions\" in order to Start a new instance. We don't provide any initial values to our variables, and we select \"Submit\". And in the Tab \"Process Variables\" we can see that both of our variables were populated. And in the Tab Diagram we can even see the Tasks that were executed.","title":"Deploy"}]}